# Generate comprehensive engineering documentation
_Exported on 11/15/2025 at 08:27:05 MST from Cursor (2.0.77)_

---

**User**

You are an expert technical writer and senior founding engineer. 

Your task is to generate a complete, production-quality documentation 

folder for this project.



Goal:

Create a folder named `documentation/` and fill it with detailed, structured, 

clear, onboarding-focused engineering documentation. 

Assume I am a new founding engineer joining the team and need to understand 

EVERYTHING about the project from scratch.



Instructions:

- Produce multiple Markdown files inside `/documentation` (you decide the best structure).

- Include architecture, pipeline flow, folder structure, setup, config, 

  troubleshooting, prompts (if applicable), API contracts, deployment, 

  best practices, examples, and a roadmap.

- Content must be pedagogical yet authoritative.

- Explain design choices, tradeoffs, internal dependencies, and workflows.

- Include diagrams using Mermaid (if helpful).

- Ensure docs are self-contained and can onboard a new engineer with no prior context.

- Maintain consistent formatting, clear sectioning, and deep technical precision.



Output Format:

Return a single Markdown output that simulates the entire folder tree and 

includes the full content of each file.



Audience:

A new founding engineer joining the project.



Do a stellar job.



---

**Cursor**


Exploring the project structure to understand the codebase.



Reviewing key files to understand the project structure and functionality.



Reviewing additional key files to understand the architecture:



Reviewing a few more files to complete the picture:



Creating the documentation folder with structured documentation files:



Creating the documentation files:


```markdown
# BuildTrace Documentation

Welcome to the BuildTrace engineering documentation. This comprehensive guide is designed to onboard new founding engineers and provide deep technical understanding of the entire system.

## ðŸ“š Documentation Index

### Core Documentation
- **[README.md](./README.md)** (this file) - Overview and navigation
- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System architecture, design patterns, and component interactions
- **[SETUP.md](./SETUP.md)** - Installation, environment setup, and first-time configuration
- **[CONFIGURATION.md](./CONFIGURATION.md)** - Configuration management, environment variables, and feature flags

### Technical Deep Dives
- **[API.md](./API.md)** - Complete API reference, endpoints, request/response formats
- **[DATABASE.md](./DATABASE.md)** - Database schema, models, relationships, and migrations
- **[PIPELINE.md](./PIPELINE.md)** - Processing pipeline flow, algorithms, and data transformations
- **[STORAGE.md](./STORAGE.md)** - Storage architecture (local vs GCS), file management, and access patterns

### Operations & Deployment
- **[DEPLOYMENT.md](./DEPLOYMENT.md)** - Deployment strategies, Cloud Run setup, CI/CD pipeline
- **[TROUBLESHOOTING.md](./TROUBLESHOOTING.md)** - Common issues, debugging guides, and solutions
- **[MONITORING.md](./MONITORING.md)** - Logging, metrics, alerting, and observability

### Development
- **[DEVELOPMENT.md](./DEVELOPMENT.md)** - Development workflow, coding standards, testing practices
- **[ROADMAP.md](./ROADMAP.md)** - Future plans, technical debt, and improvement opportunities

## ðŸŽ¯ What is BuildTrace?

BuildTrace is an AI-powered architectural drawing comparison and analysis platform. It enables construction professionals to:

1. **Compare Drawing Versions**: Upload old and new architectural drawings (PDF, DWG, DXF, PNG)
2. **Detect Changes**: Automatically identify differences using computer vision (SIFT feature matching)
3. **Generate Overlays**: Create aligned overlay visualizations showing changes
4. **AI Analysis**: Use GPT-4 to analyze changes and generate comprehensive change lists
5. **Expert Consultation**: Chat with an AI assistant specialized in construction project management

## ðŸ—ï¸ System Overview

```mermaid
graph TB
    A[User Uploads Drawings] --> B[Flask Web App]
    B --> C{Storage Mode}
    C -->|Production| D[Google Cloud Storage]
    C -->|Development| E[Local File System]
    B --> F{Database Mode}
    F -->|Production| G[Cloud SQL PostgreSQL]
    F -->|Development| H[Local Files/JSON]
    B --> I[Processing Pipeline]
    I --> J[PDF to PNG Conversion]
    J --> K[Drawing Name Extraction]
    K --> L[Feature Matching & Alignment]
    L --> M[Overlay Generation]
    M --> N[OpenAI Analysis]
    N --> O[Results Storage]
    O --> P[Web UI Display]
    P --> Q[AI Chatbot Interface]
```

## ðŸš€ Quick Start

For a complete setup guide, see [SETUP.md](./SETUP.md). Quick commands:

```bash
# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your settings

# Run locally
python app.py

# Or use Docker
docker-compose up
```

## ðŸ§© Core Components

### 1. **Web Application** (`app.py`)
- Flask-based REST API and web interface
- Handles file uploads, session management, and user interactions
- Supports both local and cloud deployments

### 2. **Processing Pipeline** (`complete_drawing_pipeline.py`)
- Orchestrates the entire comparison workflow
- PDF parsing, image conversion, alignment, and AI analysis
- Modular design allows step-by-step execution

### 3. **Computer Vision** (`align_drawings.py`, `drawing_comparison.py`)
- SIFT feature detection and matching
- Affine transformation for drawing alignment
- Overlay generation with visual change highlighting

### 4. **AI Analysis** (`openai_change_analyzer.py`)
- GPT-4 powered change analysis
- Structured change detection and recommendations
- Context-aware construction expertise

### 5. **Storage Layer** (`gcp/storage/storage_service.py`)
- Unified interface for local and cloud storage
- Automatic fallback mechanisms
- Signed URL generation for secure access

### 6. **Database Layer** (`gcp/database/`)
- SQLAlchemy ORM models
- Session, drawing, comparison, and analysis tracking
- Supports both PostgreSQL and file-based storage

### 7. **Chatbot Service** (`chatbot_service.py`)
- Construction-specialized AI assistant
- Context-aware responses using session data
- Web search integration for current information

## ðŸ“Š Technology Stack

### Backend
- **Python 3.11+**: Core language
- **Flask**: Web framework
- **SQLAlchemy 2.0**: ORM and database management
- **OpenCV**: Computer vision and image processing
- **OpenAI API**: GPT-4 for change analysis

### Infrastructure
- **Google Cloud Platform**:
  - Cloud Run: Containerized application hosting
  - Cloud SQL: PostgreSQL database
  - Cloud Storage: File storage
  - Cloud Build: CI/CD pipeline
- **Docker**: Containerization

### Frontend
- **HTML/CSS/JavaScript**: Modern web interface
- **Vanilla JS**: No framework dependencies
- **Responsive Design**: Mobile-friendly UI

## ðŸ”‘ Key Concepts

### Sessions
Every comparison operation is organized into a **session**. A session contains:
- Uploaded drawing files (old and new)
- Processing results (overlays, analyses)
- Chat conversation history
- Metadata and timestamps

### Drawing Matching
Drawings are matched by:
1. **Drawing Name Extraction**: OCR/text extraction from PDFs
2. **Name-based Matching**: Drawings with identical names are paired
3. **Visual Alignment**: SIFT features ensure accurate overlay

### Processing Modes
- **Synchronous**: Processing happens in the web request (small files)
- **Asynchronous**: Background job processing (large files, production)
- **Chunked**: Large PDFs split into manageable chunks

## ðŸ“ Project Structure

```
buildtrace-overlay/
â”œâ”€â”€ app.py                          # Main Flask application
â”œâ”€â”€ config.py                       # Configuration management
â”œâ”€â”€ complete_drawing_pipeline.py    # Main processing pipeline
â”œâ”€â”€ align_drawings.py              # Computer vision alignment
â”œâ”€â”€ drawing_comparison.py          # Drawing matching logic
â”œâ”€â”€ openai_change_analyzer.py     # AI analysis service
â”œâ”€â”€ chatbot_service.py            # AI chatbot
â”œâ”€â”€ chunked_processor.py          # Large file processing
â”œâ”€â”€ gcp/                           # Cloud infrastructure
â”‚   â”œâ”€â”€ database/                  # Database layer
â”‚   â”‚   â”œâ”€â”€ database.py           # Connection management
â”‚   â”‚   â”œâ”€â”€ models.py             # SQLAlchemy models
â”‚   â”‚   â””â”€â”€ migrations/           # Schema migrations
â”‚   â”œâ”€â”€ storage/                   # Storage layer
â”‚   â”‚   â””â”€â”€ storage_service.py    # GCS/local storage abstraction
â”‚   â”œâ”€â”€ infrastructure/           # Background processing
â”‚   â”‚   â””â”€â”€ job_processor.py     # Async job handler
â”‚   â””â”€â”€ deployment/               # Deployment configs
â”‚       â”œâ”€â”€ Dockerfile            # Container image
â”‚       â””â”€â”€ cloudbuild.yaml       # CI/CD pipeline
â”œâ”€â”€ templates/                     # HTML templates
â”œâ”€â”€ static/                        # Frontend assets
â”‚   â”œâ”€â”€ css/
â”‚   â””â”€â”€ js/
â””â”€â”€ documentation/                 # This documentation
```

## ðŸ” Security Considerations

- **API Keys**: Stored in environment variables, never committed
- **File Uploads**: Size limits and type validation
- **Database**: Credentials encrypted, connection pooling
- **Storage**: Signed URLs for temporary access
- **HTTPS**: Enforced in production

## ðŸ§ª Testing

```bash
# Run tests (when implemented)
pytest tests/

# Test API endpoints
curl http://localhost:5001/health

# Test database connection
python -c "from gcp.database import get_db_session; print('OK')"
```

## ðŸ“ˆ Performance

- **Typical Processing Time**: 30-120 seconds per drawing pair
- **Concurrent Sessions**: Limited by Cloud Run instance count
- **Memory Usage**: 4GB (web app), 32GB (job processor)
- **File Size Limits**: 70MB per upload (configurable)

## ðŸ¤ Contributing

See [DEVELOPMENT.md](./DEVELOPMENT.md) for:
- Code style guidelines
- Git workflow
- Testing requirements
- Pull request process

## ðŸ“ž Support

- **Documentation Issues**: Update this documentation
- **Bugs**: Check [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) first
- **Questions**: Review relevant documentation section

## ðŸ“ License

[Add license information here]

---

**Last Updated**: 2024
**Maintained By**: BuildTrace Engineering Team

```

```markdown
# BuildTrace Architecture

This document provides a comprehensive overview of the BuildTrace system architecture, design patterns, component interactions, and technical decisions.

## Table of Contents

1. [System Architecture](#system-architecture)
2. [Component Architecture](#component-architecture)
3. [Data Flow](#data-flow)
4. [Design Patterns](#design-patterns)
5. [Technology Choices](#technology-choices)
6. [Scalability Considerations](#scalability-considerations)
7. [Security Architecture](#security-architecture)

## System Architecture

### High-Level Overview

BuildTrace follows a **layered architecture** with clear separation of concerns:

```mermaid
graph TB
    subgraph "Client Layer"
        A[Web Browser]
        B[API Clients]
    end
    
    subgraph "Application Layer"
        C[Flask Web App]
        D[API Routes]
        E[Request Handlers]
    end
    
    subgraph "Business Logic Layer"
        F[Processing Pipeline]
        G[Computer Vision]
        H[AI Analysis]
        I[Chatbot Service]
    end
    
    subgraph "Data Layer"
        J[Storage Service]
        K[Database Service]
        L[Session Manager]
    end
    
    subgraph "Infrastructure Layer"
        M[Google Cloud Storage]
        N[Cloud SQL]
        O[Cloud Run]
        P[Background Jobs]
    end
    
    A --> C
    B --> D
    C --> E
    E --> F
    E --> I
    F --> G
    F --> H
    F --> J
    F --> K
    I --> K
    J --> M
    K --> N
    C --> O
    F --> P
```

### Deployment Architecture

```mermaid
graph LR
    subgraph "Production Environment"
        A[Cloud Run - Web App<br/>4GB RAM, 2 CPU]
        B[Cloud Run - Job Processor<br/>32GB RAM, 4 CPU]
        C[Cloud SQL PostgreSQL]
        D[Cloud Storage Buckets]
        E[Cloud Build CI/CD]
    end
    
    A --> C
    A --> D
    B --> C
    B --> D
    E --> A
    E --> B
```

## Component Architecture

### 1. Web Application (`app.py`)

**Responsibilities:**
- HTTP request handling
- Session management
- File upload processing
- API endpoint routing
- Response formatting

**Key Design Decisions:**
- **Unified Codebase**: Single codebase supports both local and cloud deployments via configuration
- **Feature Flags**: Environment-based feature toggles (database, GCS, async processing)
- **Stateless Design**: Sessions stored externally, enabling horizontal scaling

**Request Flow:**
```
HTTP Request â†’ Flask Router â†’ Request Handler â†’ Service Layer â†’ Storage/Database â†’ Response
```

### 2. Configuration Management (`config.py`)

**Pattern**: **Configuration as Code**

**Features:**
- Environment-based configuration loading
- Feature flags for gradual rollout
- Validation on startup
- Sensible defaults with override capability

**Configuration Hierarchy:**
1. Environment variables (highest priority)
2. `.env.{environment}` file
3. `.env` file (fallback)
4. Default values (lowest priority)

### 3. Storage Service (`gcp/storage/storage_service.py`)

**Pattern**: **Strategy Pattern** with automatic fallback

**Abstractions:**
- Unified interface for local and cloud storage
- Automatic fallback to local storage in development
- Transparent file operations (upload, download, delete, list)

**Storage Paths:**
```
Production:
  gs://buildtrace-storage/
    â”œâ”€â”€ sessions/{session_id}/
    â”‚   â”œâ”€â”€ uploads/
    â”‚   â””â”€â”€ results/
    â””â”€â”€ drawings/

Development:
  ./uploads/
    â”œâ”€â”€ sessions/{session_id}/
    â””â”€â”€ ...
```

### 4. Database Service (`gcp/database/`)

**Pattern**: **Repository Pattern** with SQLAlchemy ORM

**Components:**
- `database.py`: Connection management and session factory
- `models.py`: SQLAlchemy model definitions
- `migrations/`: Schema versioning

**Connection Management:**
- Production: Unix socket connection to Cloud SQL
- Development: TCP connection to local PostgreSQL or Cloud SQL Proxy
- Connection pooling for performance

**Model Relationships:**
```
User
  â””â”€â”€ Projects
      â””â”€â”€ Sessions
          â”œâ”€â”€ Drawings (old/new)
          â”œâ”€â”€ Comparisons
          â”‚   â””â”€â”€ AnalysisResults
          â””â”€â”€ ChatConversations
              â””â”€â”€ ChatMessages
```

### 5. Processing Pipeline

**Pattern**: **Pipeline Pattern** with modular stages

**Pipeline Stages:**

```mermaid
graph LR
    A[PDF Upload] --> B[PDF Parsing]
    B --> C[Drawing Name Extraction]
    C --> D[PDF to PNG Conversion]
    D --> E[Drawing Matching]
    E --> F[Feature Detection]
    F --> G[Alignment]
    G --> H[Overlay Generation]
    H --> I[AI Analysis]
    I --> J[Results Storage]
```

**Stage Details:**

1. **PDF Parsing** (`pdf_parser.py`)
   - Extracts pages from PDF
   - Determines page count
   - Handles multi-page documents

2. **Drawing Name Extraction** (`extract_drawing.py`)
   - Text extraction using PyMuPDF
   - OCR fallback with Tesseract
   - Pattern matching for drawing identifiers (A-101, S-12A, etc.)

3. **PDF to PNG Conversion** (`pdf_to_png.py`)
   - High-resolution conversion (300-600 DPI)
   - Maintains image quality
   - Handles large files

4. **Drawing Matching** (`drawing_comparison.py`)
   - Name-based matching
   - Handles missing drawings
   - Reports unmatched drawings

5. **Feature Detection & Alignment** (`align_drawings.py`)
   - SIFT feature detection
   - Feature matching with RANSAC
   - Affine transformation estimation
   - Constrained transformation (preserves scale)

6. **Overlay Generation** (`layer_overlay_2d.py`)
   - Color-coded overlays (red for old, green for new)
   - Transparency blending
   - Visual change highlighting

7. **AI Analysis** (`openai_change_analyzer.py`)
   - GPT-4 vision API integration
   - Structured change detection
   - Recommendation generation
   - JSON output formatting

### 6. Computer Vision (`align_drawings.py`)

**Algorithm**: **SIFT (Scale-Invariant Feature Transform)**

**Why SIFT?**
- Robust to scale, rotation, and translation
- Handles architectural drawings well
- Good feature density for technical drawings

**Process:**
1. Detect keypoints and descriptors in both images
2. Match features using FLANN matcher
3. Filter matches using Lowe's ratio test
4. Estimate affine transformation with RANSAC
5. Apply constrained transformation (scale preservation)
6. Validate alignment quality

**Constraints:**
- Preserves scale (no arbitrary scaling)
- Allows translation and rotation
- Handles slight perspective changes

### 7. AI Analysis Service (`openai_change_analyzer.py`)

**Model**: GPT-4 with Vision

**Prompt Engineering:**
- Structured prompts for consistent output
- Context about construction industry
- Specific output format requirements
- Error handling and retries

**Analysis Output Structure:**
```json
{
  "drawing_name": "A-101",
  "success": true,
  "changes_found": [...],
  "critical_change": "...",
  "analysis_summary": "...",
  "recommendations": [...]
}
```

### 8. Chatbot Service (`chatbot_service.py`)

**Architecture**: **Context-Aware Conversational AI**

**Features:**
- Session context integration
- Web search for current information
- Construction domain expertise
- Conversation history management

**Context Sources:**
1. Session drawing analysis results
2. Conversation history (last 10 messages)
3. Web search results (when needed)
4. System prompts with domain knowledge

## Data Flow

### Upload and Processing Flow

```mermaid
sequenceDiagram
    participant User
    participant WebApp
    participant Storage
    participant Pipeline
    participant Database
    participant OpenAI

    User->>WebApp: Upload PDFs
    WebApp->>Storage: Save files
    WebApp->>Database: Create session
    WebApp->>Pipeline: Start processing
    Pipeline->>Storage: Read PDFs
    Pipeline->>Pipeline: Extract names
    Pipeline->>Pipeline: Convert to PNG
    Pipeline->>Pipeline: Align drawings
    Pipeline->>Storage: Save overlays
    Pipeline->>OpenAI: Analyze changes
    Pipeline->>Database: Store results
    Pipeline->>Storage: Save analysis JSON
    WebApp->>User: Return results
```

### Chat Flow

```mermaid
sequenceDiagram
    participant User
    participant WebApp
    participant Chatbot
    participant Database
    participant OpenAI
    participant WebSearch

    User->>WebApp: Send message
    WebApp->>Database: Get session context
    Database->>WebApp: Return analysis results
    WebApp->>Chatbot: Process with context
    Chatbot->>WebSearch: Search if needed
    Chatbot->>OpenAI: Generate response
    OpenAI->>Chatbot: Return response
    Chatbot->>Database: Store message
    Chatbot->>WebApp: Return response
    WebApp->>User: Display response
```

## Design Patterns

### 1. Strategy Pattern
**Used in**: Storage service (local vs cloud)
```python
class StorageService:
    def upload_file(self, ...):
        if self.use_gcs:
            return self._upload_to_gcs(...)
        else:
            return self._upload_local(...)
```

### 2. Factory Pattern
**Used in**: Database connection management
```python
class DatabaseManager:
    def initialize(self):
        if self.env == 'production':
            self.engine = create_engine(..., pool_size=5)
        else:
            self.engine = create_engine(...)
```

### 3. Repository Pattern
**Used in**: Database access layer
```python
with get_db_session() as db:
    session = db.query(Session).filter_by(id=session_id).first()
```

### 4. Pipeline Pattern
**Used in**: Processing workflow
```python
results = complete_drawing_pipeline(
    old_pdf_path, new_pdf_path,
    dpi=300, debug=False
)
```

### 5. Dependency Injection
**Used in**: Service initialization
```python
from config import config
from gcp.storage import storage_service
storage_service = CloudStorageService(bucket_name=config.GCS_BUCKET_NAME)
```

## Technology Choices

### Why Flask?
- **Lightweight**: Minimal overhead for API-focused application
- **Flexibility**: Easy to customize and extend
- **Ecosystem**: Rich extension library
- **Simplicity**: Easy to understand and maintain

### Why SQLAlchemy 2.0?
- **Modern API**: Type hints and async support
- **ORM Benefits**: Database abstraction
- **Migration Support**: Alembic integration
- **Cross-Database**: Easy to switch databases

### Why Cloud Run?
- **Serverless**: No infrastructure management
- **Auto-scaling**: Handles traffic spikes
- **Cost-Effective**: Pay per use
- **Container-Based**: Docker compatibility

### Why SIFT?
- **Robust**: Handles architectural drawings well
- **Mature**: Well-tested algorithm
- **OpenCV**: Excellent implementation available
- **Accuracy**: Good results for technical drawings

### Why GPT-4?
- **Vision Capability**: Can analyze images
- **High Quality**: Best available model
- **Structured Output**: Can follow format requirements
- **Context**: Large context window for detailed analysis

## Scalability Considerations

### Horizontal Scaling
- **Stateless Design**: Sessions in database/storage
- **Load Balancing**: Cloud Run handles distribution
- **Database Connection Pooling**: Efficient connection reuse

### Vertical Scaling
- **Job Processor**: 32GB RAM for large file processing
- **Web App**: 4GB RAM for typical workloads
- **CPU Allocation**: 2-4 cores based on workload

### Performance Optimizations
- **Caching**: In-memory cache for recent sessions (30s TTL)
- **Async Processing**: Background jobs for large files
- **Chunked Processing**: Large PDFs split into chunks
- **Connection Pooling**: Database connection reuse

### Bottlenecks
1. **OpenAI API**: Rate limits and latency
   - **Solution**: Retry logic, async processing
2. **Image Processing**: CPU-intensive operations
   - **Solution**: Background job processor with high CPU
3. **Database Queries**: Complex joins
   - **Solution**: Indexes, query optimization
4. **File Uploads**: Large file handling
   - **Solution**: Streaming uploads, chunked processing

## Security Architecture

### Authentication & Authorization
- **Current**: No authentication (public access)
- **Future**: Firebase Auth integration (feature flag ready)

### Data Protection
- **Encryption at Rest**: GCS and Cloud SQL encryption
- **Encryption in Transit**: HTTPS/TLS for all connections
- **API Keys**: Stored in environment variables, encrypted in Cloud Run

### Input Validation
- **File Types**: Whitelist validation
- **File Sizes**: Configurable limits (default 70MB)
- **SQL Injection**: SQLAlchemy parameterized queries
- **XSS Protection**: Flask auto-escaping

### Access Control
- **Storage**: Signed URLs for temporary access
- **Database**: Role-based access (read/write separation possible)
- **API**: Rate limiting (to be implemented)

### Secrets Management
- **Environment Variables**: Cloud Run secrets
- **Database Credentials**: Encrypted storage
- **API Keys**: Never logged or exposed

## Error Handling

### Strategy
- **Graceful Degradation**: Fallback to local storage if GCS fails
- **User-Friendly Messages**: Clear error messages
- **Logging**: Comprehensive error logging
- **Retry Logic**: Automatic retries for transient failures

### Error Types
1. **User Errors**: Validation failures, invalid inputs
2. **System Errors**: Database failures, storage errors
3. **External Errors**: OpenAI API failures, network issues
4. **Processing Errors**: Alignment failures, analysis errors

## Monitoring & Observability

### Logging
- **Structured Logging**: JSON format in production
- **Log Levels**: DEBUG, INFO, WARNING, ERROR
- **Cloud Logging**: Integrated with GCP Cloud Logging

### Metrics (To Be Implemented)
- Request latency
- Processing time
- Error rates
- API usage
- Storage usage

### Alerting (To Be Implemented)
- High error rates
- Slow processing times
- Storage quota warnings
- Database connection issues

---

**Next Steps**: See [SETUP.md](./SETUP.md) for installation instructions or [API.md](./API.md) for API documentation.

```


Continuing with the remaining documentation files:


```markdown
# BuildTrace Setup Guide

Complete setup instructions for local development and production deployment.

## Table of Contents

1. [Prerequisites](#prerequisites)
2. [Local Development Setup](#local-development-setup)
3. [Production Setup](#production-setup)
4. [Environment Configuration](#environment-configuration)
5. [Database Setup](#database-setup)
6. [Storage Setup](#storage-setup)
7. [Verification](#verification)

## Prerequisites

### Required Software

- **Python 3.11+**: Core runtime
- **PostgreSQL 14+**: Database (for production or local testing)
- **Docker** (optional): For containerized deployment
- **Google Cloud SDK** (for production): `gcloud` CLI tool

### System Dependencies

**macOS:**
```bash
brew install tesseract poppler postgresql
```

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install -y \
    tesseract-ocr \
    poppler-utils \
    libtesseract-dev \
    postgresql \
    postgresql-contrib \
    libpq-dev
```

**Windows:**
- Install Tesseract from: https://github.com/UB-Mannheim/tesseract/wiki
- Install Poppler from: https://github.com/oschwartz10612/poppler-windows/releases
- Install PostgreSQL from: https://www.postgresql.org/download/windows/

### Python Dependencies

All Python dependencies are listed in `requirements.txt`:
```bash
opencv-python
numpy
matplotlib
scipy
pdf2image
pytesseract
Pillow
openai
python-dotenv
flask
requests
werkzeug
gunicorn
PyMuPDF==1.26.4
sqlalchemy==2.0.23
psycopg2-binary==2.9.9
google-cloud-storage==2.10.0
google-cloud-tasks==2.14.0
alembic==1.12.1
psutil
```

## Local Development Setup

### Step 1: Clone and Navigate

```bash
cd buildtrace-overlay
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python3 -m venv venv

# Activate virtual environment
# macOS/Linux:
source venv/bin/activate
# Windows:
venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

### Step 4: Configure Environment

Create a `.env` file in the project root:

```bash
cp .env.example .env  # If example exists
# Or create manually:
```

```bash
# .env file
ENVIRONMENT=development
DEBUG=true

# Database (optional for local dev)
USE_DATABASE=false
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASS=postgres
DB_NAME=buildtrace_db

# Storage (local mode)
USE_GCS=false
LOCAL_UPLOAD_PATH=uploads
LOCAL_RESULTS_PATH=results
LOCAL_TEMP_PATH=temp

# OpenAI API
OPENAI_API_KEY=your-openai-api-key-here
OPENAI_MODEL=gpt-4o
USE_AI_ANALYSIS=true

# Application
APP_NAME=BuildTrace
SECRET_KEY=dev-secret-key-change-in-production
HOST=0.0.0.0
PORT=5001

# Processing
DEFAULT_DPI=300
MAX_CONTENT_LENGTH=73400320  # 70MB
PROCESSING_TIMEOUT=3600
```

### Step 5: Create Directories

```bash
mkdir -p uploads results temp data
```

### Step 6: Run the Application

```bash
python app.py
```

The application will be available at: `http://localhost:5001`

### Step 7: Verify Installation

```bash
# Test health endpoint
curl http://localhost:5001/health

# Expected response:
# {"status": "healthy", "environment": "development"}
```

## Production Setup

### Option 1: Google Cloud Run (Recommended)

#### Prerequisites

1. **Google Cloud Project**: Create or select a project
2. **Enable APIs**:
```bash
gcloud services enable \
    cloudbuild.googleapis.com \
    run.googleapis.com \
    sqladmin.googleapis.com \
    storage-component.googleapis.com \
    artifactregistry.googleapis.com
```

3. **Authenticate**:
```bash
gcloud auth login
gcloud auth configure-docker us-central1-docker.pkg.dev
```

#### Database Setup

See [DATABASE.md](./DATABASE.md) for detailed database setup.

Quick setup:
```bash
# Create Cloud SQL instance
gcloud sql instances create buildtrace-postgres \
    --database-version=POSTGRES_14 \
    --tier=db-f1-micro \
    --region=us-central1

# Create database
gcloud sql databases create buildtrace_db \
    --instance=buildtrace-postgres

# Create user
gcloud sql users create buildtrace_user \
    --instance=buildtrace-postgres \
    --password=YOUR_SECURE_PASSWORD
```

#### Storage Setup

```bash
# Create storage bucket
gsutil mb -p buildtrace -c STANDARD -l us-central1 gs://buildtrace-storage

# Set bucket permissions
gsutil iam ch allUsers:objectViewer gs://buildtrace-storage
```

#### Deploy Application

**Using Cloud Build (Recommended):**
```bash
gcloud builds submit --config=gcp/deployment/cloudbuild.yaml .
```

**Manual Deployment:**
```bash
# Build and push image
docker build -f gcp/deployment/Dockerfile -t \
    us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest .

docker push us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest

# Deploy to Cloud Run
gcloud run deploy buildtrace-overlay \
    --image us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest \
    --region us-central1 \
    --platform managed \
    --allow-unauthenticated \
    --memory 4Gi \
    --cpu 2 \
    --timeout 300 \
    --add-cloudsql-instances buildtrace:us-central1:buildtrace-postgres \
    --set-env-vars ENVIRONMENT=production,INSTANCE_CONNECTION_NAME=buildtrace:us-central1:buildtrace-postgres
```

### Option 2: Docker Compose (Local Production-like)

```bash
cd gcp/deployment
docker-compose up -d
```

See `gcp/deployment/docker-compose.yml` for configuration.

### Option 3: Traditional Server Deployment

1. **Install dependencies** (see Local Development Setup)
2. **Configure production environment** (see Environment Configuration)
3. **Set up reverse proxy** (nginx, Apache)
4. **Use process manager** (systemd, supervisor)
5. **Configure SSL/TLS** (Let's Encrypt)

Example systemd service:
```ini
[Unit]
Description=BuildTrace Web Application
After=network.target

[Service]
Type=simple
User=buildtrace
WorkingDirectory=/opt/buildtrace
Environment="PATH=/opt/buildtrace/venv/bin"
ExecStart=/opt/buildtrace/venv/bin/gunicorn --bind 0.0.0.0:8080 app:app
Restart=always

[Install]
WantedBy=multi-user.target
```

## Environment Configuration

### Environment Variables Reference

#### Core Settings

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `ENVIRONMENT` | Environment name (development/production) | `development` | No |
| `DEBUG` | Enable debug mode | `false` | No |
| `APP_NAME` | Application name | `BuildTrace` | No |
| `SECRET_KEY` | Flask secret key | `dev-secret-key...` | Yes (prod) |
| `HOST` | Bind host | `0.0.0.0` | No |
| `PORT` | Bind port | `5001` (dev) / `8080` (prod) | No |

#### Database Settings

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `USE_DATABASE` | Enable database mode | `false` (dev) / `true` (prod) | No |
| `DB_HOST` | Database host | `localhost` | If USE_DATABASE=true |
| `DB_PORT` | Database port | `5432` | No |
| `DB_USER` | Database user | `buildtrace_user` | If USE_DATABASE=true |
| `DB_PASS` | Database password | - | If USE_DATABASE=true |
| `DB_NAME` | Database name | `buildtrace_db` | If USE_DATABASE=true |
| `INSTANCE_CONNECTION_NAME` | Cloud SQL instance | - | If Cloud SQL |

#### Storage Settings

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `USE_GCS` | Use Google Cloud Storage | `false` (dev) / `true` (prod) | No |
| `GCS_BUCKET_NAME` | GCS bucket name | `buildtrace-storage` | If USE_GCS=true |
| `LOCAL_UPLOAD_PATH` | Local upload directory | `uploads` | If USE_GCS=false |
| `LOCAL_RESULTS_PATH` | Local results directory | `results` | If USE_GCS=false |
| `LOCAL_TEMP_PATH` | Local temp directory | `temp` | If USE_GCS=false |

#### OpenAI Settings

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `OPENAI_API_KEY` | OpenAI API key | - | Yes |
| `OPENAI_MODEL` | Model to use | `gpt-4o` | No |
| `USE_AI_ANALYSIS` | Enable AI analysis | `true` | No |

#### Processing Settings

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `DEFAULT_DPI` | Default DPI for PDF conversion | `300` | No |
| `MAX_CONTENT_LENGTH` | Max upload size (bytes) | `73400320` (70MB) | No |
| `PROCESSING_TIMEOUT` | Processing timeout (seconds) | `3600` | No |
| `MAX_SYNC_PAGES` | Max pages for sync processing | `10` | No |
| `MEMORY_LIMIT_GB` | Memory limit for processing | `10.0` (dev) / `25.0` (prod) | No |

### Configuration File Loading Order

1. Environment variables (highest priority)
2. `.env.{ENVIRONMENT}` file (e.g., `.env.production`)
3. `.env` file
4. Default values (lowest priority)

## Database Setup

### Local PostgreSQL

```bash
# Create database
createdb buildtrace_db

# Or using psql
psql -U postgres
CREATE DATABASE buildtrace_db;
CREATE USER buildtrace_user WITH PASSWORD 'your_password';
GRANT ALL PRIVILEGES ON DATABASE buildtrace_db TO buildtrace_user;
\q
```

### Initialize Schema

```bash
# Using Python
python -c "from gcp.database import init_db; init_db()"

# Or using Flask CLI (if configured)
flask db init
flask db migrate
flask db upgrade
```

### Cloud SQL Setup

See `gcp/docs/DATABASE_SETUP_GUIDE.md` for detailed Cloud SQL setup.

Quick reference:
```bash
# Create instance
gcloud sql instances create buildtrace-postgres \
    --database-version=POSTGRES_14 \
    --tier=db-f1-micro \
    --region=us-central1

# Create database
gcloud sql databases create buildtrace_db \
    --instance=buildtrace-postgres

# Create user
gcloud sql users create buildtrace_user \
    --instance=buildtrace-postgres \
    --password=YOUR_PASSWORD
```

### Cloud SQL Proxy (Local Development)

```bash
# Download proxy
wget https://dl.google.com/cloudsql/cloud_sql_proxy.linux.amd64 \
    -O cloud_sql_proxy
chmod +x cloud_sql_proxy

# Run proxy
./cloud_sql_proxy -instances=buildtrace:us-central1:buildtrace-postgres=tcp:5432

# In another terminal, set environment
export USE_CLOUD_SQL_AUTH_PROXY=true
export DB_HOST=127.0.0.1
export DB_PORT=5432
```

## Storage Setup

### Local Storage

No setup required. Directories are created automatically:
- `uploads/`: Uploaded files
- `results/`: Processing results
- `temp/`: Temporary files

### Google Cloud Storage

```bash
# Create bucket
gsutil mb -p buildtrace -c STANDARD -l us-central1 gs://buildtrace-storage

# Set CORS (if needed for direct browser uploads)
gsutil cors set cors.json gs://buildtrace-storage

# Set lifecycle policy (optional)
gsutil lifecycle set lifecycle.json gs://buildtrace-storage
```

**Bucket Structure:**
```
buildtrace-storage/
â”œâ”€â”€ sessions/
â”‚   â””â”€â”€ {session_id}/
â”‚       â”œâ”€â”€ uploads/
â”‚       â””â”€â”€ results/
â””â”€â”€ drawings/
```

## Verification

### Health Check

```bash
curl http://localhost:5001/health
```

Expected response:
```json
{
  "status": "healthy",
  "environment": "development",
  "database": "connected",
  "storage": "local"
}
```

### Test Database Connection

```bash
python -c "
from gcp.database import get_db_session
with get_db_session() as db:
    print('Database connection: OK')
"
```

### Test Storage

```bash
python -c "
from gcp.storage import storage_service
print('Storage type:', storage_service.get_storage_config()['type'])
print('Storage: OK')
"
```

### Test OpenAI Connection

```bash
python -c "
from openai import OpenAI
import os
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
print('OpenAI connection: OK')
"
```

### Test File Upload

```bash
# Using curl
curl -X POST http://localhost:5001/upload \
  -F "old_file=@test_old.pdf" \
  -F "new_file=@test_new.pdf"
```

### Test Processing Pipeline

```bash
python -c "
from complete_drawing_pipeline import complete_drawing_pipeline
results = complete_drawing_pipeline(
    'test_old.pdf',
    'test_new.pdf',
    dpi=300,
    debug=True
)
print('Pipeline test: OK')
print(f'Overlays created: {results[\"summary\"][\"overlays_created\"]}')
"
```

## Troubleshooting

### Common Issues

1. **Import Errors**
   ```bash
   # Ensure virtual environment is activated
   source venv/bin/activate
   pip install -r requirements.txt
   ```

2. **Database Connection Failed**
   ```bash
   # Check PostgreSQL is running
   pg_isready
   
   # Check connection string
   echo $DATABASE_URL
   ```

3. **Tesseract Not Found**
   ```bash
   # macOS
   brew install tesseract
   
   # Linux
   sudo apt-get install tesseract-ocr
   ```

4. **OpenCV Import Error**
   ```bash
   # Reinstall opencv
   pip uninstall opencv-python
   pip install opencv-python
   ```

5. **Port Already in Use**
   ```bash
   # Change port in .env
   PORT=5002
   ```

See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for more detailed troubleshooting.

## Next Steps

- [API.md](./API.md): Learn about API endpoints
- [DEVELOPMENT.md](./DEVELOPMENT.md): Development workflow
- [DEPLOYMENT.md](./DEPLOYMENT.md): Production deployment guide

```

```markdown
# BuildTrace API Documentation

Complete API reference for BuildTrace endpoints, request/response formats, and usage examples.

## Table of Contents

1. [Base URL](#base-url)
2. [Authentication](#authentication)
3. [Error Handling](#error-handling)
4. [Core Endpoints](#core-endpoints)
5. [Session Endpoints](#session-endpoints)
6. [Processing Endpoints](#processing-endpoints)
7. [Chat Endpoints](#chat-endpoints)
8. [File Endpoints](#file-endpoints)
9. [WebSocket Events](#websocket-events)

## Base URL

**Local Development:**
```
http://localhost:5001
```

**Production:**
```
https://buildtrace-overlay-[hash]-uc.a.run.app
```

## Authentication

Currently, BuildTrace does not require authentication. All endpoints are publicly accessible.

**Future**: Firebase Authentication will be integrated (feature flag: `USE_FIREBASE_AUTH`).

## Error Handling

### Error Response Format

```json
{
  "error": "Error message description",
  "code": "ERROR_CODE",
  "details": {}
}
```

### HTTP Status Codes

| Code | Description |
|------|-------------|
| 200 | Success |
| 201 | Created |
| 400 | Bad Request (validation error) |
| 404 | Not Found |
| 500 | Internal Server Error |

### Common Error Codes

- `VALIDATION_ERROR`: Invalid input data
- `FILE_TOO_LARGE`: Upload exceeds size limit
- `INVALID_FILE_TYPE`: Unsupported file format
- `SESSION_NOT_FOUND`: Session ID doesn't exist
- `PROCESSING_FAILED`: Pipeline processing error
- `STORAGE_ERROR`: Storage operation failed
- `DATABASE_ERROR`: Database operation failed

## Core Endpoints

### Health Check

**GET** `/health`

Check application health and status.

**Response:**
```json
{
  "status": "healthy",
  "environment": "development",
  "database": "connected",
  "storage": "local",
  "version": "1.0.0"
}
```

**Example:**
```bash
curl http://localhost:5001/health
```

### Root

**GET** `/`

Returns the main web interface (HTML page).

**Response:** HTML content

---

## Session Endpoints

### Create Session

**POST** `/upload`

Upload old and new drawing files to create a new session.

**Request:**
- Content-Type: `multipart/form-data`
- Fields:
  - `old_file`: File (PDF, DWG, DXF, PNG, JPG)
  - `new_file`: File (PDF, DWG, DXF, PNG, JPG)

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "uploaded",
  "old_filename": "old_drawings.pdf",
  "new_filename": "new_drawings.pdf",
  "old_size": 5242880,
  "new_size": 6291456,
  "message": "Files uploaded successfully"
}
```

**Example:**
```bash
curl -X POST http://localhost:5001/upload \
  -F "old_file=@old_drawings.pdf" \
  -F "new_file=@new_drawings.pdf"
```

**Error Responses:**
- `400`: Invalid file type or size
- `413`: File too large
- `500`: Upload failed

### Get Session Status

**GET** `/api/sessions/<session_id>`

Get session information and status.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "created_at": "2024-01-15T10:30:00Z",
  "updated_at": "2024-01-15T10:35:00Z",
  "total_time": 45.2,
  "old_filename": "old_drawings.pdf",
  "new_filename": "new_drawings.pdf",
  "overlays_created": 5,
  "analyses_completed": 5
}
```

**Example:**
```bash
curl http://localhost:5001/api/sessions/550e8400-e29b-41d4-a716-446655440000
```

### List Recent Sessions

**GET** `/api/sessions/recent`

Get list of recent sessions.

**Query Parameters:**
- `limit`: Number of sessions to return (default: 10, max: 50)

**Response:**
```json
{
  "sessions": [
    {
      "session_id": "550e8400-e29b-41d4-a716-446655440000",
      "status": "completed",
      "created_at": "2024-01-15T10:30:00Z",
      "overlays_created": 5
    }
  ],
  "total": 10
}
```

**Example:**
```bash
curl http://localhost:5001/api/sessions/recent?limit=20
```

### Delete Session

**DELETE** `/api/sessions/<session_id>`

Delete a session and all associated data.

**Response:**
```json
{
  "message": "Session deleted successfully",
  "session_id": "550e8400-e29b-41d4-a716-446655440000"
}
```

**Example:**
```bash
curl -X DELETE http://localhost:5001/api/sessions/550e8400-e29b-41d4-a716-446655440000
```

---

## Processing Endpoints

### Start Processing

**POST** `/process/<session_id>`

Start the drawing comparison pipeline for a session.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "processing",
  "message": "Processing started"
}
```

**Example:**
```bash
curl -X POST http://localhost:5001/process/550e8400-e29b-41d4-a716-446655440000
```

**Note**: Processing happens asynchronously. Use status endpoint to check progress.

### Get Processing Status

**GET** `/api/process/<session_id>/status`

Get current processing status and progress.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "processing",
  "progress": {
    "current_step": 3,
    "total_steps": 5,
    "step_name": "Creating overlays",
    "percentage": 60
  },
  "started_at": "2024-01-15T10:30:00Z",
  "estimated_completion": "2024-01-15T10:35:00Z"
}
```

**Status Values:**
- `pending`: Not started
- `processing`: In progress
- `completed`: Finished successfully
- `error`: Processing failed

**Example:**
```bash
curl http://localhost:5001/api/process/550e8400-e29b-41d4-a716-446655440000/status
```

### Get Results

**GET** `/results/<session_id>`

Get complete processing results for a session.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "completed",
  "summary": {
    "overlays_created": 5,
    "analyses_completed": 5,
    "total_time": 45.2
  },
  "changes": [
    {
      "drawing_name": "A-101",
      "critical_change": "Room 101 extended by 10 feet",
      "changes_found": [
        "Room 101 extended from 200 to 300 sq ft",
        "Door relocated from north to west wall"
      ],
      "recommendations": [
        "Update foundation plans",
        "Coordinate with structural engineer"
      ],
      "overlay_url": "/api/files/550e8400.../A-101_overlay.png"
    }
  ]
}
```

**Example:**
```bash
curl http://localhost:5001/results/550e8400-e29b-41d4-a716-446655440000
```

### Get Changes Data

**GET** `/api/changes/<session_id>`

Get structured changes data for a session.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "changes": [
    {
      "drawing_name": "A-101",
      "critical_change": "...",
      "changes_found": [...],
      "analysis_summary": "...",
      "recommendations": [...],
      "overlay_path": "sessions/.../A-101_overlay.png",
      "old_image_path": "sessions/.../A-101_old.png",
      "new_image_path": "sessions/.../A-101_new.png"
    }
  ],
  "total_changes": 5
}
```

**Example:**
```bash
curl http://localhost:5001/api/changes/550e8400-e29b-41d4-a716-446655440000
```

### Get Session Summary

**GET** `/api/session/<session_id>/summary`

Get aggregated summary across all drawings in a session.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "total_pages": 5,
  "total_analyses": 5,
  "successful_analyses": 5,
  "failed_analyses": 0,
  "critical_changes": [
    {
      "drawing_name": "A-101",
      "critical_change": "..."
    }
  ],
  "all_changes": [...],
  "all_recommendations": [...],
  "overall_summary": "..."
}
```

**Example:**
```bash
curl http://localhost:5001/api/session/550e8400-e29b-41d4-a716-446655440000/summary
```

---

## Chat Endpoints

### Send Chat Message

**POST** `/api/chat/<session_id>`

Send a message to the AI chatbot.

**Request:**
```json
{
  "message": "What are the typical costs for these changes?"
}
```

**Response:**
```json
{
  "response": "Based on the changes detected in your drawings...",
  "timestamp": "2024-01-15T10:40:00Z"
}
```

**Example:**
```bash
curl -X POST http://localhost:5001/api/chat/550e8400-e29b-41d4-a716-446655440000 \
  -H "Content-Type: application/json" \
  -d '{"message": "What are the typical costs for these changes?"}'
```

### Get Suggested Questions

**GET** `/api/chat/<session_id>/suggested`

Get context-aware suggested questions.

**Response:**
```json
{
  "questions": [
    "What are the typical costs for these changes?",
    "How long should this project take to complete?",
    "What permits might be required for these modifications?",
    "Are there any safety considerations I should be aware of?",
    "What's the best sequence for implementing these changes?"
  ]
}
```

**Example:**
```bash
curl http://localhost:5001/api/chat/550e8400-e29b-41d4-a716-446655440000/suggested
```

### Get Chat History

**GET** `/api/chat/<session_id>/history`

Get conversation history for a session.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "messages": [
    {
      "role": "user",
      "content": "What are the typical costs?",
      "timestamp": "2024-01-15T10:40:00Z"
    },
    {
      "role": "assistant",
      "content": "Based on the changes...",
      "timestamp": "2024-01-15T10:40:05Z"
    }
  ],
  "total_messages": 2
}
```

**Example:**
```bash
curl http://localhost:5001/api/chat/550e8400-e29b-41d4-a716-446655440000/history
```

---

## File Endpoints

### Get File

**GET** `/api/files/<session_id>/<filename>`

Download a file from a session.

**Query Parameters:**
- `type`: File type (`overlay`, `old`, `new`, `analysis`)

**Response:** File content (binary)

**Example:**
```bash
curl http://localhost:5001/api/files/550e8400-e29b-41d4-a716-446655440000/A-101_overlay.png
```

### Get Signed URL

**GET** `/api/files/<session_id>/<filename>/url`

Get a signed URL for temporary file access (GCS only).

**Query Parameters:**
- `expiration`: Expiration time in minutes (default: 60)

**Response:**
```json
{
  "url": "https://storage.googleapis.com/...",
  "expires_at": "2024-01-15T11:40:00Z"
}
```

**Example:**
```bash
curl http://localhost:5001/api/files/550e8400-e29b-41d4-a716-446655440000/A-101_overlay.png/url?expiration=120
```

### List Session Files

**GET** `/api/files/<session_id>`

List all files in a session.

**Response:**
```json
{
  "session_id": "550e8400-e29b-41d4-a716-446655440000",
  "files": [
    {
      "filename": "A-101_overlay.png",
      "type": "overlay",
      "size": 1048576,
      "url": "/api/files/550e8400.../A-101_overlay.png"
    },
    {
      "filename": "A-101_old.png",
      "type": "old",
      "size": 2097152,
      "url": "/api/files/550e8400.../A-101_old.png"
    }
  ],
  "total_files": 15
}
```

**Example:**
```bash
curl http://localhost:5001/api/files/550e8400-e29b-41d4-a716-446655440000
```

---

## WebSocket Events

**Note**: WebSocket support is planned but not yet implemented.

### Planned Events

- `processing_started`: Processing begins
- `processing_progress`: Progress update
- `processing_completed`: Processing finished
- `processing_error`: Processing failed

---

## Rate Limiting

**Current**: No rate limiting (to be implemented)

**Planned Limits:**
- Upload: 10 requests/minute
- Processing: 5 requests/minute
- Chat: 30 requests/minute
- General: 100 requests/minute

## CORS

**Current**: CORS not configured (all origins allowed)

**Production**: Should restrict to specific domains

---

## SDK Examples

### Python

```python
import requests

BASE_URL = "http://localhost:5001"

# Upload files
with open("old.pdf", "rb") as old_file, open("new.pdf", "rb") as new_file:
    files = {
        "old_file": old_file,
        "new_file": new_file
    }
    response = requests.post(f"{BASE_URL}/upload", files=files)
    session_id = response.json()["session_id"]

# Start processing
requests.post(f"{BASE_URL}/process/{session_id}")

# Check status
status = requests.get(f"{BASE_URL}/api/process/{session_id}/status").json()
print(f"Status: {status['status']}")

# Get results
results = requests.get(f"{BASE_URL}/results/{session_id}").json()
print(f"Overlays created: {results['summary']['overlays_created']}")
```

### JavaScript

```javascript
const BASE_URL = "http://localhost:5001";

// Upload files
const formData = new FormData();
formData.append("old_file", oldFile);
formData.append("new_file", newFile);

const uploadResponse = await fetch(`${BASE_URL}/upload`, {
  method: "POST",
  body: formData
});

const { session_id } = await uploadResponse.json();

// Start processing
await fetch(`${BASE_URL}/process/${session_id}`, {
  method: "POST"
});

// Poll for status
const checkStatus = async () => {
  const response = await fetch(`${BASE_URL}/api/process/${session_id}/status`);
  const status = await response.json();
  
  if (status.status === "completed") {
    const results = await fetch(`${BASE_URL}/results/${session_id}`);
    return await results.json();
  } else {
    setTimeout(checkStatus, 2000);
  }
};
```

### cURL

```bash
# Complete workflow
SESSION_ID=$(curl -X POST http://localhost:5001/upload \
  -F "old_file=@old.pdf" \
  -F "new_file=@new.pdf" | jq -r .session_id)

curl -X POST http://localhost:5001/process/$SESSION_ID

# Wait and check results
sleep 30
curl http://localhost:5001/results/$SESSION_ID
```

---

**Next Steps**: See [PIPELINE.md](./PIPELINE.md) for processing pipeline details or [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for API troubleshooting.

```


Continuing with the remaining documentation files:


```markdown
# BuildTrace Database Documentation

Complete database schema, models, relationships, and migration guide.

## Table of Contents

1. [Overview](#overview)
2. [Database Schema](#database-schema)
3. [Models](#models)
4. [Relationships](#relationships)
5. [Migrations](#migrations)
6. [Queries](#queries)
7. [Performance](#performance)

## Overview

BuildTrace uses **PostgreSQL** as the primary database (with file-based fallback for development). The database is managed through **SQLAlchemy 2.0** ORM.

### Database Modes

- **Production**: Cloud SQL PostgreSQL
- **Development**: Local PostgreSQL or file-based storage
- **Fallback**: JSON files (when `USE_DATABASE=false`)

### Connection Management

- **Production**: Unix socket connection to Cloud SQL
- **Development**: TCP connection to local PostgreSQL
- **Cloud SQL Proxy**: For local development with Cloud SQL

## Database Schema

### Entity Relationship Diagram

```mermaid
erDiagram
    User ||--o{ Project : has
    User ||--o{ Session : creates
    Project ||--o{ Session : contains
    Project ||--o{ DrawingVersion : tracks
    Session ||--o{ Drawing : contains
    Session ||--o{ Comparison : generates
    Session ||--o{ ChatConversation : has
    Session ||--o{ ProcessingJob : processes
    Comparison ||--o{ AnalysisResult : produces
    ChatConversation ||--o{ ChatMessage : contains
    Drawing ||--o{ Comparison : old_drawing
    Drawing ||--o{ Comparison : new_drawing
    Drawing ||--o{ DrawingVersion : references
```

### Tables Overview

| Table | Purpose | Key Fields |
|-------|---------|------------|
| `users` | User accounts | id, email, name, role |
| `projects` | Project organization | id, user_id, name, status |
| `sessions` | Processing sessions | id, user_id, project_id, status |
| `drawings` | Drawing files | id, session_id, drawing_type, filename |
| `drawing_versions` | Version tracking | id, project_id, drawing_id, version_number |
| `comparisons` | Drawing comparisons | id, session_id, old_drawing_id, new_drawing_id |
| `analysis_results` | AI analysis results | id, comparison_id, changes_found |
| `chat_conversations` | Chat sessions | id, session_id |
| `chat_messages` | Chat messages | id, conversation_id, role, content |
| `processing_jobs` | Background jobs | id, session_id, job_type, status |

## Models

### User Model

```python
class User(Base):
    __tablename__ = 'users'
    
    id = Column(String(36), primary_key=True)
    email = Column(String(255), unique=True, nullable=False)
    name = Column(String(255))
    company = Column(String(255))
    role = Column(String(100))  # architect, engineer, contractor, owner
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    is_active = Column(Boolean, default=True)
```

**Relationships:**
- `projects`: One-to-many with Project
- `sessions`: One-to-many with Session

### Project Model

```python
class Project(Base):
    __tablename__ = 'projects'
    
    id = Column(String(36), primary_key=True)
    user_id = Column(String(36), ForeignKey('users.id', ondelete='CASCADE'))
    name = Column(String(255), nullable=False)
    description = Column(Text)
    project_number = Column(String(100))
    client_name = Column(String(255))
    location = Column(String(255))
    status = Column(String(50), default='active')  # active, archived, completed
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

**Relationships:**
- `user`: Many-to-one with User
- `sessions`: One-to-many with Session
- `drawing_versions`: One-to-many with DrawingVersion

**Indexes:**
- `idx_user_project`: (user_id, name)

### Session Model

```python
class Session(Base):
    __tablename__ = 'sessions'
    
    id = Column(String(36), primary_key=True)
    user_id = Column(String(36), ForeignKey('users.id'), nullable=True)
    project_id = Column(String(36), ForeignKey('projects.id'), nullable=True)
    session_type = Column(String(50), default='comparison')
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
    status = Column(String(50), default='active')  # active, processing, completed, error
    total_time = Column(Float)  # Processing time in seconds
    session_metadata = Column(JSON)  # Additional metadata
```

**Relationships:**
- `user`: Many-to-one with User (nullable)
- `project`: Many-to-one with Project (nullable)
- `drawings`: One-to-many with Drawing
- `comparisons`: One-to-many with Comparison
- `chat_conversations`: One-to-many with ChatConversation
- `processing_jobs`: One-to-many with ProcessingJob

**Status Values:**
- `active`: Session created, ready for processing
- `processing`: Currently processing
- `completed`: Processing finished successfully
- `error`: Processing failed

### Drawing Model

```python
class Drawing(Base):
    __tablename__ = 'drawings'
    
    id = Column(String(36), primary_key=True)
    session_id = Column(String(36), ForeignKey('sessions.id', ondelete='CASCADE'))
    drawing_type = Column(String(20), nullable=False)  # 'old' or 'new'
    filename = Column(String(255), nullable=False)
    original_filename = Column(String(255), nullable=False)
    storage_path = Column(Text)  # GCS path or local path
    drawing_name = Column(String(100))  # Extracted identifier (e.g., A-101)
    page_number = Column(Integer)
    processed_at = Column(DateTime, default=datetime.utcnow)
    drawing_metadata = Column(JSON)  # Dimensions, scale, etc.
```

**Relationships:**
- `session`: Many-to-one with Session

**Indexes:**
- `idx_session_drawing`: (session_id, drawing_name)
- `unique_session_drawing_type`: (session_id, drawing_name, drawing_type) - Unique constraint

### Comparison Model

```python
class Comparison(Base):
    __tablename__ = 'comparisons'
    
    id = Column(String(36), primary_key=True)
    session_id = Column(String(36), ForeignKey('sessions.id', ondelete='CASCADE'))
    old_drawing_id = Column(String(36), ForeignKey('drawings.id'))
    new_drawing_id = Column(String(36), ForeignKey('drawings.id'))
    drawing_name = Column(String(100), nullable=False)
    overlay_path = Column(Text)  # GCS path to overlay image
    old_image_path = Column(Text)  # GCS path to processed old image
    new_image_path = Column(Text)  # GCS path to processed new image
    alignment_score = Column(Float)  # Quality of alignment (0-1)
    changes_detected = Column(Boolean, default=False)
    created_at = Column(DateTime, default=datetime.utcnow)
```

**Relationships:**
- `session`: Many-to-one with Session
- `old_drawing`: Many-to-one with Drawing
- `new_drawing`: Many-to-one with Drawing
- `analysis_results`: One-to-many with AnalysisResult

**Constraints:**
- `unique_session_comparison`: (session_id, drawing_name) - Unique constraint

### AnalysisResult Model

```python
class AnalysisResult(Base):
    __tablename__ = 'analysis_results'
    
    id = Column(String(36), primary_key=True)
    comparison_id = Column(String(36), ForeignKey('comparisons.id', ondelete='CASCADE'))
    session_id = Column(String(36), ForeignKey('sessions.id', ondelete='CASCADE'))
    drawing_name = Column(String(100), nullable=False)
    changes_found = Column(JSON)  # List of changes
    critical_change = Column(Text)
    analysis_summary = Column(Text)
    recommendations = Column(JSON)  # List of recommendations
    success = Column(Boolean, default=True)
    error_message = Column(Text, nullable=True)
    ai_model_used = Column(String(50), default='gpt-4-vision-preview')
    created_at = Column(DateTime, default=datetime.utcnow)
```

**Relationships:**
- `comparison`: Many-to-one with Comparison

**Constraints:**
- `unique_session_analysis`: (session_id, drawing_name) - Unique constraint

**JSON Fields:**
- `changes_found`: Array of change descriptions
- `recommendations`: Array of recommendation strings

### ChatConversation Model

```python
class ChatConversation(Base):
    __tablename__ = 'chat_conversations'
    
    id = Column(String(36), primary_key=True)
    session_id = Column(String(36), ForeignKey('sessions.id', ondelete='CASCADE'))
    created_at = Column(DateTime, default=datetime.utcnow)
    updated_at = Column(DateTime, default=datetime.utcnow, onupdate=datetime.utcnow)
```

**Relationships:**
- `session`: Many-to-one with Session
- `messages`: One-to-many with ChatMessage

### ChatMessage Model

```python
class ChatMessage(Base):
    __tablename__ = 'chat_messages'
    
    id = Column(String(36), primary_key=True)
    conversation_id = Column(String(36), ForeignKey('chat_conversations.id', ondelete='CASCADE'))
    role = Column(String(20), nullable=False)  # 'user', 'assistant', 'system'
    content = Column(Text, nullable=False)
    timestamp = Column(DateTime, default=datetime.utcnow)
    message_metadata = Column(JSON)  # Token usage, model version, etc.
```

**Relationships:**
- `conversation`: Many-to-one with ChatConversation

### ProcessingJob Model

```python
class ProcessingJob(Base):
    __tablename__ = 'processing_jobs'
    
    id = Column(String(36), primary_key=True)
    session_id = Column(String(36), ForeignKey('sessions.id', ondelete='CASCADE'))
    job_type = Column(String(50), nullable=False)  # 'pdf_extraction', 'comparison', 'ai_analysis'
    status = Column(String(20), default='pending')  # pending, running, completed, failed
    started_at = Column(DateTime)
    completed_at = Column(DateTime)
    error_message = Column(Text)
    job_metadata = Column(JSON)
    created_at = Column(DateTime, default=datetime.utcnow)
```

**Relationships:**
- `session`: Many-to-one with Session

**Status Values:**
- `pending`: Job queued, not started
- `running`: Job currently executing
- `completed`: Job finished successfully
- `failed`: Job failed with error

## Relationships

### Cascade Deletes

- **User â†’ Projects**: CASCADE (deleting user deletes projects)
- **Project â†’ Sessions**: CASCADE
- **Session â†’ Drawings**: CASCADE
- **Session â†’ Comparisons**: CASCADE
- **Session â†’ ChatConversations**: CASCADE
- **Session â†’ ProcessingJobs**: CASCADE
- **Comparison â†’ AnalysisResults**: CASCADE
- **ChatConversation â†’ ChatMessages**: CASCADE

### Foreign Key Constraints

All foreign keys use `ondelete='CASCADE'` to ensure data consistency when parent records are deleted.

## Migrations

### Initialization

```bash
# Initialize database tables
python -c "from gcp.database import init_db; init_db()"
```

### Using Alembic (Recommended)

```bash
# Initialize Alembic (first time)
alembic init alembic

# Create migration
alembic revision --autogenerate -m "Description of changes"

# Apply migration
alembic upgrade head

# Rollback migration
alembic downgrade -1
```

### Manual Migrations

Migration scripts are located in `gcp/database/migrations/`:

- `init_database.py`: Initial schema creation
- `add_total_time_migration.py`: Add total_time to sessions
- `add_session_id_migration.sql`: SQL migration example

### Migration Best Practices

1. **Always backup** before running migrations in production
2. **Test migrations** in development first
3. **Use transactions** for multi-step migrations
4. **Document breaking changes** in migration comments
5. **Version control** all migration files

## Queries

### Common Query Patterns

#### Get Session with All Data

```python
from gcp.database import get_db_session
from gcp.database.models import Session, Drawing, Comparison, AnalysisResult

with get_db_session() as db:
    session = db.query(Session).filter_by(id=session_id).first()
    
    # Eager load relationships
    drawings = db.query(Drawing).filter_by(session_id=session_id).all()
    comparisons = db.query(Comparison).filter_by(session_id=session_id).all()
    analyses = db.query(AnalysisResult).filter_by(session_id=session_id).all()
```

#### Get Recent Sessions

```python
from sqlalchemy import desc

recent_sessions = db.query(Session)\
    .order_by(desc(Session.created_at))\
    .limit(10)\
    .all()
```

#### Get Analysis Results for Drawing

```python
analysis = db.query(AnalysisResult)\
    .filter_by(session_id=session_id, drawing_name="A-101")\
    .first()
```

#### Count Comparisons by Status

```python
from sqlalchemy import func

status_counts = db.query(
    Comparison.changes_detected,
    func.count(Comparison.id)
).filter_by(session_id=session_id)\
 .group_by(Comparison.changes_detected)\
 .all()
```

#### Get Chat History

```python
conversation = db.query(ChatConversation)\
    .filter_by(session_id=session_id)\
    .first()

if conversation:
    messages = db.query(ChatMessage)\
        .filter_by(conversation_id=conversation.id)\
        .order_by(ChatMessage.timestamp)\
        .all()
```

### Complex Queries

#### Get Session Summary

```python
from sqlalchemy import func, case

summary = db.query(
    Session.id,
    func.count(Comparison.id).label('total_comparisons'),
    func.count(case((AnalysisResult.success == True, 1))).label('successful_analyses'),
    func.avg(Comparison.alignment_score).label('avg_alignment_score')
).join(Comparison, Session.id == Comparison.session_id)\
 .outerjoin(AnalysisResult, Comparison.id == AnalysisResult.comparison_id)\
 .filter_by(id=session_id)\
 .group_by(Session.id)\
 .first()
```

## Performance

### Indexes

**Existing Indexes:**
- `idx_user_project`: (user_id, name) on projects
- `idx_project_drawing_version`: (project_id, drawing_name, version_number) on drawing_versions
- `idx_session_drawing`: (session_id, drawing_name) on drawings
- `unique_session_drawing_type`: (session_id, drawing_name, drawing_type) on drawings
- `unique_session_comparison`: (session_id, drawing_name) on comparisons
- `unique_session_analysis`: (session_id, drawing_name) on analysis_results

### Recommended Additional Indexes

```sql
-- For session queries
CREATE INDEX idx_session_status ON sessions(status);
CREATE INDEX idx_session_created_at ON sessions(created_at DESC);

-- For drawing queries
CREATE INDEX idx_drawing_type ON drawings(drawing_type);
CREATE INDEX idx_drawing_name ON drawings(drawing_name);

-- For comparison queries
CREATE INDEX idx_comparison_changes ON comparisons(changes_detected);

-- For analysis queries
CREATE INDEX idx_analysis_success ON analysis_results(success);
CREATE INDEX idx_analysis_created_at ON analysis_results(created_at DESC);
```

### Query Optimization

1. **Use Eager Loading**: Load relationships in single query
   ```python
   from sqlalchemy.orm import joinedload
   session = db.query(Session)\
       .options(joinedload(Session.drawings))\
       .filter_by(id=session_id)\
       .first()
   ```

2. **Limit Results**: Always use `.limit()` for list queries
   ```python
   sessions = db.query(Session)\
       .order_by(desc(Session.created_at))\
       .limit(50)\
       .all()
   ```

3. **Use Select Specific Columns**: Don't load unnecessary data
   ```python
   session_ids = db.query(Session.id)\
       .filter_by(status='completed')\
       .all()
   ```

4. **Connection Pooling**: Configured in `database.py`
   - Production: pool_size=5, max_overflow=10
   - Development: Default pool settings

### Monitoring

**Query Performance:**
```python
# Enable query logging
import logging
logging.getLogger('sqlalchemy.engine').setLevel(logging.INFO)
```

**Slow Query Detection:**
- Monitor query execution time
- Use EXPLAIN ANALYZE for complex queries
- Set up database performance monitoring

---

**Next Steps**: See [PIPELINE.md](./PIPELINE.md) for processing pipeline or [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for database issues.

```

```markdown
# BuildTrace Processing Pipeline

Complete documentation of the drawing comparison and analysis pipeline, including algorithms, data flow, and processing stages.

## Table of Contents

1. [Overview](#overview)
2. [Pipeline Stages](#pipeline-stages)
3. [Algorithms](#algorithms)
4. [Data Flow](#data-flow)
5. [Error Handling](#error-handling)
6. [Performance Optimization](#performance-optimization)

## Overview

The BuildTrace processing pipeline transforms uploaded drawing files into comprehensive change analysis reports through multiple stages:

```mermaid
graph LR
    A[PDF Upload] --> B[PDF Parsing]
    B --> C[Drawing Name Extraction]
    C --> D[PDF to PNG Conversion]
    D --> E[Drawing Matching]
    E --> F[Feature Detection]
    F --> G[Alignment]
    G --> H[Overlay Generation]
    H --> I[AI Analysis]
    I --> J[Results Storage]
```

**Total Processing Time**: 30-120 seconds per drawing pair (depending on complexity)

## Pipeline Stages

### Stage 1: PDF Parsing

**Module**: `pdf_parser.py`

**Purpose**: Extract pages and metadata from PDF files

**Process:**
1. Open PDF using PyMuPDF (fitz)
2. Count total pages
3. Extract page dimensions
4. Create output directory structure

**Output:**
- Page count
- Page dimensions
- Directory structure for output

**Code Example:**
```python
import fitz  # PyMuPDF

doc = fitz.open(pdf_path)
page_count = len(doc)
for page_num in range(page_count):
    page = doc[page_num]
    dimensions = page.rect
```

### Stage 2: Drawing Name Extraction

**Module**: `extract_drawing.py`

**Purpose**: Extract drawing identifiers (e.g., A-101, S-12A) from each page

**Process:**
1. **Text Extraction** (Primary Method)
   - Use PyMuPDF to extract text with positions
   - Filter text in bottom-right region (typical location for drawing names)
   - Pattern matching for drawing name formats

2. **OCR Fallback** (If no text found)
   - Extract bottom-right region as image
   - Use Tesseract OCR to read text
   - Pattern matching for drawing names

**Pattern Matching:**
- Supports formats: `A-101`, `A-102`, `A-344-MB`, `S-12A`, `E-201`, etc.
- Regex pattern: `[A-Z]-\d+[A-Z]?(-[A-Z]+)?`

**Code Example:**
```python
def extract_drawing_name(page):
    # Try text extraction first
    text_blocks = page.get_text("blocks")
    for block in text_blocks:
        if is_bottom_right(block):
            name = match_drawing_pattern(block[4])
            if name:
                return name
    
    # Fallback to OCR
    return ocr_extract_name(page)
```

**Output:**
- Dictionary mapping page numbers to drawing names
- Example: `{1: "A-101", 2: "A-102", 3: "A-103"}`

### Stage 3: PDF to PNG Conversion

**Module**: `pdf_to_png.py`

**Purpose**: Convert PDF pages to high-resolution PNG images

**Process:**
1. Load PDF using pdf2image
2. Convert each page to PNG at specified DPI (default: 300)
3. Save with drawing name as filename (if extracted) or fallback naming

**Configuration:**
- **DPI**: 300 (default), configurable up to 600
- **Format**: PNG (lossless)
- **Color Mode**: RGB

**Code Example:**
```python
from pdf2image import convert_from_path

images = convert_from_path(
    pdf_path,
    dpi=300,
    fmt='png'
)

for i, image in enumerate(images):
    image.save(f"{drawing_name}_{i+1}.png")
```

**Output:**
- PNG files for each page
- Named with drawing identifier or fallback: `{drawing_name}.png` or `{pdf_name}_page{N}.png`

### Stage 4: Drawing Matching

**Module**: `drawing_comparison.py`

**Purpose**: Match drawings between old and new sets by name

**Process:**
1. Extract drawing names from both sets
2. Find common drawing names
3. Identify drawings only in old set
4. Identify drawings only in new set
5. Create matched pairs

**Matching Logic:**
```python
old_drawings = {name: path for name, path in old_set}
new_drawings = {name: path for name, path in new_set}

matches = []
for name in old_drawings.keys() & new_drawings.keys():
    matches.append((name, old_drawings[name], new_drawings[name]))

only_old = old_drawings.keys() - new_drawings.keys()
only_new = new_drawings.keys() - old_drawings.keys()
```

**Output:**
- List of matched drawing pairs
- Lists of unmatched drawings (old-only, new-only)

### Stage 5: Feature Detection

**Module**: `align_drawings.py`

**Purpose**: Detect distinctive features in both images using SIFT

**Algorithm**: SIFT (Scale-Invariant Feature Transform)

**Why SIFT?**
- Robust to scale, rotation, and translation
- Good for technical drawings
- Handles architectural line drawings well

**Process:**
1. Convert images to grayscale
2. Detect keypoints and compute descriptors
3. Store keypoints and descriptors for matching

**Code Example:**
```python
import cv2

sift = cv2.SIFT_create()
gray_old = cv2.cvtColor(old_img, cv2.COLOR_BGR2GRAY)
gray_new = cv2.cvtColor(new_img, cv2.COLOR_BGR2GRAY)

keypoints_old, descriptors_old = sift.detectAndCompute(gray_old, None)
keypoints_new, descriptors_new = sift.detectAndCompute(gray_new, None)
```

**Output:**
- Keypoints: (x, y) coordinates with scale and orientation
- Descriptors: 128-dimensional feature vectors

### Stage 6: Feature Matching

**Module**: `align_drawings.py`

**Purpose**: Match features between old and new images

**Algorithm**: FLANN-based matcher with Lowe's ratio test

**Process:**
1. Use FLANN (Fast Library for Approximate Nearest Neighbors) matcher
2. Find k-nearest neighbors (k=2) for each descriptor
3. Apply Lowe's ratio test to filter good matches
4. Ratio threshold: 0.7 (configurable)

**Lowe's Ratio Test:**
- Compares distance to nearest neighbor vs. second nearest
- If ratio < threshold, match is considered good
- Filters ambiguous matches

**Code Example:**
```python
FLANN_INDEX_KDTREE = 1
index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)
search_params = dict(checks=50)
flann = cv2.FlannBasedMatcher(index_params, search_params)

matches = flann.knnMatch(descriptors_old, descriptors_new, k=2)

good_matches = []
for match_pair in matches:
    if len(match_pair) == 2:
        m, n = match_pair
        if m.distance < 0.7 * n.distance:
            good_matches.append(m)
```

**Output:**
- List of good feature matches
- Minimum 4 matches required for transformation estimation

### Stage 7: Alignment Transformation

**Module**: `align_drawings.py`

**Purpose**: Estimate and apply transformation to align drawings

**Algorithm**: RANSAC (Random Sample Consensus) for robust estimation

**Transformation Type**: Constrained Affine Transformation

**Constraints:**
- **Scale Preservation**: No arbitrary scaling (maintains drawing scale)
- **Translation**: Allowed (x, y shifts)
- **Rotation**: Allowed
- **Shear**: Minimal (handles slight perspective)

**Process:**
1. Extract matched point pairs
2. Use RANSAC to estimate affine transformation
3. Apply constraints to preserve scale
4. Validate transformation quality

**Code Example:**
```python
if len(good_matches) >= 4:
    src_pts = np.float32([keypoints_old[m.queryIdx].pt for m in good_matches])
    dst_pts = np.float32([keypoints_new[m.trainIdx].pt for m in good_matches])
    
    # Estimate affine transformation
    M, mask = cv2.estimateAffinePartial2D(
        src_pts, dst_pts,
        method=cv2.RANSAC,
        ransacReprojThreshold=5.0
    )
    
    # Apply transformation
    aligned_old = cv2.warpAffine(old_img, M, (new_img.shape[1], new_img.shape[0]))
```

**Validation:**
- Check transformation matrix quality
- Verify alignment score (0-1 scale)
- Minimum alignment score threshold: 0.5

**Output:**
- Transformation matrix (2x3 affine matrix)
- Aligned old image
- Alignment score

### Stage 8: Overlay Generation

**Module**: `layer_overlay_2d.py`

**Purpose**: Create visual overlay showing changes

**Process:**
1. Load aligned old image and new image
2. Create color-coded overlay:
   - **Red**: Old drawing (aligned)
   - **Green**: New drawing
   - **Overlap**: Blended transparency
3. Generate overlay image

**Code Example:**
```python
def create_overlay_image(aligned_old, new_img):
    # Create color channels
    overlay = np.zeros_like(new_img)
    
    # Red channel: old image
    overlay[:, :, 2] = aligned_old[:, :, 0]  # Red
    
    # Green channel: new image
    overlay[:, :, 1] = new_img[:, :, 1]  # Green
    
    # Blend with transparency
    alpha = 0.5
    result = cv2.addWeighted(overlay, alpha, new_img, 1-alpha, 0)
    
    return result
```

**Output:**
- Overlay PNG image showing changes
- Saved as: `{drawing_name}_overlay.png`

### Stage 9: AI Analysis

**Module**: `openai_change_analyzer.py`

**Purpose**: Analyze overlay images using GPT-4 Vision

**Process:**
1. Load overlay, old, and new images
2. Encode images as base64
3. Construct prompt with:
   - System instructions
   - Image context
   - Output format requirements
4. Call OpenAI GPT-4 Vision API
5. Parse structured response

**Prompt Structure:**
```
You are an expert construction project manager analyzing architectural drawing changes.

Analyze the provided overlay image showing changes between old (red) and new (green) drawings.

Provide:
1. List of all changes found
2. Most critical change
3. Construction impact analysis
4. Recommendations

Format output as JSON with specific structure.
```

**Code Example:**
```python
from openai import OpenAI

client = OpenAI(api_key=api_key)

response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": system_prompt},
        {
            "role": "user",
            "content": [
                {"type": "text", "text": user_prompt},
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{overlay_base64}"}}
            ]
        }
    ],
    max_tokens=2000
)
```

**Output Structure:**
```json
{
  "drawing_name": "A-101",
  "success": true,
  "changes_found": [
    "Room 101 extended from 200 to 300 sq ft",
    "Door relocated from north to west wall"
  ],
  "critical_change": "Major structural modification to Room 101",
  "analysis_summary": "Detailed analysis...",
  "recommendations": [
    "Update foundation plans",
    "Coordinate with structural engineer"
  ]
}
```

### Stage 10: Results Storage

**Module**: `complete_drawing_pipeline.py`

**Purpose**: Store all results in database and storage

**Process:**
1. Save overlay images to storage (GCS or local)
2. Save analysis JSON files
3. Store metadata in database (if enabled)
4. Create summary report

**Storage Structure:**
```
sessions/{session_id}/
â”œâ”€â”€ uploads/
â”‚   â”œâ”€â”€ old_drawings.pdf
â”‚   â””â”€â”€ new_drawings.pdf
â””â”€â”€ results/
    â”œâ”€â”€ A-101/
    â”‚   â”œâ”€â”€ A-101_old.png
    â”‚   â”œâ”€â”€ A-101_new.png
    â”‚   â”œâ”€â”€ A-101_overlay.png
    â”‚   â””â”€â”€ change_analysis_A-101.json
    â””â”€â”€ A-102/
        â””â”€â”€ ...
```

## Algorithms

### SIFT Feature Detection

**Scale-Invariant Feature Transform**

**Key Properties:**
- Scale invariance
- Rotation invariance
- Robust to illumination changes
- Good for line drawings

**Parameters:**
- `nfeatures`: Maximum number of features (default: 0 = unlimited)
- `contrastThreshold`: Contrast threshold (default: 0.04)
- `edgeThreshold`: Edge threshold (default: 10)
- `sigma`: Gaussian sigma (default: 1.6)

### RANSAC

**Random Sample Consensus**

**Purpose**: Robustly estimate transformation in presence of outliers

**Process:**
1. Randomly sample minimum points (4 for affine)
2. Estimate transformation
3. Count inliers (points within threshold)
4. Repeat N times
5. Return best transformation (most inliers)

**Parameters:**
- `ransacReprojThreshold`: Reprojection error threshold (default: 5.0 pixels)
- `maxIters`: Maximum iterations (default: 2000)
- `confidence`: Confidence level (default: 0.99)

### Affine Transformation

**Constrained Affine (Similarity Transformation)**

**Matrix Form:**
```
[s*cos(Î¸)  -s*sin(Î¸)  tx]
[s*sin(Î¸)   s*cos(Î¸)  ty]
[   0         0       1 ]
```

Where:
- `s`: Scale factor (constrained to be close to 1.0)
- `Î¸`: Rotation angle
- `tx, ty`: Translation

**Constraints Applied:**
- Scale preservation: `0.9 < s < 1.1`
- No arbitrary scaling

## Data Flow

### Complete Pipeline Flow

```mermaid
sequenceDiagram
    participant User
    participant Pipeline
    participant PDFParser
    participant NameExtractor
    participant Converter
    participant Matcher
    participant Aligner
    participant OverlayGen
    participant OpenAI
    participant Storage

    User->>Pipeline: Upload PDFs
    Pipeline->>PDFParser: Parse PDFs
    PDFParser->>NameExtractor: Extract names
    NameExtractor->>Converter: Convert to PNG
    Converter->>Matcher: Match drawings
    Matcher->>Aligner: Align pairs
    Aligner->>OverlayGen: Generate overlays
    OverlayGen->>OpenAI: Analyze changes
    OpenAI->>Storage: Save results
    Storage->>User: Return results
```

### Data Transformations

1. **PDF â†’ Text**: PyMuPDF text extraction
2. **PDF â†’ Image**: pdf2image conversion
3. **Image â†’ Features**: SIFT detection
4. **Features â†’ Matches**: FLANN matching
5. **Matches â†’ Transformation**: RANSAC estimation
6. **Transformation â†’ Aligned Image**: Affine warping
7. **Images â†’ Overlay**: Color blending
8. **Overlay â†’ Analysis**: GPT-4 Vision
9. **Analysis â†’ JSON**: Structured output

## Error Handling

### Stage-Level Error Handling

Each stage has specific error handling:

1. **PDF Parsing**: Handle corrupted PDFs, unsupported formats
2. **Name Extraction**: Fallback to OCR, handle missing names
3. **Conversion**: Handle memory errors, large files
4. **Matching**: Handle no matches, partial matches
5. **Alignment**: Handle insufficient features, poor alignment
6. **Overlay**: Handle dimension mismatches
7. **AI Analysis**: Handle API errors, rate limits, timeouts

### Error Recovery

- **Retry Logic**: Automatic retries for transient failures
- **Fallback Methods**: OCR fallback, alternative algorithms
- **Partial Results**: Continue processing even if some stages fail
- **Error Reporting**: Detailed error messages in results

### Common Errors

| Error | Cause | Solution |
|-------|------|----------|
| No features detected | Low quality image | Increase DPI, check image quality |
| Insufficient matches | Drawings too different | Check if correct drawings matched |
| Alignment failed | Poor feature matching | Try different images, check preprocessing |
| API timeout | Large image, slow API | Increase timeout, retry |
| Memory error | Large file | Use chunked processing |

## Performance Optimization

### Parallel Processing

**Current**: Sequential processing
**Future**: Parallel processing for multiple drawings

### Caching

- **Feature Descriptors**: Cache for repeated comparisons
- **Converted Images**: Cache PNG conversions
- **Analysis Results**: Cache AI analysis results

### Memory Management

- **Chunked Processing**: Process large PDFs in chunks
- **Image Resizing**: Resize before processing if too large
- **Memory Limits**: Monitor and enforce memory limits

### Optimization Strategies

1. **Early Exit**: Skip processing if alignment score too low
2. **Feature Reduction**: Limit number of features detected
3. **Image Compression**: Compress images before storage
4. **Batch Processing**: Process multiple drawings in batch

---

**Next Steps**: See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for pipeline issues or [API.md](./API.md) for API usage.

```


Continuing with the remaining documentation files:


```markdown
# BuildTrace Configuration Guide

Complete guide to configuration management, environment variables, feature flags, and deployment settings.

## Table of Contents

1. [Overview](#overview)
2. [Configuration System](#configuration-system)
3. [Environment Variables](#environment-variables)
4. [Feature Flags](#feature-flags)
5. [Environment-Specific Configs](#environment-specific-configs)
6. [Configuration Validation](#configuration-validation)

## Overview

BuildTrace uses a **hierarchical configuration system** that supports:
- Environment-based configuration
- Feature flags for gradual rollout
- Sensible defaults with override capability
- Validation on startup

**Configuration File**: `config.py`

## Configuration System

### Loading Order

Configuration is loaded in the following priority order (highest to lowest):

1. **Environment Variables** (highest priority)
2. `.env.{ENVIRONMENT}` file (e.g., `.env.production`)
3. `.env` file (fallback)
4. **Default Values** (lowest priority)

### Configuration Class

The `Config` class in `config.py` centralizes all configuration:

```python
from config import config

# Access configuration
print(config.ENVIRONMENT)
print(config.USE_DATABASE)
print(config.OPENAI_API_KEY)
```

## Environment Variables

### Core Application Settings

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `ENVIRONMENT` | string | `development` | Environment name (development/production/staging) |
| `DEBUG` | boolean | `true` (dev) / `false` (prod) | Enable debug mode |
| `APP_NAME` | string | `BuildTrace` | Application name |
| `SECRET_KEY` | string | `dev-secret-key...` | Flask secret key (required in production) |
| `HOST` | string | `0.0.0.0` | Bind host |
| `PORT` | integer | `5001` (dev) / `8080` (prod) | Bind port |

### Database Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `USE_DATABASE` | boolean | `false` (dev) / `true` (prod) | Enable database mode |
| `DB_HOST` | string | `localhost` | Database host |
| `DB_PORT` | integer | `5432` | Database port |
| `DB_USER` | string | `buildtrace_user` | Database user |
| `DB_PASS` | string | - | Database password (required if USE_DATABASE=true) |
| `DB_NAME` | string | `buildtrace_db` | Database name |
| `INSTANCE_CONNECTION_NAME` | string | - | Cloud SQL instance connection name |

**Database URL Construction:**
- **Production (Cloud SQL)**: `postgresql://{user}:{pass}@/{db}?host=/cloudsql/{instance}`
- **Development (Local)**: `postgresql://{user}:{pass}@{host}:{port}/{db}`
- **Cloud SQL Proxy**: `postgresql://{user}:{pass}@127.0.0.1:5432/{db}`

### Storage Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `USE_GCS` | boolean | `false` (dev) / `true` (prod) | Use Google Cloud Storage |
| `GCS_BUCKET_NAME` | string | `buildtrace-storage` | GCS bucket name |
| `GCS_UPLOAD_BUCKET` | string | `buildtrace-drawings-upload` | Upload bucket (if different) |
| `GCS_PROCESSED_BUCKET` | string | `buildtrace-drawings-processed` | Processed files bucket |
| `LOCAL_UPLOAD_PATH` | string | `uploads` | Local upload directory |
| `LOCAL_RESULTS_PATH` | string | `results` | Local results directory |
| `LOCAL_TEMP_PATH` | string | `temp` | Local temp directory |

### OpenAI Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `OPENAI_API_KEY` | string | - | OpenAI API key (required) |
| `OPENAI_MODEL` | string | `gpt-4o` | Model to use |
| `USE_AI_ANALYSIS` | boolean | `true` | Enable AI analysis |

### Processing Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `DEFAULT_DPI` | integer | `300` | Default DPI for PDF conversion |
| `MAX_CONTENT_LENGTH` | integer | `73400320` (70MB) | Max upload size in bytes |
| `PROCESSING_TIMEOUT` | integer | `3600` | Processing timeout in seconds |
| `MAX_SYNC_PAGES` | integer | `10` | Max pages for synchronous processing |
| `MEMORY_LIMIT_GB` | float | `10.0` (dev) / `25.0` (prod) | Memory limit for processing |

### Async Processing Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `USE_ASYNC_PROCESSING` | boolean | `false` | Enable async processing |
| `USE_BACKGROUND_PROCESSING` | boolean | `true` | Use background job processor |
| `CLOUD_TASKS_QUEUE` | string | `buildtrace-processing-queue` | Cloud Tasks queue name |
| `CLOUD_TASKS_LOCATION` | string | `us-central1` | Cloud Tasks location |
| `CLOUD_TASKS_PROJECT` | string | `buildtrace` | GCP project for Cloud Tasks |
| `WORKER_URL` | string | - | Worker service URL |

### Security Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `ALLOWED_EXTENSIONS` | set | `{'pdf', 'dwg', 'dxf', 'png', 'jpg', 'jpeg'}` | Allowed file extensions |
| `MAX_UPLOAD_SIZE_MB` | integer | `70` | Max upload size in MB |

### Session Configuration

| Variable | Type | Default | Description |
|----------|------|---------|-------------|
| `SESSION_LIFETIME_HOURS` | integer | `24` | Session lifetime in hours |
| `CLEANUP_OLD_SESSIONS` | boolean | `true` | Enable automatic cleanup |

## Feature Flags

Feature flags allow gradual rollout and A/B testing:

### Database Feature Flag

```python
USE_DATABASE=true  # Enable database mode
USE_DATABASE=false # Use file-based storage
```

**When Enabled:**
- Sessions stored in PostgreSQL
- Full relational data model
- Better query capabilities
- Scalable architecture

**When Disabled:**
- Sessions stored as JSON files
- Simpler setup for development
- No database required

### GCS Feature Flag

```python
USE_GCS=true   # Use Google Cloud Storage
USE_GCS=false  # Use local file system
```

**When Enabled:**
- Files stored in GCS buckets
- Scalable storage
- Signed URLs for access
- Production-ready

**When Disabled:**
- Files stored locally
- Simpler for development
- No GCS setup required

### Async Processing Flag

```python
USE_ASYNC_PROCESSING=true   # Use background jobs
USE_ASYNC_PROCESSING=false  # Process synchronously
```

**When Enabled:**
- Large files processed in background
- Better user experience
- Requires database
- Uses Cloud Tasks or job processor

**When Disabled:**
- Processing happens in request
- Simpler architecture
- May timeout on large files

### AI Analysis Flag

```python
USE_AI_ANALYSIS=true   # Enable AI analysis
USE_AI_ANALYSIS=false  # Skip AI analysis
```

**When Enabled:**
- GPT-4 analyzes changes
- Generates recommendations
- Requires OpenAI API key

**When Disabled:**
- Skips AI analysis step
- Faster processing
- No API costs

### Firebase Auth Flag

```python
USE_FIREBASE_AUTH=true   # Enable Firebase authentication
USE_FIREBASE_AUTH=false  # No authentication (current)
```

**When Enabled:**
- User authentication required
- User-specific sessions
- Project management

**When Disabled:**
- Public access
- Anonymous sessions
- Simpler setup

## Environment-Specific Configs

### Development Environment

**File**: `.env` or `.env.development`

```bash
ENVIRONMENT=development
DEBUG=true
USE_DATABASE=false
USE_GCS=false
USE_ASYNC_PROCESSING=false
USE_FIREBASE_AUTH=false

# Local database (if using)
DB_HOST=localhost
DB_PORT=5432
DB_USER=postgres
DB_PASS=postgres
DB_NAME=buildtrace_db

# Local storage
LOCAL_UPLOAD_PATH=uploads
LOCAL_RESULTS_PATH=results
LOCAL_TEMP_PATH=temp

# OpenAI (required)
OPENAI_API_KEY=sk-...

# Application
HOST=0.0.0.0
PORT=5001
SECRET_KEY=dev-secret-key-change-in-production
```

### Production Environment

**File**: `.env.production` (not committed to git)

```bash
ENVIRONMENT=production
DEBUG=false
USE_DATABASE=true
USE_GCS=true
USE_ASYNC_PROCESSING=true
USE_BACKGROUND_PROCESSING=true

# Cloud SQL
INSTANCE_CONNECTION_NAME=buildtrace:us-central1:buildtrace-postgres
DB_USER=buildtrace_user
DB_PASS=<encrypted-password>
DB_NAME=buildtrace_db

# Cloud Storage
GCS_BUCKET_NAME=buildtrace-storage
GCS_UPLOAD_BUCKET=buildtrace-drawings-upload
GCS_PROCESSED_BUCKET=buildtrace-drawings-processed

# OpenAI
OPENAI_API_KEY=<encrypted-key>
OPENAI_MODEL=gpt-4o
USE_AI_ANALYSIS=true

# Application
HOST=0.0.0.0
PORT=8080
SECRET_KEY=<strong-random-secret>

# Processing
DEFAULT_DPI=300
MAX_CONTENT_LENGTH=73400320
PROCESSING_TIMEOUT=3600
MEMORY_LIMIT_GB=25.0

# Cloud Tasks (if using)
CLOUD_TASKS_QUEUE=buildtrace-processing-queue
CLOUD_TASKS_LOCATION=us-central1
CLOUD_TASKS_PROJECT=buildtrace
```

### Staging Environment

**File**: `.env.staging`

```bash
ENVIRONMENT=staging
DEBUG=false
USE_DATABASE=true
USE_GCS=true

# Similar to production but with test data
# Use separate Cloud SQL instance
# Use separate GCS bucket
```

## Configuration Validation

### Automatic Validation

The `Config` class validates configuration on import (production only):

```python
if config.IS_PRODUCTION and not config.validate():
    raise RuntimeError("Invalid configuration for production environment")
```

### Validation Rules

**Production Requirements:**
- `OPENAI_API_KEY` must be set
- `DB_PASS` required if `USE_DATABASE=true`
- `GCS_BUCKET_NAME` required if `USE_GCS=true`
- `SECRET_KEY` must not be default value

**Conflicts:**
- `USE_ASYNC_PROCESSING=true` requires `USE_DATABASE=true`
- Cannot use both local and GCS storage simultaneously

### Manual Validation

```python
from config import config

# Check if configuration is valid
if config.validate():
    print("Configuration is valid")
else:
    print("Configuration has errors")
```

## Configuration Access

### In Application Code

```python
from config import config

# Access configuration
if config.USE_DATABASE:
    # Use database
    pass

if config.USE_GCS:
    # Use GCS
    pass
```

### Get Storage Config

```python
storage_config = config.get_storage_config()
# Returns: {'type': 'gcs', 'bucket_name': '...'} or {'type': 'local', 'upload_path': '...'}
```

### Get Data Config

```python
data_config = config.get_data_config()
# Returns: {'type': 'database', 'url': '...'} or {'type': 'file', 'data_dir': '...'}
```

## Environment Variable Examples

### Local Development

```bash
# .env file
ENVIRONMENT=development
DEBUG=true
USE_DATABASE=false
USE_GCS=false
OPENAI_API_KEY=sk-your-key-here
PORT=5001
```

### Production (Cloud Run)

Set via Cloud Run environment variables:

```bash
gcloud run services update buildtrace-overlay \
  --set-env-vars \
    ENVIRONMENT=production,\
    USE_DATABASE=true,\
    USE_GCS=true,\
    INSTANCE_CONNECTION_NAME=buildtrace:us-central1:buildtrace-postgres,\
    DB_USER=buildtrace_user,\
    DB_NAME=buildtrace_db,\
    GCS_BUCKET_NAME=buildtrace-storage,\
    OPENAI_API_KEY=sk-...
```

### Docker Compose

```yaml
# docker-compose.yml
services:
  app:
    environment:
      - ENVIRONMENT=development
      - USE_DATABASE=true
      - DB_HOST=postgres
      - DB_USER=buildtrace_user
      - DB_PASS=password
      - OPENAI_API_KEY=${OPENAI_API_KEY}
```

## Secrets Management

### Development

Store in `.env` file (not committed to git):
```bash
OPENAI_API_KEY=sk-...
DB_PASS=password
```

### Production (Cloud Run)

Use Cloud Run secrets:
```bash
# Create secret
echo -n "sk-..." | gcloud secrets create openai-api-key --data-file=-

# Grant access
gcloud secrets add-iam-policy-binding openai-api-key \
  --member="serviceAccount:PROJECT_NUMBER-compute@developer.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"

# Use in Cloud Run
gcloud run services update buildtrace-overlay \
  --update-secrets OPENAI_API_KEY=openai-api-key:latest
```

## Configuration Best Practices

1. **Never Commit Secrets**: Use environment variables or secrets management
2. **Use Feature Flags**: Enable features gradually
3. **Validate on Startup**: Catch configuration errors early
4. **Document Defaults**: Keep defaults documented
5. **Environment-Specific**: Use different configs for dev/staging/prod
6. **Type Safety**: Use appropriate types (boolean, integer, string)
7. **Sensible Defaults**: Provide working defaults for development

## Troubleshooting Configuration

### Check Current Configuration

```python
from config import config
print(config)
# Output: <Config env=development db=False gcs=False async=False>
```

### Verify Environment Variables

```bash
# Check if variable is set
echo $OPENAI_API_KEY

# List all environment variables
env | grep BUILDTRACE
```

### Test Configuration Loading

```python
from config import config
print(f"Environment: {config.ENVIRONMENT}")
print(f"Database: {config.USE_DATABASE}")
print(f"GCS: {config.USE_GCS}")
print(f"OpenAI Key Set: {bool(config.OPENAI_API_KEY)}")
```

---

**Next Steps**: See [SETUP.md](./SETUP.md) for setup instructions or [DEPLOYMENT.md](./DEPLOYMENT.md) for deployment configuration.

```

```markdown
# BuildTrace Deployment Guide

Complete guide for deploying BuildTrace to production, including Cloud Run, Docker, and traditional server deployments.

## Table of Contents

1. [Overview](#overview)
2. [Google Cloud Run Deployment](#google-cloud-run-deployment)
3. [Docker Deployment](#docker-deployment)
4. [CI/CD Pipeline](#cicd-pipeline)
5. [Database Deployment](#database-deployment)
6. [Storage Setup](#storage-setup)
7. [Monitoring & Logging](#monitoring--logging)

## Overview

BuildTrace supports multiple deployment strategies:

1. **Google Cloud Run** (Recommended): Serverless container deployment
2. **Docker Compose**: Local or VM deployment
3. **Traditional Server**: Direct Python deployment

**Production Architecture:**
- **Web App**: Cloud Run (4GB RAM, 2 CPU)
- **Job Processor**: Cloud Run (32GB RAM, 4 CPU)
- **Database**: Cloud SQL PostgreSQL
- **Storage**: Google Cloud Storage

## Google Cloud Run Deployment

### Prerequisites

1. **Google Cloud Project**: Create or select project
2. **Enable APIs**:
```bash
gcloud services enable \
    cloudbuild.googleapis.com \
    run.googleapis.com \
    sqladmin.googleapis.com \
    storage-component.googleapis.com \
    artifactregistry.googleapis.com \
    cloudtasks.googleapis.com
```

3. **Authenticate**:
```bash
gcloud auth login
gcloud auth configure-docker us-central1-docker.pkg.dev
gcloud config set project buildtrace
```

### Step 1: Database Setup

See [DATABASE.md](./DATABASE.md) for detailed database setup.

**Quick Setup:**
```bash
# Create Cloud SQL instance
gcloud sql instances create buildtrace-postgres \
    --database-version=POSTGRES_14 \
    --tier=db-f1-micro \
    --region=us-central1 \
    --network=default

# Create database
gcloud sql databases create buildtrace_db \
    --instance=buildtrace-postgres

# Create user
gcloud sql users create buildtrace_user \
    --instance=buildtrace-postgres \
    --password=YOUR_SECURE_PASSWORD
```

### Step 2: Storage Setup

```bash
# Create storage bucket
gsutil mb -p buildtrace -c STANDARD -l us-central1 gs://buildtrace-storage

# Set bucket permissions (if needed)
gsutil iam ch allUsers:objectViewer gs://buildtrace-storage

# Set CORS (for direct browser uploads)
cat > cors.json << EOF
[
  {
    "origin": ["*"],
    "method": ["GET", "POST", "PUT"],
    "responseHeader": ["Content-Type"],
    "maxAgeSeconds": 3600
  }
]
EOF
gsutil cors set cors.json gs://buildtrace-storage
```

### Step 3: Artifact Registry Setup

```bash
# Create Artifact Registry repository
gcloud artifacts repositories create buildtrace-repo \
    --repository-format=docker \
    --location=us-central1 \
    --description="BuildTrace Docker images"
```

### Step 4: Deploy Using Cloud Build (Recommended)

**File**: `gcp/deployment/cloudbuild.yaml`

```bash
# Deploy everything
gcloud builds submit --config=gcp/deployment/cloudbuild.yaml .
```

**What This Does:**
1. Builds Docker image
2. Pushes to Artifact Registry
3. Deploys web app to Cloud Run
4. Deploys job processor to Cloud Run
5. Sets environment variables
6. Configures Cloud SQL connection

### Step 5: Manual Deployment

**Build and Push Image:**
```bash
# Build image
docker build -f gcp/deployment/Dockerfile \
    -t us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest .

# Push to Artifact Registry
docker push us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest
```

**Deploy Web App:**
```bash
gcloud run deploy buildtrace-overlay \
    --image us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest \
    --region us-central1 \
    --platform managed \
    --allow-unauthenticated \
    --memory 4Gi \
    --cpu 2 \
    --timeout 300 \
    --max-instances 10 \
    --min-instances 0 \
    --add-cloudsql-instances buildtrace:us-central1:buildtrace-postgres \
    --set-env-vars \
        ENVIRONMENT=production,\
        USE_DATABASE=true,\
        USE_GCS=true,\
        INSTANCE_CONNECTION_NAME=buildtrace:us-central1:buildtrace-postgres,\
        DB_USER=buildtrace_user,\
        DB_NAME=buildtrace_db,\
        GCS_BUCKET_NAME=buildtrace-storage,\
        OPENAI_API_KEY=sk-...
```

**Deploy Job Processor:**
```bash
gcloud run deploy buildtrace-job-processor \
    --image us-central1-docker.pkg.dev/buildtrace/buildtrace-repo/buildtrace-overlay:latest \
    --region us-central1 \
    --platform managed \
    --no-allow-unauthenticated \
    --memory 32Gi \
    --cpu 4 \
    --timeout 3600 \
    --max-instances 3 \
    --min-instances 1 \
    --add-cloudsql-instances buildtrace:us-central1:buildtrace-postgres \
    --set-env-vars \
        ENVIRONMENT=production,\
        USE_DATABASE=true,\
        USE_GCS=true,\
        INSTANCE_CONNECTION_NAME=buildtrace:us-central1:buildtrace-postgres,\
        DB_USER=buildtrace_user,\
        DB_NAME=buildtrace_db,\
        GCS_BUCKET_NAME=buildtrace-storage,\
        OPENAI_API_KEY=sk-... \
    --command python,gcp/infrastructure/job_processor.py
```

### Step 6: Initialize Database

```bash
# Connect to Cloud SQL
gcloud sql connect buildtrace-postgres --user=buildtrace_user

# Or use Cloud SQL Proxy
./cloud_sql_proxy -instances=buildtrace:us-central1:buildtrace-postgres=tcp:5432

# Initialize schema
python -c "from gcp.database import init_db; init_db()"
```

### Step 7: Verify Deployment

```bash
# Get service URL
gcloud run services describe buildtrace-overlay --region=us-central1 --format="value(status.url)"

# Test health endpoint
curl https://buildtrace-overlay-[hash]-uc.a.run.app/health

# Check logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=buildtrace-overlay" --limit=50
```

## Docker Deployment

### Local Docker Compose

**File**: `gcp/deployment/docker-compose.yml`

```bash
cd gcp/deployment
docker-compose up -d
```

**Services:**
- `app`: Web application
- `postgres`: PostgreSQL database
- `job-processor`: Background job processor (optional)

### Custom Docker Deployment

**Build Image:**
```bash
docker build -f gcp/deployment/Dockerfile -t buildtrace:latest .
```

**Run Container:**
```bash
docker run -d \
    --name buildtrace \
    -p 8080:8080 \
    -e ENVIRONMENT=production \
    -e USE_DATABASE=true \
    -e DB_HOST=postgres \
    -e DB_USER=buildtrace_user \
    -e DB_PASS=password \
    -e DB_NAME=buildtrace_db \
    -e OPENAI_API_KEY=sk-... \
    buildtrace:latest
```

## CI/CD Pipeline

### Cloud Build Configuration

**File**: `gcp/deployment/cloudbuild.yaml`

**Pipeline Steps:**
1. Build Docker image
2. Push to Artifact Registry
3. Deploy web app to Cloud Run
4. Deploy job processor to Cloud Run

**Trigger Setup:**
```bash
# Create trigger
gcloud builds triggers create github \
    --name="buildtrace-deploy" \
    --repo-name="buildtrace" \
    --repo-owner="your-org" \
    --branch-pattern="^main$" \
    --build-config="gcp/deployment/cloudbuild.yaml"
```

### GitHub Actions (Alternative)

**File**: `.github/workflows/deploy.yml`

```yaml
name: Deploy to Cloud Run

on:
  push:
    branches: [main]

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      
      - id: 'auth'
        uses: 'google-github-actions/auth@v0'
        with:
          credentials_json: '${{ secrets.GCP_SA_KEY }}'
      
      - name: 'Set up Cloud SDK'
        uses: 'google-github-actions/setup-gcloud@v0'
      
      - name: 'Deploy'
        run: |
          gcloud builds submit --config=gcp/deployment/cloudbuild.yaml .
```

## Database Deployment

### Cloud SQL Setup

**Create Instance:**
```bash
gcloud sql instances create buildtrace-postgres \
    --database-version=POSTGRES_14 \
    --tier=db-f1-micro \
    --region=us-central1 \
    --backup-start-time=03:00 \
    --enable-bin-log
```

**Create Database:**
```bash
gcloud sql databases create buildtrace_db \
    --instance=buildtrace-postgres
```

**Create User:**
```bash
gcloud sql users create buildtrace_user \
    --instance=buildtrace-postgres \
    --password=YOUR_SECURE_PASSWORD
```

### Database Migrations

**Using Alembic:**
```bash
# Create migration
alembic revision --autogenerate -m "Initial schema"

# Apply migration
alembic upgrade head
```

**Manual Migration:**
```python
from gcp.database import init_db
init_db()
```

### Backup Strategy

**Automated Backups:**
- Cloud SQL automatic backups (enabled by default)
- Daily backups at 3:00 AM
- 7-day retention

**Manual Backup:**
```bash
gcloud sql backups create \
    --instance=buildtrace-postgres \
    --description="Manual backup before migration"
```

**Restore:**
```bash
gcloud sql backups restore BACKUP_ID \
    --backup-instance=buildtrace-postgres \
    --restore-instance=buildtrace-postgres
```

## Storage Setup

### GCS Bucket Configuration

**Create Bucket:**
```bash
gsutil mb -p buildtrace -c STANDARD -l us-central1 gs://buildtrace-storage
```

**Set Lifecycle Policy:**
```json
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "Delete"},
        "condition": {"age": 90}
      }
    ]
  }
}
```

```bash
gsutil lifecycle set lifecycle.json gs://buildtrace-storage
```

**Set CORS:**
```json
[
  {
    "origin": ["https://yourdomain.com"],
    "method": ["GET", "POST", "PUT"],
    "responseHeader": ["Content-Type"],
    "maxAgeSeconds": 3600
  }
]
```

```bash
gsutil cors set cors.json gs://buildtrace-storage
```

## Monitoring & Logging

### Cloud Logging

**View Logs:**
```bash
# Web app logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=buildtrace-overlay" --limit=50

# Job processor logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=buildtrace-job-processor" --limit=50

# All logs
gcloud logging read "resource.type=cloud_run_revision" --limit=100
```

### Cloud Monitoring

**Metrics to Monitor:**
- Request latency
- Request count
- Error rate
- Processing time
- Memory usage
- CPU usage

**Set Up Alerts:**
```bash
# Create alert policy (via Console or API)
gcloud alpha monitoring policies create \
    --notification-channels=CHANNEL_ID \
    --display-name="High Error Rate" \
    --condition-threshold-value=0.05 \
    --condition-threshold-duration=300s
```

### Health Checks

**Endpoint**: `/health`

**Response:**
```json
{
  "status": "healthy",
  "environment": "production",
  "database": "connected",
  "storage": "gcs"
}
```

**Set Up Uptime Check:**
```bash
gcloud monitoring uptime-checks create \
    --display-name="BuildTrace Health Check" \
    --http-check-path="/health" \
    --http-check-port=8080 \
    --period=60s \
    --timeout=10s
```

## Scaling Configuration

### Auto-Scaling

**Web App:**
- Min instances: 0 (scale to zero)
- Max instances: 10
- CPU utilization: 60% (default)

**Job Processor:**
- Min instances: 1 (always warm)
- Max instances: 3
- CPU utilization: 60% (default)

### Manual Scaling

```bash
# Set min instances
gcloud run services update buildtrace-overlay \
    --min-instances=2 \
    --region=us-central1

# Set max instances
gcloud run services update buildtrace-overlay \
    --max-instances=20 \
    --region=us-central1
```

## Security

### IAM Roles

**Service Account Permissions:**
- `roles/cloudsql.client`: Database access
- `roles/storage.objectAdmin`: GCS access
- `roles/cloudtasks.enqueuer`: Cloud Tasks (if using)

**Grant Permissions:**
```bash
gcloud projects add-iam-policy-binding buildtrace \
    --member="serviceAccount:SERVICE_ACCOUNT" \
    --role="roles/cloudsql.client"
```

### Secrets Management

**Store Secrets:**
```bash
# Create secret
echo -n "sk-..." | gcloud secrets create openai-api-key --data-file=-

# Grant access
gcloud secrets add-iam-policy-binding openai-api-key \
    --member="serviceAccount:SERVICE_ACCOUNT" \
    --role="roles/secretmanager.secretAccessor"

# Use in Cloud Run
gcloud run services update buildtrace-overlay \
    --update-secrets OPENAI_API_KEY=openai-api-key:latest
```

### Network Security

- **HTTPS Only**: Enforced by Cloud Run
- **Private IP**: Cloud SQL uses private IP
- **VPC**: Can configure VPC connector for private access

## Rollback

### Rollback Deployment

```bash
# List revisions
gcloud run revisions list --service=buildtrace-overlay --region=us-central1

# Rollback to previous revision
gcloud run services update-traffic buildtrace-overlay \
    --to-revisions PREVIOUS_REVISION=100 \
    --region=us-central1
```

### Database Rollback

```bash
# Restore from backup
gcloud sql backups restore BACKUP_ID \
    --backup-instance=buildtrace-postgres \
    --restore-instance=buildtrace-postgres
```

## Troubleshooting

### Common Issues

1. **Deployment Fails**
   - Check Cloud Build logs
   - Verify Dockerfile syntax
   - Check image push permissions

2. **Database Connection Fails**
   - Verify Cloud SQL instance exists
   - Check connection name format
   - Verify service account permissions

3. **Storage Access Fails**
   - Check bucket exists
   - Verify service account has storage permissions
   - Check bucket name in environment variables

4. **High Memory Usage**
   - Increase memory allocation
   - Check for memory leaks
   - Monitor job processor memory

See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for more details.

---

**Next Steps**: See [MONITORING.md](./MONITORING.md) for monitoring setup or [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for deployment issues.

```


Creating the final documentation files:


```markdown
# BuildTrace Troubleshooting Guide

Comprehensive troubleshooting guide for common issues, errors, and solutions.

## Table of Contents

1. [Quick Diagnostics](#quick-diagnostics)
2. [Setup Issues](#setup-issues)
3. [Database Issues](#database-issues)
4. [Storage Issues](#storage-issues)
5. [Processing Issues](#processing-issues)
6. [API Issues](#api-issues)
7. [Deployment Issues](#deployment-issues)
8. [Performance Issues](#performance-issues)

## Quick Diagnostics

### Health Check

```bash
# Check application health
curl http://localhost:5001/health

# Expected response:
# {"status": "healthy", "environment": "development", ...}
```

### Configuration Check

```python
from config import config
print(f"Environment: {config.ENVIRONMENT}")
print(f"Database: {config.USE_DATABASE}")
print(f"GCS: {config.USE_GCS}")
print(f"OpenAI Key: {'Set' if config.OPENAI_API_KEY else 'Missing'}")
```

### Logs Check

```bash
# Local development
tail -f app.log

# Cloud Run
gcloud logging read "resource.type=cloud_run_revision" --limit=50
```

## Setup Issues

### Issue: Import Errors

**Symptoms:**
```
ModuleNotFoundError: No module named 'cv2'
ImportError: cannot import name 'Config' from 'config'
```

**Solutions:**

1. **Activate Virtual Environment:**
```bash
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate    # Windows
```

2. **Reinstall Dependencies:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

3. **Check Python Version:**
```bash
python --version  # Should be 3.11+
```

### Issue: Tesseract Not Found

**Symptoms:**
```
TesseractNotFoundError: tesseract is not installed
```

**Solutions:**

**macOS:**
```bash
brew install tesseract
```

**Ubuntu/Debian:**
```bash
sudo apt-get update
sudo apt-get install tesseract-ocr
```

**Windows:**
- Download from: https://github.com/UB-Mannheim/tesseract/wiki
- Add to PATH

**Verify Installation:**
```bash
tesseract --version
```

### Issue: Poppler Not Found

**Symptoms:**
```
PDFInfoNotInstalledError: Unable to get page count
```

**Solutions:**

**macOS:**
```bash
brew install poppler
```

**Ubuntu/Debian:**
```bash
sudo apt-get install poppler-utils
```

**Windows:**
- Download from: https://github.com/oschwartz10612/poppler-windows/releases

### Issue: Port Already in Use

**Symptoms:**
```
OSError: [Errno 48] Address already in use
```

**Solutions:**

1. **Change Port:**
```bash
# In .env file
PORT=5002
```

2. **Find and Kill Process:**
```bash
# macOS/Linux
lsof -ti:5001 | xargs kill -9

# Windows
netstat -ano | findstr :5001
taskkill /PID <PID> /F
```

### Issue: Missing .env File

**Symptoms:**
```
KeyError: 'OPENAI_API_KEY'
```

**Solutions:**

1. **Create .env File:**
```bash
cp .env.example .env
# Edit .env with your values
```

2. **Set Environment Variables:**
```bash
export OPENAI_API_KEY=sk-...
export ENVIRONMENT=development
```

## Database Issues

### Issue: Database Connection Failed

**Symptoms:**
```
sqlalchemy.exc.OperationalError: could not connect to server
psycopg2.OperationalError: connection refused
```

**Solutions:**

1. **Check PostgreSQL is Running:**
```bash
# macOS/Linux
pg_isready

# Check status
sudo systemctl status postgresql  # Linux
brew services list | grep postgres  # macOS
```

2. **Verify Connection String:**
```python
from config import config
print(config.DATABASE_URL)
```

3. **Test Connection:**
```bash
psql -h localhost -U buildtrace_user -d buildtrace_db
```

4. **Check Firewall:**
```bash
# Allow PostgreSQL port
sudo ufw allow 5432
```

### Issue: Authentication Failed

**Symptoms:**
```
psycopg2.OperationalError: password authentication failed
```

**Solutions:**

1. **Reset Password:**
```bash
# Connect as postgres user
psql -U postgres

# Change password
ALTER USER buildtrace_user WITH PASSWORD 'new_password';
```

2. **Update .env:**
```bash
DB_PASS=new_password
```

### Issue: Database Does Not Exist

**Symptoms:**
```
psycopg2.OperationalError: database "buildtrace_db" does not exist
```

**Solutions:**

1. **Create Database:**
```bash
createdb buildtrace_db

# Or using psql
psql -U postgres
CREATE DATABASE buildtrace_db;
```

2. **Initialize Schema:**
```python
from gcp.database import init_db
init_db()
```

### Issue: Cloud SQL Connection Failed

**Symptoms:**
```
Cloud SQL connection failed: connection timeout
```

**Solutions:**

1. **Verify Instance Name:**
```bash
gcloud sql instances list
```

2. **Check Cloud SQL Proxy:**
```bash
# Start proxy
./cloud_sql_proxy -instances=buildtrace:us-central1:buildtrace-postgres=tcp:5432

# In another terminal, test connection
psql -h 127.0.0.1 -U buildtrace_user -d buildtrace_db
```

3. **Verify Service Account Permissions:**
```bash
gcloud projects get-iam-policy buildtrace \
    --flatten="bindings[].members" \
    --filter="bindings.members:serviceAccount:*"
```

## Storage Issues

### Issue: GCS Access Denied

**Symptoms:**
```
google.api_core.exceptions.Forbidden: 403 Access Denied
```

**Solutions:**

1. **Verify Service Account:**
```bash
gcloud auth application-default login
```

2. **Check Bucket Permissions:**
```bash
gsutil iam get gs://buildtrace-storage
```

3. **Grant Permissions:**
```bash
gsutil iam ch serviceAccount:SERVICE_ACCOUNT:objectAdmin gs://buildtrace-storage
```

### Issue: Local Storage Path Not Found

**Symptoms:**
```
FileNotFoundError: [Errno 2] No such file or directory: 'uploads/...'
```

**Solutions:**

1. **Create Directories:**
```bash
mkdir -p uploads results temp
```

2. **Check Permissions:**
```bash
chmod 755 uploads results temp
```

### Issue: File Upload Fails

**Symptoms:**
```
413 Request Entity Too Large
```

**Solutions:**

1. **Increase Upload Limit:**
```bash
# In .env
MAX_CONTENT_LENGTH=104857600  # 100MB
```

2. **Check Nginx/Apache Limits** (if using reverse proxy):
```nginx
client_max_body_size 100M;
```

## Processing Issues

### Issue: No Features Detected

**Symptoms:**
```
ValueError: Need at least 4 matches to estimate transformation
```

**Solutions:**

1. **Increase Image Quality:**
```python
# Use higher DPI
complete_drawing_pipeline(..., dpi=600)
```

2. **Check Image Quality:**
```python
# Verify images are not corrupted
import cv2
img = cv2.imread('image.png')
print(img.shape)  # Should not be None
```

3. **Preprocess Images:**
```python
# Enhance contrast
img = cv2.convertScaleAbs(img, alpha=1.5, beta=30)
```

### Issue: Alignment Failed

**Symptoms:**
```
Alignment score too low: 0.2
```

**Solutions:**

1. **Check if Correct Drawings Matched:**
```python
# Verify drawing names match
print(f"Old: {old_drawing_name}, New: {new_drawing_name}")
```

2. **Try Different DPI:**
```python
# Higher DPI may improve feature detection
complete_drawing_pipeline(..., dpi=600)
```

3. **Manual Alignment:**
```python
# Use manual transformation if automatic fails
# (Feature to be implemented)
```

### Issue: OpenAI API Error

**Symptoms:**
```
openai.error.RateLimitError: Rate limit exceeded
openai.error.APIError: Invalid API key
```

**Solutions:**

1. **Check API Key:**
```bash
echo $OPENAI_API_KEY
# Should start with sk-
```

2. **Verify API Key:**
```python
from openai import OpenAI
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
# Test with simple request
```

3. **Handle Rate Limits:**
```python
# Add retry logic
import time
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def call_openai():
    # API call
    pass
```

### Issue: Processing Timeout

**Symptoms:**
```
TimeoutError: Processing exceeded timeout
```

**Solutions:**

1. **Increase Timeout:**
```bash
# In .env
PROCESSING_TIMEOUT=7200  # 2 hours
```

2. **Use Async Processing:**
```bash
USE_ASYNC_PROCESSING=true
USE_BACKGROUND_PROCESSING=true
```

3. **Process in Chunks:**
```python
# Use chunked processor for large files
from chunked_processor import process_documents
results = process_documents(old_file, new_file)
```

## API Issues

### Issue: 404 Not Found

**Symptoms:**
```
404 Not Found: The requested URL was not found
```

**Solutions:**

1. **Check Route:**
```python
# Verify route exists in app.py
@app.route('/api/endpoint')
```

2. **Check Base URL:**
```bash
# Verify correct base URL
curl http://localhost:5001/health
```

3. **Check Session ID:**
```python
# Verify session exists
from gcp.database import get_db_session
with get_db_session() as db:
    session = db.query(Session).filter_by(id=session_id).first()
```

### Issue: 500 Internal Server Error

**Symptoms:**
```
500 Internal Server Error
```

**Solutions:**

1. **Check Logs:**
```bash
# Local
tail -f app.log

# Cloud Run
gcloud logging read "resource.type=cloud_run_revision" --limit=50
```

2. **Enable Debug Mode:**
```bash
# In .env
DEBUG=true
```

3. **Check Error Details:**
```python
# Add error handling
try:
    # Code
except Exception as e:
    logger.error(f"Error: {e}", exc_info=True)
```

### Issue: CORS Error

**Symptoms:**
```
Access to fetch at '...' from origin '...' has been blocked by CORS policy
```

**Solutions:**

1. **Add CORS Headers:**
```python
from flask_cors import CORS
CORS(app)
```

2. **Configure CORS:**
```python
CORS(app, resources={
    r"/api/*": {
        "origins": ["https://yourdomain.com"],
        "methods": ["GET", "POST", "PUT", "DELETE"]
    }
})
```

## Deployment Issues

### Issue: Docker Build Fails

**Symptoms:**
```
ERROR: failed to solve: process "/bin/sh -c pip install..." did not complete successfully
```

**Solutions:**

1. **Check Dockerfile:**
```bash
# Verify Dockerfile syntax
docker build -f gcp/deployment/Dockerfile -t test .
```

2. **Check Dependencies:**
```bash
# Verify requirements.txt is valid
pip install -r requirements.txt
```

3. **Clear Docker Cache:**
```bash
docker build --no-cache -f gcp/deployment/Dockerfile -t buildtrace .
```

### Issue: Cloud Run Deployment Fails

**Symptoms:**
```
ERROR: (gcloud.run.deploy) Revision failed with message: Container failed to start
```

**Solutions:**

1. **Check Logs:**
```bash
gcloud logging read "resource.type=cloud_run_revision" --limit=100
```

2. **Verify Environment Variables:**
```bash
gcloud run services describe buildtrace-overlay \
    --region=us-central1 \
    --format="value(spec.template.spec.containers[0].env)"
```

3. **Test Locally:**
```bash
docker run -e ENVIRONMENT=production ...
```

### Issue: Cloud Build Fails

**Symptoms:**
```
ERROR: build step "gcr.io/cloud-builders/docker" failed
```

**Solutions:**

1. **Check Cloud Build Logs:**
```bash
gcloud builds list --limit=5
gcloud builds log BUILD_ID
```

2. **Verify Permissions:**
```bash
gcloud projects get-iam-policy buildtrace
```

3. **Check Artifact Registry:**
```bash
gcloud artifacts repositories list
```

## Performance Issues

### Issue: Slow Processing

**Symptoms:**
- Processing takes > 5 minutes per drawing
- High CPU usage
- Memory usage spikes

**Solutions:**

1. **Optimize DPI:**
```python
# Lower DPI for faster processing (trade-off: quality)
complete_drawing_pipeline(..., dpi=200)
```

2. **Use Async Processing:**
```bash
USE_ASYNC_PROCESSING=true
```

3. **Increase Resources:**
```bash
# Cloud Run
gcloud run services update buildtrace-overlay \
    --memory 8Gi \
    --cpu 4
```

4. **Cache Results:**
```python
# Cache feature descriptors
# Cache converted images
```

### Issue: High Memory Usage

**Symptoms:**
- Out of memory errors
- Container killed

**Solutions:**

1. **Increase Memory:**
```bash
# Cloud Run
gcloud run services update buildtrace-overlay \
    --memory 8Gi
```

2. **Process in Chunks:**
```python
from chunked_processor import process_documents
```

3. **Optimize Image Processing:**
```python
# Resize images before processing
img = cv2.resize(img, (width//2, height//2))
```

### Issue: Database Slow Queries

**Symptoms:**
- Slow API responses
- Database connection timeouts

**Solutions:**

1. **Add Indexes:**
```sql
CREATE INDEX idx_session_status ON sessions(status);
CREATE INDEX idx_drawing_name ON drawings(drawing_name);
```

2. **Optimize Queries:**
```python
# Use eager loading
session = db.query(Session)\
    .options(joinedload(Session.drawings))\
    .filter_by(id=session_id)\
    .first()
```

3. **Connection Pooling:**
```python
# Already configured in database.py
# Adjust pool size if needed
```

## Getting Help

### Debug Mode

Enable debug mode for detailed error messages:

```bash
# In .env
DEBUG=true
LOG_LEVEL=DEBUG
```

### Logging

Add detailed logging:

```python
import logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)
logger.debug("Debug message")
```

### Support Resources

1. **Check Documentation**: Review relevant docs
2. **Check Logs**: Always check logs first
3. **Reproduce Issue**: Create minimal reproduction
4. **Check GitHub Issues**: Search for similar issues

---

**Next Steps**: See [DEVELOPMENT.md](./DEVELOPMENT.md) for development workflow or [API.md](./API.md) for API usage.

```

```markdown
# BuildTrace Development Guide

Development workflow, coding standards, testing practices, and contribution guidelines.

## Table of Contents

1. [Development Workflow](#development-workflow)
2. [Code Style](#code-style)
3. [Git Workflow](#git-workflow)
4. [Testing](#testing)
5. [Debugging](#debugging)
6. [Code Review](#code-review)

## Development Workflow

### Local Development Setup

1. **Clone Repository:**
```bash
git clone <repository-url>
cd buildtrace-overlay
```

2. **Create Virtual Environment:**
```bash
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
venv\Scripts\activate     # Windows
```

3. **Install Dependencies:**
```bash
pip install --upgrade pip
pip install -r requirements.txt
pip install -r requirements-dev.txt  # If exists
```

4. **Configure Environment:**
```bash
cp .env.example .env
# Edit .env with your settings
```

5. **Initialize Database** (if using):
```bash
python -c "from gcp.database import init_db; init_db()"
```

6. **Run Application:**
```bash
python app.py
```

### Development Server

**Flask Development Server:**
```bash
export FLASK_APP=app.py
export FLASK_ENV=development
flask run --port=5001
```

**With Auto-Reload:**
```bash
flask run --reload --port=5001
```

### Hot Reload

The Flask development server automatically reloads on code changes. For other processes:

```bash
# Use watchdog for file watching
pip install watchdog
watchmedo auto-restart --patterns="*.py" --recursive -- python app.py
```

## Code Style

### Python Style Guide

Follow **PEP 8** with these modifications:

**Line Length:** 100 characters (not 79)

**Imports:** Grouped and sorted
```python
# Standard library
import os
import sys
from datetime import datetime

# Third-party
import cv2
import numpy as np
from flask import Flask, request

# Local
from config import config
from gcp.database import get_db_session
```

**Naming Conventions:**
- **Classes**: `PascalCase` (e.g., `CloudStorageService`)
- **Functions/Variables**: `snake_case` (e.g., `get_db_session`)
- **Constants**: `UPPER_SNAKE_CASE` (e.g., `MAX_UPLOAD_SIZE`)
- **Private**: Prefix with `_` (e.g., `_internal_method`)

### Code Formatting

**Use Black** (when configured):
```bash
black --line-length 100 .
```

**Use isort** for imports:
```bash
isort .
```

### Type Hints

Use type hints for function signatures:

```python
from typing import Optional, List, Dict

def process_drawing(
    file_path: str,
    dpi: int = 300,
    debug: bool = False
) -> Dict[str, any]:
    """Process a drawing file.
    
    Args:
        file_path: Path to drawing file
        dpi: Resolution for conversion
        debug: Enable debug mode
        
    Returns:
        Dictionary with processing results
    """
    pass
```

### Docstrings

Use Google-style docstrings:

```python
def align_drawings(old_img: np.ndarray, new_img: np.ndarray) -> np.ndarray:
    """Align two drawing images using SIFT feature matching.
    
    This function detects features in both images, matches them, and
    applies an affine transformation to align the old image with the new.
    
    Args:
        old_img: Old drawing image as numpy array
        new_img: New drawing image as numpy array
        
    Returns:
        Aligned old image as numpy array
        
    Raises:
        ValueError: If insufficient features are detected
        RuntimeError: If alignment fails
    """
    pass
```

## Git Workflow

### Branch Strategy

- **main**: Production-ready code
- **develop**: Integration branch
- **feature/**: Feature branches
- **bugfix/**: Bug fix branches
- **hotfix/**: Urgent production fixes

### Commit Messages

Follow **Conventional Commits**:

```
<type>(<scope>): <subject>

<body>

<footer>
```

**Types:**
- `feat`: New feature
- `fix`: Bug fix
- `docs`: Documentation
- `style`: Code style changes
- `refactor`: Code refactoring
- `test`: Test additions/changes
- `chore`: Maintenance tasks

**Examples:**
```
feat(api): Add session summary endpoint

Add new endpoint to get aggregated summary of all drawings
in a session with statistics and recommendations.

Closes #123
```

```
fix(pipeline): Fix alignment score calculation

Correct alignment score calculation to use normalized
feature match ratio instead of raw match count.

Fixes #456
```

### Pull Request Process

1. **Create Feature Branch:**
```bash
git checkout -b feature/new-endpoint
```

2. **Make Changes:**
```bash
# Make code changes
git add .
git commit -m "feat(api): Add new endpoint"
```

3. **Push and Create PR:**
```bash
git push origin feature/new-endpoint
# Create PR on GitHub
```

4. **Code Review:**
- Address review comments
- Update PR description
- Ensure tests pass

5. **Merge:**
- Squash and merge (preferred)
- Delete branch after merge

## Testing

### Test Structure

```
tests/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ conftest.py
â”œâ”€â”€ test_api.py
â”œâ”€â”€ test_pipeline.py
â”œâ”€â”€ test_alignment.py
â””â”€â”€ test_database.py
```

### Unit Tests

**Example:**
```python
import pytest
from align_drawings import align_drawings
import cv2
import numpy as np

def test_align_drawings_success():
    """Test successful alignment of two images."""
    old_img = cv2.imread('tests/fixtures/old.png')
    new_img = cv2.imread('tests/fixtures/new.png')
    
    aligned = align_drawings(old_img, new_img)
    
    assert aligned is not None
    assert aligned.shape == new_img.shape
```

### Integration Tests

**Example:**
```python
def test_complete_pipeline(client):
    """Test complete processing pipeline."""
    response = client.post('/upload', data={
        'old_file': open('tests/fixtures/old.pdf', 'rb'),
        'new_file': open('tests/fixtures/new.pdf', 'rb')
    })
    
    assert response.status_code == 200
    session_id = response.json['session_id']
    
    # Process
    response = client.post(f'/process/{session_id}')
    assert response.status_code == 200
    
    # Check results
    response = client.get(f'/results/{session_id}')
    assert response.status_code == 200
    assert 'changes' in response.json
```

### Running Tests

```bash
# Run all tests
pytest

# Run specific test file
pytest tests/test_api.py

# Run with coverage
pytest --cov=. --cov-report=html

# Run with verbose output
pytest -v

# Run specific test
pytest tests/test_api.py::test_upload_endpoint
```

### Test Fixtures

**conftest.py:**
```python
import pytest
from app import app
from gcp.database import get_db_session

@pytest.fixture
def client():
    """Flask test client."""
    app.config['TESTING'] = True
    with app.test_client() as client:
        yield client

@pytest.fixture
def db_session():
    """Database session for testing."""
    with get_db_session() as db:
        yield db
        db.rollback()
```

## Debugging

### Debug Mode

Enable debug mode for detailed error messages:

```python
# In app.py
app.config['DEBUG'] = True
```

### Logging

Use structured logging:

```python
import logging

logger = logging.getLogger(__name__)

def process_file(file_path: str):
    logger.info(f"Processing file: {file_path}")
    try:
        # Process
        logger.debug(f"File processed successfully")
    except Exception as e:
        logger.error(f"Error processing file: {e}", exc_info=True)
        raise
```

### Debugger

Use Python debugger:

```python
import pdb

def problematic_function():
    # Set breakpoint
    pdb.set_trace()
    # Code execution pauses here
    pass
```

Or use IDE debugger (VS Code, PyCharm).

### Print Debugging

For quick debugging:

```python
print(f"DEBUG: Variable value: {variable}")
print(f"DEBUG: Function called with: {args}")
```

**Note**: Remove debug prints before committing.

## Code Review

### Review Checklist

**Functionality:**
- [ ] Code works as intended
- [ ] Handles edge cases
- [ ] Error handling is appropriate
- [ ] Performance is acceptable

**Code Quality:**
- [ ] Follows style guide
- [ ] Has appropriate comments
- [ ] No hardcoded values
- [ ] Uses configuration properly

**Testing:**
- [ ] Tests added/updated
- [ ] Tests pass
- [ ] Coverage maintained

**Documentation:**
- [ ] Docstrings added
- [ ] README updated if needed
- [ ] API docs updated

### Review Comments

**Be Constructive:**
- Explain why, not just what
- Suggest alternatives
- Acknowledge good work

**Example:**
```
Good: "Consider using a dictionary here instead of multiple if/else 
       statements for better readability and maintainability."

Bad: "This is wrong."
```

## Best Practices

### Error Handling

```python
# Good
try:
    result = process_file(file_path)
except FileNotFoundError:
    logger.error(f"File not found: {file_path}")
    raise
except ProcessingError as e:
    logger.error(f"Processing failed: {e}")
    return {"error": str(e)}, 500
except Exception as e:
    logger.error(f"Unexpected error: {e}", exc_info=True)
    raise
```

### Resource Management

```python
# Use context managers
with open(file_path, 'r') as f:
    content = f.read()

with get_db_session() as db:
    session = db.query(Session).filter_by(id=session_id).first()
```

### Configuration

```python
# Use config, not hardcoded values
from config import config

if config.USE_DATABASE:
    # Use database
    pass
```

### Security

- Never commit secrets
- Validate all inputs
- Use parameterized queries
- Sanitize file paths
- Check file types and sizes

### Performance

- Use connection pooling
- Cache expensive operations
- Process in chunks for large files
- Use async processing when appropriate
- Monitor memory usage

## IDE Setup

### VS Code

**Recommended Extensions:**
- Python
- Pylance
- Black Formatter
- isort
- Python Test Explorer

**.vscode/settings.json:**
```json
{
  "python.formatting.provider": "black",
  "python.linting.enabled": true,
  "python.linting.pylintEnabled": true,
  "editor.formatOnSave": true
}
```

### PyCharm

**Settings:**
- Code Style: PEP 8
- Formatter: Black
- Type Checking: Enable

## Documentation

### Code Comments

```python
# Good: Explains why
# Use SIFT because it's robust to scale and rotation changes
# common in architectural drawings
sift = cv2.SIFT_create()

# Bad: States the obvious
# Create SIFT object
sift = cv2.SIFT_create()
```

### Update Documentation

When making changes:
- Update relevant documentation
- Add examples if needed
- Update API docs
- Update README if behavior changes

---

**Next Steps**: See [ROADMAP.md](./ROADMAP.md) for planned features or [API.md](./API.md) for API usage.

```

```markdown
# BuildTrace Roadmap

Future plans, technical debt, improvement opportunities, and feature roadmap.

## Table of Contents

1. [Current Status](#current-status)
2. [Short-Term Goals](#short-term-goals)
3. [Medium-Term Goals](#medium-term-goals)
4. [Long-Term Vision](#long-term-vision)
5. [Technical Debt](#technical-debt)
6. [Performance Improvements](#performance-improvements)

## Current Status

### Completed Features

âœ… **Core Functionality:**
- PDF to PNG conversion
- Drawing name extraction (text + OCR)
- SIFT-based alignment
- Overlay generation
- GPT-4 change analysis
- Web interface
- AI chatbot
- Database integration
- Cloud Storage integration
- Cloud Run deployment

âœ… **Infrastructure:**
- Docker containerization
- CI/CD pipeline (Cloud Build)
- Database migrations
- Storage abstraction
- Configuration management

## Short-Term Goals (1-3 months)

### 1. Authentication & Authorization

**Priority**: High

**Features:**
- Firebase Authentication integration
- User accounts and sessions
- Project management
- Role-based access control

**Implementation:**
- Enable `USE_FIREBASE_AUTH` feature flag
- Add user authentication middleware
- Implement project sharing
- Add permission checks

### 2. Enhanced Error Handling

**Priority**: High

**Features:**
- Better error messages
- Error recovery mechanisms
- Retry logic for transient failures
- User-friendly error pages

**Implementation:**
- Custom exception classes
- Error logging and monitoring
- Retry decorators
- Error notification system

### 3. Testing Infrastructure

**Priority**: High

**Features:**
- Unit test suite
- Integration tests
- End-to-end tests
- Test coverage reporting

**Implementation:**
- Set up pytest
- Create test fixtures
- Add CI test pipeline
- Target 80%+ coverage

### 4. API Rate Limiting

**Priority**: Medium

**Features:**
- Rate limiting per endpoint
- Per-user rate limits
- Rate limit headers
- Graceful degradation

**Implementation:**
- Use Flask-Limiter
- Configure limits per endpoint
- Add rate limit headers
- Handle rate limit errors

### 5. WebSocket Support

**Priority**: Medium

**Features:**
- Real-time processing updates
- Live progress indicators
- Push notifications
- WebSocket API

**Implementation:**
- Add Flask-SocketIO
- Implement progress events
- Client-side WebSocket handling
- Fallback to polling

## Medium-Term Goals (3-6 months)

### 1. Advanced Computer Vision

**Priority**: High

**Features:**
- Multiple alignment algorithms (ORB, AKAZE)
- Automatic algorithm selection
- Improved feature matching
- Better handling of low-quality images

**Implementation:**
- Add algorithm abstraction layer
- Implement multiple algorithms
- Add quality assessment
- Automatic algorithm selection

### 2. Batch Processing

**Priority**: Medium

**Features:**
- Process multiple drawing sets
- Batch upload interface
- Progress tracking per batch
- Batch results export

**Implementation:**
- Batch session management
- Queue system for batches
- Progress tracking
- Export functionality

### 3. Advanced Analytics

**Priority**: Medium

**Features:**
- Change statistics dashboard
- Trend analysis
- Cost estimation
- Timeline predictions

**Implementation:**
- Analytics data model
- Dashboard UI
- Statistical analysis
- Visualization components

### 4. Export & Reporting

**Priority**: Medium

**Features:**
- PDF report generation
- Excel export
- Custom report templates
- Email reports

**Implementation:**
- Report generation service
- Template system
- Export formats
- Email integration

### 5. Mobile App

**Priority**: Low

**Features:**
- iOS app
- Android app
- Mobile-optimized UI
- Offline support

**Implementation:**
- React Native or Flutter
- API integration
- Mobile UI components
- Offline data sync

## Long-Term Vision (6-12 months)

### 1. Multi-User Collaboration

**Features:**
- Real-time collaboration
- Comments and annotations
- Version control
- Change approval workflow

### 2. AI Model Training

**Features:**
- Custom model training
- Domain-specific models
- Continuous learning
- Model versioning

### 3. Integration Ecosystem

**Features:**
- CAD software plugins
- Project management integrations
- API marketplace
- Webhook support

### 4. Enterprise Features

**Features:**
- SSO integration
- Advanced security
- Compliance features
- Audit logging
- SLA guarantees

### 5. Global Scale

**Features:**
- Multi-region deployment
- CDN integration
- Localization
- Regional compliance

## Technical Debt

### High Priority

1. **Error Handling**
   - Inconsistent error handling across modules
   - Missing error recovery
   - Need standardized error classes

2. **Testing**
   - Limited test coverage
   - Missing integration tests
   - No E2E tests

3. **Documentation**
   - Some functions lack docstrings
   - API documentation incomplete
   - Missing architecture diagrams

4. **Code Organization**
   - Some large files need refactoring
   - Duplicate code in places
   - Need better module boundaries

### Medium Priority

1. **Performance**
   - Some slow database queries
   - Image processing could be optimized
   - Caching opportunities

2. **Security**
   - Need input validation improvements
   - Rate limiting missing
   - Security headers needed

3. **Monitoring**
   - Limited observability
   - Missing metrics
   - No alerting system

### Low Priority

1. **Code Style**
   - Inconsistent formatting
   - Some legacy code patterns
   - Type hints incomplete

2. **Dependencies**
   - Some outdated packages
   - Unused dependencies
   - Version pinning needed

## Performance Improvements

### Immediate (1 month)

1. **Database Optimization**
   - Add missing indexes
   - Optimize slow queries
   - Connection pool tuning

2. **Caching**
   - Redis integration
   - Cache feature descriptors
   - Cache analysis results

3. **Image Processing**
   - Optimize image operations
   - Parallel processing
   - Memory optimization

### Short-Term (3 months)

1. **Async Processing**
   - Full async pipeline
   - Background job queue
   - Progress tracking

2. **CDN Integration**
   - Static asset CDN
   - Image CDN
   - API caching

3. **Database Scaling**
   - Read replicas
   - Query optimization
   - Partitioning

### Long-Term (6+ months)

1. **Microservices**
   - Split into services
   - Independent scaling
   - Service mesh

2. **Edge Computing**
   - Edge processing
   - Regional deployment
   - Reduced latency

## Feature Requests

### User-Requested Features

1. **Drawing Version History**
   - Track all versions
   - Compare any two versions
   - Version timeline

2. **Custom Change Categories**
   - User-defined categories
   - Custom analysis prompts
   - Category-based filtering

3. **Team Workspaces**
   - Shared workspaces
   - Team permissions
   - Collaboration tools

4. **Mobile App**
   - View results on mobile
   - Quick comparisons
   - Notifications

### Internal Ideas

1. **AI Model Fine-Tuning**
   - Train on user data
   - Domain-specific models
   - Improved accuracy

2. **Advanced Visualization**
   - 3D overlays
   - Interactive comparisons
   - Change highlighting

3. **Automated Reporting**
   - Scheduled reports
   - Custom templates
   - Email delivery

## Research Areas

### Computer Vision

- **Deep Learning**: Explore deep learning for alignment
- **Object Detection**: Detect specific elements in drawings
- **Change Detection**: Direct change detection without alignment

### AI/ML

- **Custom Models**: Train domain-specific models
- **Few-Shot Learning**: Learn from limited examples
- **Active Learning**: Improve from user feedback

### Infrastructure

- **Serverless**: Explore serverless options
- **Edge Computing**: Process at edge
- **GraphQL**: Consider GraphQL API

## Success Metrics

### User Metrics

- **User Satisfaction**: Target 4.5+ stars
- **Processing Time**: < 60 seconds per drawing
- **Accuracy**: > 95% alignment success rate
- **Uptime**: 99.9% availability

### Technical Metrics

- **Test Coverage**: > 80%
- **API Response Time**: < 200ms (p95)
- **Error Rate**: < 0.1%
- **Database Query Time**: < 100ms (p95)

## Contribution

We welcome contributions! See [DEVELOPMENT.md](./DEVELOPMENT.md) for:
- Development setup
- Code style guide
- Testing requirements
- Pull request process

## Feedback

Please share feedback, feature requests, or bug reports:
- GitHub Issues
- Email: [contact email]
- Slack: [slack channel]

---

**Last Updated**: 2024
**Next Review**: Quarterly

```

```markdown
# BuildTrace Storage Documentation

Complete guide to storage architecture, file management, and access patterns.

## Table of Contents

1. [Overview](#overview)
2. [Storage Modes](#storage-modes)
3. [File Organization](#file-organization)
4. [Storage Service](#storage-service)
5. [Access Patterns](#access-patterns)
6. [Performance](#performance)

## Overview

BuildTrace uses a **unified storage abstraction** that supports:
- **Local Storage**: File system (development)
- **Google Cloud Storage**: Object storage (production)
- **Automatic Fallback**: Seamless switching between modes

**Storage Service**: `gcp/storage/storage_service.py`

## Storage Modes

### Local Storage Mode

**When Used**: Development or when `USE_GCS=false`

**Configuration:**
```bash
USE_GCS=false
LOCAL_UPLOAD_PATH=uploads
LOCAL_RESULTS_PATH=results
LOCAL_TEMP_PATH=temp
```

**Storage Location:**
```
project-root/
â”œâ”€â”€ uploads/
â”‚   â””â”€â”€ sessions/
â”‚       â””â”€â”€ {session_id}/
â”‚           â”œâ”€â”€ uploads/
â”‚           â””â”€â”€ results/
â”œâ”€â”€ results/
â””â”€â”€ temp/
```

**Advantages:**
- No cloud setup required
- Fast local access
- Easy debugging
- No costs

**Limitations:**
- Not scalable
- Not persistent across deployments
- No sharing between instances

### Google Cloud Storage Mode

**When Used**: Production or when `USE_GCS=true`

**Configuration:**
```bash
USE_GCS=true
GCS_BUCKET_NAME=buildtrace-storage
GCS_UPLOAD_BUCKET=buildtrace-drawings-upload
GCS_PROCESSED_BUCKET=buildtrace-drawings-processed
```

**Storage Location:**
```
gs://buildtrace-storage/
â”œâ”€â”€ sessions/
â”‚   â””â”€â”€ {session_id}/
â”‚       â”œâ”€â”€ uploads/
â”‚       â””â”€â”€ results/
â””â”€â”€ drawings/
```

**Advantages:**
- Scalable
- Persistent
- Shared across instances
- Production-ready
- Signed URLs for secure access

**Limitations:**
- Requires GCS setup
- Network latency
- Storage costs

## File Organization

### Directory Structure

```
sessions/
â””â”€â”€ {session_id}/
    â”œâ”€â”€ uploads/
    â”‚   â”œâ”€â”€ old_drawings.pdf
    â”‚   â””â”€â”€ new_drawings.pdf
    â””â”€â”€ results/
        â”œâ”€â”€ {drawing_name}/
        â”‚   â”œâ”€â”€ {drawing_name}_old.png
        â”‚   â”œâ”€â”€ {drawing_name}_new.png
        â”‚   â”œâ”€â”€ {drawing_name}_overlay.png
        â”‚   â””â”€â”€ change_analysis_{drawing_name}.json
        â””â”€â”€ summary.json
```

### File Naming Conventions

**Uploaded Files:**
- Original filename preserved in metadata
- Stored as: `{original_filename}` or `{secure_filename}`

**Processed Files:**
- Drawing images: `{drawing_name}_old.png`, `{drawing_name}_new.png`
- Overlays: `{drawing_name}_overlay.png`
- Analysis: `change_analysis_{drawing_name}.json`

**Session Files:**
- Session metadata: `sessions/{session_id}/metadata.json`
- Results summary: `sessions/{session_id}/results/summary.json`

## Storage Service

### CloudStorageService Class

**Location**: `gcp/storage/storage_service.py`

**Key Methods:**

#### Upload Operations

```python
# Upload file from memory
storage_service.upload_file(
    file_content=file_bytes,
    destination_path="sessions/{session_id}/uploads/file.pdf",
    content_type="application/pdf"
)

# Upload file from local path
storage_service.upload_from_filename(
    local_path="/tmp/file.pdf",
    destination_path="sessions/{session_id}/uploads/file.pdf"
)
```

#### Download Operations

```python
# Download as bytes
content = storage_service.download_file("sessions/{session_id}/file.pdf")

# Download to local file
storage_service.download_to_filename(
    source_path="sessions/{session_id}/file.pdf",
    local_path="/tmp/file.pdf"
)

# Download to file object
with open("/tmp/file.pdf", "wb") as f:
    storage_service.download_to_file("sessions/{session_id}/file.pdf", f)
```

#### File Management

```python
# Check if file exists
exists = storage_service.file_exists("sessions/{session_id}/file.pdf")

# Delete file
success = storage_service.delete_file("sessions/{session_id}/file.pdf")

# List files
files = storage_service.list_files(prefix="sessions/{session_id}/")

# Move/rename file
storage_service.move_file(
    source_path="sessions/{session_id}/old_name.pdf",
    destination_path="sessions/{session_id}/new_name.pdf"
)
```

#### Signed URLs

```python
# Generate signed URL for download
url = storage_service.generate_signed_url(
    path="sessions/{session_id}/file.pdf",
    expiration_minutes=60
)

# Generate signed URL for upload
upload_url = storage_service.generate_signed_upload_url(
    path="sessions/{session_id}/file.pdf",
    expiration_minutes=60,
    content_type="application/pdf"
)
```

### Automatic Fallback

The storage service automatically falls back to local storage if GCS is unavailable:

```python
# In development, automatically uses local storage
# In production, uses GCS with local fallback on error
storage_service.upload_file(...)  # Works in both modes
```

## Access Patterns

### Upload Pattern

```python
# 1. Receive file upload
file = request.files['file']

# 2. Generate storage path
session_id = generate_session_id()
storage_path = f"sessions/{session_id}/uploads/{file.filename}"

# 3. Upload to storage
storage_service.upload_file(
    file_content=file.read(),
    destination_path=storage_path,
    content_type=file.content_type
)

# 4. Store metadata in database
with get_db_session() as db:
    drawing = Drawing(
        session_id=session_id,
        filename=file.filename,
        storage_path=storage_path
    )
    db.add(drawing)
    db.commit()
```

### Download Pattern

```python
# 1. Get file metadata from database
with get_db_session() as db:
    drawing = db.query(Drawing).filter_by(id=drawing_id).first()
    storage_path = drawing.storage_path

# 2. Generate signed URL (GCS) or local path
if config.USE_GCS:
    url = storage_service.generate_signed_url(storage_path)
else:
    url = f"/api/files/{session_id}/{filename}"

# 3. Return URL to client
return jsonify({"url": url})
```

### Processing Pattern

```python
# 1. Download files to temporary directory
with tempfile.TemporaryDirectory() as temp_dir:
    old_path = os.path.join(temp_dir, "old.pdf")
    new_path = os.path.join(temp_dir, "new.pdf")
    
    storage_service.download_to_filename(old_storage_path, old_path)
    storage_service.download_to_filename(new_storage_path, new_path)
    
    # 2. Process files
    results = complete_drawing_pipeline(old_path, new_path)
    
    # 3. Upload results
    for overlay_path in results['overlays']:
        storage_path = f"sessions/{session_id}/results/{overlay_path}"
        storage_service.upload_from_filename(
            local_path=overlay_path,
            destination_path=storage_path
        )
```

## Performance

### Optimization Strategies

1. **Parallel Uploads**
```python
from concurrent.futures import ThreadPoolExecutor

with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [
        executor.submit(storage_service.upload_file, file, path)
        for file, path in files
    ]
    results = [f.result() for f in futures]
```

2. **Streaming Uploads**
```python
# For large files, use streaming
with open(local_path, 'rb') as f:
    storage_service.upload_file(f, destination_path)
```

3. **Batch Operations**
```python
# List files in batch
files = storage_service.list_files(prefix="sessions/{session_id}/")
```

### Caching

**Local Cache:**
- Cache file metadata
- Cache signed URLs (with expiration)
- Cache file existence checks

**CDN Integration** (Future):
- Use CDN for static assets
- Cache processed images
- Reduce GCS requests

## Security

### Access Control

**Local Storage:**
- File system permissions
- Directory isolation
- Secure filename handling

**GCS Storage:**
- IAM-based access control
- Signed URLs for temporary access
- Bucket-level permissions
- Object-level ACLs (if needed)

### Signed URLs

**Download URLs:**
- Temporary access (default: 60 minutes)
- Expiration time configurable
- HTTPS only

**Upload URLs:**
- Direct client uploads
- Temporary access
- Content type validation

### File Validation

```python
# Validate file type
ALLOWED_EXTENSIONS = {'pdf', 'dwg', 'dxf', 'png', 'jpg', 'jpeg'}
if not file.filename.endswith(tuple(ALLOWED_EXTENSIONS)):
    raise ValueError("Invalid file type")

# Validate file size
MAX_SIZE = config.MAX_CONTENT_LENGTH
if len(file_content) > MAX_SIZE:
    raise ValueError("File too large")

# Sanitize filename
filename = secure_filename(file.filename)
```

## Lifecycle Management

### GCS Lifecycle Policies

**Automatic Cleanup:**
```json
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "Delete"},
        "condition": {
          "age": 90,
          "matchesPrefix": ["sessions/"]
        }
      }
    ]
  }
}
```

**Archive Old Files:**
```json
{
  "lifecycle": {
    "rule": [
      {
        "action": {"type": "SetStorageClass", "storageClass": "NEARLINE"},
        "condition": {"age": 30}
      },
      {
        "action": {"type": "SetStorageClass", "storageClass": "COLDLINE"},
        "condition": {"age": 90}
      }
    ]
  }
}
```

### Manual Cleanup

```python
# Clean up old sessions
from datetime import datetime, timedelta

cutoff_date = datetime.utcnow() - timedelta(days=90)
old_sessions = db.query(Session).filter(
    Session.created_at < cutoff_date
).all()

for session in old_sessions:
    # Delete files
    files = storage_service.list_files(prefix=f"sessions/{session.id}/")
    for file_path in files:
        storage_service.delete_file(file_path)
    
    # Delete database records
    db.delete(session)
```

## Monitoring

### Storage Metrics

**GCS Metrics:**
- Storage usage
- Request counts
- Error rates
- Bandwidth usage

**Local Storage:**
- Disk usage
- File counts
- Directory sizes

### Alerts

**Set Up Alerts For:**
- Storage quota warnings (80%, 90%, 95%)
- High error rates
- Unusual access patterns
- Cost thresholds

---

**Next Steps**: See [CONFIGURATION.md](./CONFIGURATION.md) for storage configuration or [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for storage issues.

```


Adding a monitoring documentation file, then verifying the structure:


```markdown
# BuildTrace Monitoring & Observability

Complete guide to logging, metrics, alerting, and observability for BuildTrace.

## Table of Contents

1. [Overview](#overview)
2. [Logging](#logging)
3. [Metrics](#metrics)
4. [Alerting](#alerting)
5. [Tracing](#tracing)
6. [Dashboards](#dashboards)

## Overview

BuildTrace uses **Google Cloud Logging** and **Cloud Monitoring** for observability:

- **Logging**: Structured logs with Cloud Logging
- **Metrics**: Custom and built-in metrics
- **Alerting**: Cloud Monitoring alerts
- **Tracing**: Request tracing (future)

## Logging

### Log Levels

**Standard Levels:**
- `DEBUG`: Detailed diagnostic information
- `INFO`: General informational messages
- `WARNING`: Warning messages
- `ERROR`: Error messages
- `CRITICAL`: Critical errors

### Logging Configuration

**Local Development:**
```python
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
```

**Production (Cloud Run):**
```python
import logging

# Cloud Run automatically captures stdout/stderr
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
```

### Structured Logging

**Use Structured Logs:**
```python
import logging
import json

logger = logging.getLogger(__name__)

# Structured log
logger.info("Processing started", extra={
    "session_id": session_id,
    "file_size": file_size,
    "file_type": file_type
})
```

**JSON Format (Production):**
```python
import json
import logging

class JSONFormatter(logging.Formatter):
    def format(self, record):
        log_data = {
            "timestamp": self.formatTime(record),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        if hasattr(record, 'session_id'):
            log_data['session_id'] = record.session_id
        return json.dumps(log_data)

handler = logging.StreamHandler()
handler.setFormatter(JSONFormatter())
logger.addHandler(handler)
```

### Log Categories

**Application Logs:**
```python
logger.info("Application started", extra={"environment": config.ENVIRONMENT})
logger.info("Request received", extra={"method": request.method, "path": request.path})
logger.info("Processing completed", extra={"session_id": session_id, "duration": duration})
```

**Error Logs:**
```python
try:
    # Code
except Exception as e:
    logger.error("Processing failed", extra={
        "session_id": session_id,
        "error": str(e),
        "error_type": type(e).__name__
    }, exc_info=True)
```

**Performance Logs:**
```python
import time

start_time = time.time()
# Processing
duration = time.time() - start_time
logger.info("Processing duration", extra={
    "session_id": session_id,
    "duration_seconds": duration,
    "operation": "pipeline"
})
```

### Viewing Logs

**Local:**
```bash
tail -f app.log
grep "ERROR" app.log
```

**Cloud Run:**
```bash
# Recent logs
gcloud logging read "resource.type=cloud_run_revision" --limit=50

# Filter by service
gcloud logging read \
    "resource.type=cloud_run_revision AND resource.labels.service_name=buildtrace-overlay" \
    --limit=50

# Filter by severity
gcloud logging read \
    "resource.type=cloud_run_revision AND severity>=ERROR" \
    --limit=50

# Filter by time
gcloud logging read \
    "resource.type=cloud_run_revision AND timestamp>=\"2024-01-15T00:00:00Z\"" \
    --limit=50
```

**Cloud Console:**
- Navigate to Cloud Logging
- Filter by resource type: Cloud Run Revision
- Filter by service name, severity, etc.

## Metrics

### Built-in Metrics

**Cloud Run Metrics:**
- Request count
- Request latency
- Error rate
- CPU utilization
- Memory usage
- Instance count

**View Metrics:**
```bash
# Using gcloud
gcloud monitoring time-series list \
    --filter='resource.type="cloud_run_revision"'

# Cloud Console
# Navigate to Cloud Monitoring > Metrics Explorer
```

### Custom Metrics

**Define Custom Metrics:**
```python
from google.cloud import monitoring_v3

client = monitoring_v3.MetricServiceClient()
project_name = f"projects/{config.CLOUD_TASKS_PROJECT}"

# Create metric descriptor
descriptor = monitoring_v3.MetricDescriptor()
descriptor.type = "custom.googleapis.com/buildtrace/processing_time"
descriptor.metric_kind = monitoring_v3.MetricDescriptor.MetricKind.GAUGE
descriptor.value_type = monitoring_v3.MetricDescriptor.ValueType.DOUBLE
descriptor.description = "Processing time in seconds"

client.create_metric_descriptor(
    name=project_name,
    metric_descriptor=descriptor
)
```

**Write Metrics:**
```python
from google.cloud import monitoring_v3
import time

client = monitoring_v3.MetricServiceClient()
project_name = f"projects/{config.CLOUD_TASKS_PROJECT}"

series = monitoring_v3.TimeSeries()
series.metric.type = "custom.googleapis.com/buildtrace/processing_time"
series.resource.type = "cloud_run_revision"
series.resource.labels["service_name"] = "buildtrace-overlay"
series.resource.labels["revision_name"] = "buildtrace-overlay-00001"

point = monitoring_v3.Point()
point.value.double_value = processing_time
point.interval.end_time.seconds = int(time.time())

series.points = [point]
client.create_time_series(name=project_name, time_series=[series])
```

### Key Metrics to Track

**Application Metrics:**
- Processing time per drawing
- Alignment success rate
- AI analysis success rate
- File upload size
- Session count

**Infrastructure Metrics:**
- Database connection pool usage
- Storage operations (upload/download)
- API response times
- Error rates by endpoint

**Business Metrics:**
- Active sessions
- Drawings processed per day
- User engagement
- Feature usage

## Alerting

### Alert Policies

**Create Alert Policy:**
```bash
gcloud alpha monitoring policies create \
    --notification-channels=CHANNEL_ID \
    --display-name="High Error Rate" \
    --condition-threshold-value=0.05 \
    --condition-threshold-duration=300s \
    --condition-filter='resource.type="cloud_run_revision" AND resource.labels.service_name="buildtrace-overlay"'
```

### Common Alerts

**High Error Rate:**
```yaml
displayName: High Error Rate
conditions:
  - displayName: Error rate > 5%
    conditionThreshold:
      filter: |
        resource.type="cloud_run_revision"
        AND resource.labels.service_name="buildtrace-overlay"
        AND metric.type="run.googleapis.com/request_count"
        AND metric.labels.response_code_class="5xx"
      comparison: COMPARISON_GT
      thresholdValue: 0.05
      duration: 300s
```

**High Latency:**
```yaml
displayName: High Latency
conditions:
  - displayName: P95 latency > 2s
    conditionThreshold:
      filter: |
        resource.type="cloud_run_revision"
        AND metric.type="run.googleapis.com/request_latencies"
      comparison: COMPARISON_GT
      thresholdValue: 2000
      duration: 300s
```

**Low Instance Count:**
```yaml
displayName: Low Instance Count
conditions:
  - displayName: Instances < 1
    conditionThreshold:
      filter: |
        resource.type="cloud_run_revision"
        AND metric.type="run.googleapis.com/container/instance_count"
      comparison: COMPARISON_LT
      thresholdValue: 1
      duration: 60s
```

### Notification Channels

**Create Email Channel:**
```bash
gcloud alpha monitoring channels create \
    --type=email \
    --display-name="BuildTrace Alerts" \
    --channel-labels=email_address=alerts@example.com
```

**Create Slack Channel:**
```bash
gcloud alpha monitoring channels create \
    --type=slack \
    --display-name="BuildTrace Slack" \
    --channel-labels=channel_name=#buildtrace-alerts
```

## Tracing

### Request Tracing (Future)

**OpenTelemetry Integration:**
```python
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.cloud_trace import CloudTraceSpanExporter

trace.set_tracer_provider(TracerProvider())
tracer = trace.get_tracer(__name__)

cloud_trace_exporter = CloudTraceSpanExporter()
span_processor = BatchSpanProcessor(cloud_trace_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)

# Use in code
with tracer.start_as_current_span("process_drawing"):
    # Processing code
    pass
```

## Dashboards

### Cloud Monitoring Dashboard

**Create Dashboard:**
```bash
# Via gcloud (JSON config)
gcloud monitoring dashboards create --config-from-file=dashboard.json

# Via Cloud Console
# Navigate to Cloud Monitoring > Dashboards > Create Dashboard
```

**Dashboard Widgets:**
- Line charts for metrics over time
- Bar charts for counts
- Gauge charts for current values
- Tables for detailed data

### Key Dashboards

**Application Health:**
- Request count
- Error rate
- Latency (p50, p95, p99)
- Instance count

**Processing Metrics:**
- Processing time
- Success rate
- Files processed
- Queue depth

**Infrastructure:**
- CPU usage
- Memory usage
- Database connections
- Storage usage

## Best Practices

### Logging Best Practices

1. **Use Appropriate Levels**
   - DEBUG: Development only
   - INFO: Important events
   - WARNING: Recoverable issues
   - ERROR: Failures requiring attention

2. **Include Context**
   - Session IDs
   - User IDs
   - Request IDs
   - Timestamps

3. **Avoid Sensitive Data**
   - Never log passwords
   - Mask API keys
   - Sanitize user input

4. **Structured Logs**
   - Use JSON format in production
   - Include relevant metadata
   - Consistent field names

### Metrics Best Practices

1. **Track Key Business Metrics**
   - User engagement
   - Feature usage
   - Processing success rates

2. **Monitor Infrastructure**
   - Resource utilization
   - Error rates
   - Performance metrics

3. **Set Appropriate Thresholds**
   - Based on historical data
   - Account for normal variations
   - Avoid alert fatigue

### Alerting Best Practices

1. **Actionable Alerts**
   - Clear description
   - Include context
   - Suggest remediation

2. **Appropriate Severity**
   - Critical: Immediate action needed
   - Warning: Attention required
   - Info: Informational only

3. **Avoid Alert Fatigue**
   - Set reasonable thresholds
   - Use alert grouping
   - Review and tune regularly

---

**Next Steps**: See [TROUBLESHOOTING.md](./TROUBLESHOOTING.md) for debugging or [DEPLOYMENT.md](./DEPLOYMENT.md) for deployment monitoring.

```


Created a documentation folder for BuildTrace. Summary:

## Documentation Structure

Created 13 documentation files:

### Core Documentation
1. **README.md** - Main overview, navigation, and quick start
2. **ARCHITECTURE.md** - System architecture, design patterns, component interactions
3. **SETUP.md** - Installation and environment setup
4. **CONFIGURATION.md** - Configuration management and environment variables

### Technical Deep Dives
5. **API.md** - API reference with endpoints, request/response formats
6. **DATABASE.md** - Database schema, models, relationships, migrations
7. **PIPELINE.md** - Processing pipeline, algorithms, data flow
8. **STORAGE.md** - Storage architecture (local vs GCS), file management

### Operations & Deployment
9. **DEPLOYMENT.md** - Deployment strategies, Cloud Run, CI/CD
10. **TROUBLESHOOTING.md** - Common issues, debugging guides, solutions
11. **MONITORING.md** - Logging, metrics, alerting, observability

### Development
12. **DEVELOPMENT.md** - Development workflow, coding standards, testing
13. **ROADMAP.md** - Future plans, technical debt, improvements

## Features

- Mermaid diagrams for architecture and data flow
- Code examples throughout
- Step-by-step instructions
- Troubleshooting sections
- Cross-references between documents
- Production-ready guidance
- Onboarding-focused for new engineers

## Highlights

- Architecture diagrams showing system components
- API documentation with request/response examples
- Database schema with relationships
- Pipeline documentation explaining algorithms (SIFT, RANSAC)
- Deployment guides for Cloud Run
- Troubleshooting for common issues
- Development workflow and best practices

The documentation is self-contained and should enable a new founding engineer to understand and work with the BuildTrace project.

---

**User**

when the upload happens the file is directly uploaded to the container running the service or first to gcp then processed?

---

**Cursor**


Examining the upload flow in the code:



Reviewing the main upload endpoint:



Checking how the storage service handles uploads:



## Current Upload Flow

Files go through the container first, then to storage.

### Main Upload Endpoint (`/upload`)

1. Client uploads â†’ Flask container receives the file in memory
2. Container â†’ Storage service uploads:
   - Production (`USE_GCS=true`): Uploads to Google Cloud Storage
   - Development (`USE_GCS=false`): Saves to local filesystem

**Code Flow:**
```python
# app.py line 523-618
@app.route('/upload', methods=['POST'])
def upload_files():
    # 1. File received by Flask container (in memory)
    old_file = request.files['old_file']
    new_file = request.files['new_file']
    
    # 2. Upload to storage (GCS or local)
    old_url = storage_service.upload_file(old_file, old_storage_path)
    new_url = storage_service.upload_file(new_file, new_storage_path)
```

**Storage Service Logic:**
```python
# gcp/storage/storage_service.py line 61-94
def upload_file(self, file_content, destination_path):
    if not self.bucket:  # USE_GCS=false
        # Save to local filesystem
        return self._save_local_file(file_content, destination_path)
    else:  # USE_GCS=true
        # Upload directly to GCS
        blob.upload_from_file(file_content, rewind=True)
        return f"gs://{self.bucket_name}/{destination_path}"
```

### Alternative: Direct Upload to GCS (Signed URLs)

There's also a direct upload path that bypasses the container:

**Endpoint**: `/api/upload-urls` (line 275-344)

1. Client requests signed upload URL
2. Container generates signed URL for GCS
3. Client uploads directly to GCS (bypasses container)
4. Client notifies container when done via `/api/process-uploaded-files`

This is useful for large files to avoid container memory limits.

## Summary

- Standard `/upload`: Container receives â†’ then uploads to GCS/local
- Direct upload (`/api/upload-urls`): Client uploads directly to GCS, container only coordinates

In production with `USE_GCS=true`, files are uploaded to GCS, not stored in the container's filesystem. The container acts as a pass-through for the standard endpoint.

**Note**: Cloud Run containers are ephemeral, so files should not be stored in the container filesystem. The code correctly uploads to GCS or uses local storage only in development.

```python
# app.py line 523-618
@app.route('/upload', methods=['POST'])
def upload_files():
    # 1. File received by Flask container (in memory)
    old_file = request.files['old_file']
    new_file = request.files['new_file']
    
    # 2. Upload to storage (GCS or local)
    old_url = storage_service.upload_file(old_file, old_storage_path)
    new_url = storage_service.upload_file(new_file, new_storage_path)
```

```python
# gcp/storage/storage_service.py line 61-94
def upload_file(self, file_content, destination_path):
    if not self.bucket:  # USE_GCS=false
        # Save to local filesystem
        return self._save_local_file(file_content, destination_path)
    else:  # USE_GCS=true
        # Upload directly to GCS
        blob.upload_from_file(file_content, rewind=True)
        return f"gs://{self.bucket_name}/{destination_path}"
```

---

**User**

but as soon as the client makes the upload request it is loaded to container then sent to gcs right or no?

---

**Cursor**


Yes. For the `/upload` endpoint, the file is loaded into the container first, then sent to GCS. Flow:



Yes. For the `/upload` endpoint, the file is loaded into the container first, then sent to GCS.

## Exact Flow for `/upload` Endpoint

```
1. Client â†’ HTTP POST with multipart/form-data
   â†“
2. Flask Container â†’ Receives and parses request
   â†“
3. Container Memory â†’ File loaded into Flask's FileStorage object
   (request.files['old_file'] - this is in container memory)
   â†“
4. Container â†’ Calls storage_service.upload_file()
   â†“
5. GCS â†’ File uploaded from container to Cloud Storage
```

**Code Evidence:**

```python
# Line 531-532: Flask receives file into container memory
old_file = request.files['old_file']  # â† File is now in container
new_file = request.files['new_file']  # â† File is now in container

# Line 614-615: Reset file pointer (file is already in memory)
old_file.seek(0)
new_file.seek(0)

# Line 617-618: Upload from container memory to GCS
old_url = storage_service.upload_file(old_file, old_storage_path)
# â†‘ Reads from container memory and uploads to GCS
```

## Implications

- Memory usage: The file is held in container memory during upload
- Size limits: Limited by `MAX_CONTENT_LENGTH` (70MB default) and container memory
- Latency: Upload time = clientâ†’container + containerâ†’GCS

## Alternative: Direct Upload (Bypasses Container)

The `/api/upload-urls` endpoint allows direct upload to GCS:

```
1. Client â†’ Request signed URL from container
   â†“
2. Container â†’ Returns signed GCS upload URL
   â†“
3. Client â†’ Uploads DIRECTLY to GCS (bypasses container)
   â†“
4. Client â†’ Notifies container when done
```

This avoids loading the file into container memory.

## Summary

- `/upload`: File goes through container memory first, then to GCS
- `/api/upload-urls`: Client uploads directly to GCS, container only coordinates

For large files in production, prefer the direct upload flow to avoid container memory limits.

```plaintext
1. Client â†’ HTTP POST with multipart/form-data
   â†“
2. Flask Container â†’ Receives and parses request
   â†“
3. Container Memory â†’ File loaded into Flask's FileStorage object
   (request.files['old_file'] - this is in container memory)
   â†“
4. Container â†’ Calls storage_service.upload_file()
   â†“
5. GCS â†’ File uploaded from container to Cloud Storage
```

```python
# Line 531-532: Flask receives file into container memory
old_file = request.files['old_file']  # â† File is now in container
new_file = request.files['new_file']  # â† File is now in container

# Line 614-615: Reset file pointer (file is already in memory)
old_file.seek(0)
new_file.seek(0)

# Line 617-618: Upload from container memory to GCS
old_url = storage_service.upload_file(old_file, old_storage_path)
# â†‘ Reads from container memory and uploads to GCS
```

```plaintext
1. Client â†’ Request signed URL from container
   â†“
2. Container â†’ Returns signed GCS upload URL
   â†“
3. Client â†’ Uploads DIRECTLY to GCS (bypasses container)
   â†“
4. Client â†’ Notifies container when done
```

---

**User**

what is happening in production right now 

---

**Cursor**


Checking which upload flow the frontend uses in production:



Checking which upload method is used:



Checking the direct upload endpoint implementation:



Checking what the signed URL actually points to:



## What's happening in production

The frontend uses a size-based split:

### Small files (â‰¤15MB total)
- Uses `/upload` endpoint
- Flow: Client â†’ Container (in memory) â†’ GCS
- Code: `ProcessingMonitor.js` line 39 calls `processFilesDirectly()`

### Large files (>15MB total)
- Uses `/api/upload-direct/{path}` endpoint
- Flow: Client â†’ Container (in memory) â†’ GCS
- Code: `ProcessingMonitor.js` line 35 calls `processFilesViaCloudStorage()`

## Important finding

Even the "direct upload" path still goes through the container.

Looking at `storage_service.py` line 254:
```python
def generate_signed_upload_url(...):
    # For Cloud Run, use our own upload endpoint instead of signed URLs
    return f"/api/upload-direct/{path}"  # â† Still a container endpoint!
```

And `app.py` line 480-505 shows `/api/upload-direct` receives the file in the container:
```python
@app.route('/api/upload-direct/<path:storage_path>', methods=['PUT'])
def upload_direct_to_storage(storage_path):
    file_content = request.get_data()  # â† File received in container
    # Then uploads to GCS
    storage_service.upload_file(file_stream, storage_path, ...)
```

## Summary

In production, all uploads go through the container first, then to GCS:
- Small files: `/upload` â†’ Container â†’ GCS
- Large files: `/api/upload-direct/{path}` â†’ Container â†’ GCS

The "direct upload" is a different endpoint, but it does not bypass the container. The file is still loaded into container memory before being uploaded to GCS.

Note: The code comment mentions the service account doesn't have a private key for signing, so true GCS signed URLs aren't used. The file always passes through the container.

```python
def generate_signed_upload_url(...):
    # For Cloud Run, use our own upload endpoint instead of signed URLs
    return f"/api/upload-direct/{path}"  # â† Still a container endpoint!
```

```python
@app.route('/api/upload-direct/<path:storage_path>', methods=['PUT'])
def upload_direct_to_storage(storage_path):
    file_content = request.get_data()  # â† File received in container
    # Then uploads to GCS
    storage_service.upload_file(file_stream, storage_path, ...)
```

---

**User**

do you think it's the right approach ?

---

**Cursor**


## Current approach: container â†’ GCS

### Pros
1. Simpler implementation â€” no signed URLs or CORS
2. Server-side validation before storage
3. Immediate processing if needed
4. Easier error handling in one place

### Cons
1. Memory limits â€” Cloud Run has memory constraints (4GB web app, 32GB job processor)
2. Latency â€” extra hop: Client â†’ Container â†’ GCS
3. Cost â€” container time for large uploads
4. Timeout risk â€” Cloud Run has request timeouts (5 minutes for web app)
5. Scalability â€” container becomes a bottleneck for large files
6. Not optimal for large files â€” architectural drawings can be large

## Better approach: direct upload to GCS

For production, especially with large files, direct upload is preferable:

### Benefits
1. No container memory usage â€” files go directly to GCS
2. Faster â€” Client â†’ GCS (one hop)
3. More scalable â€” no container bottleneck
4. Lower cost â€” container only coordinates, doesn't process uploads
5. No timeout risk â€” uploads can take as long as needed
6. Better UX â€” progress tracking directly from client

### Implementation changes needed

1. Fix signed URL generation:
```python
# Current (storage_service.py line 244-260)
def generate_signed_upload_url(...):
    return f"/api/upload-direct/{path}"  # âŒ Still goes through container

# Should be:
def generate_signed_upload_url(...):
    blob = self.bucket.blob(path)
    url = blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=expiration_minutes),
        method="PUT",
        content_type=content_type
    )
    return url  # âœ… True GCS signed URL
```

2. Service account setup:
   - Grant the service account the `roles/storage.objectCreator` role
   - Or use a service account key for signing (stored as a secret)

3. CORS configuration:
   - Configure GCS bucket CORS to allow direct browser uploads

4. Upload completion flow:
   - Client uploads directly to GCS
   - Client notifies container via `/api/process-uploaded-files`
   - Container validates and starts processing

## Recommendation

For production with large files:
- Use direct upload to GCS for files >15MB (or all files)
- Keep container upload as a fallback for development/small files
- The current approach works but limits scalability and efficiency

### When current approach is acceptable
- Small files (<10MB)
- Development/testing
- When you need server-side validation before storage
- When immediate processing is required

### When direct upload is better
- Production with large files
- High traffic scenarios
- Cost optimization
- Better user experience with progress tracking

## Suggested improvement

Implement true direct upload to GCS:

1. Fix signed URL generation to use GCS signed URLs
2. Set up service account permissions
3. Configure CORS on the GCS bucket
4. Keep container endpoint as fallback for development

This would improve scalability, reduce costs, and provide a better experience for large file uploads.

Should I provide a detailed implementation plan for direct upload?

```python
# Current (storage_service.py line 244-260)
def generate_signed_upload_url(...):
    return f"/api/upload-direct/{path}"  # âŒ Still goes through container

# Should be:
def generate_signed_upload_url(...):
    blob = self.bucket.blob(path)
    url = blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=expiration_minutes),
        method="PUT",
        content_type=content_type
    )
    return url  # âœ… True GCS signed URL
```

---

**User**

give a breif suggestion 

---

**Cursor**


## Brief suggestion: implement direct GCS upload

### Quick fix (3 steps)

1. Fix signed URL generation
   - Update `gcp/storage/storage_service.py` line 244-260
   - Use GCS blob's `generate_signed_url()` with method="PUT"
   - Requires service account with signing permissions

2. Service account setup
   ```bash
   # Grant signing permission
   gcloud iam service-accounts add-iam-policy-binding \
     SERVICE_ACCOUNT_EMAIL \
     --member="serviceAccount:SERVICE_ACCOUNT_EMAIL" \
     --role="roles/iam.serviceAccountTokenCreator"
   ```

3. CORS on GCS bucket
   ```bash
   # Allow PUT from your domain
   gsutil cors set cors.json gs://buildtrace-storage
   ```

### Code change needed

```python
# In storage_service.py, replace generate_signed_upload_url():
def generate_signed_upload_url(self, path, expiration_minutes=60, content_type=None):
    if not self.bucket:
        return f"/api/upload-local/{path}"
    
    blob = self.bucket.blob(path)
    url = blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=expiration_minutes),
        method="PUT",
        content_type=content_type or "application/pdf"
    )
    return url  # Returns true GCS URL, bypasses container
```

### Result
- Files upload directly: Client â†’ GCS (bypasses container)
- Container only coordinates processing
- Better for large files, lower cost, faster

### Keep as-is if
- Files are consistently small (<10MB)
- Current setup works for your scale
- You need server-side validation before storage

Recommendation: implement direct upload for production scalability.

```shellscript
   # Grant signing permission
   gcloud iam service-accounts add-iam-policy-binding \
     SERVICE_ACCOUNT_EMAIL \
     --member="serviceAccount:SERVICE_ACCOUNT_EMAIL" \
     --role="roles/iam.serviceAccountTokenCreator"
```

```shellscript
   # Allow PUT from your domain
   gsutil cors set cors.json gs://buildtrace-storage
```

```python
# In storage_service.py, replace generate_signed_upload_url():
def generate_signed_upload_url(self, path, expiration_minutes=60, content_type=None):
    if not self.bucket:
        return f"/api/upload-local/{path}"
    
    blob = self.bucket.blob(path)
    url = blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=expiration_minutes),
        method="PUT",
        content_type=content_type or "application/pdf"
    )
    return url  # Returns true GCS URL, bypasses container
```

---

**User**

no files are currently varaiable sizes and it is failing at times 

---

**Cursor**


## Immediate fix: implement direct GCS upload

### Problem
- Variable file sizes cause failures
- Container memory limits (4GB) are hit
- Timeouts on large files
- Current approach doesn't scale

### Solution: 3 changes

**1. Fix `storage_service.py` (line 244-260):**
```python
def generate_signed_upload_url(self, path, expiration_minutes=60, content_type=None):
    if not self.bucket:
        return f"/api/upload-local/{path}"
    
    # Use GCS service account key for signing
    from google.cloud import storage
    from google.oauth2 import service_account
    import json
    
    # Get service account key from secret or env
    sa_key = json.loads(os.getenv('GCS_SERVICE_ACCOUNT_KEY', '{}'))
    credentials = service_account.Credentials.from_service_account_info(sa_key)
    client = storage.Client(credentials=credentials, project=config.CLOUD_TASKS_PROJECT)
    
    blob = client.bucket(self.bucket_name).blob(path)
    url = blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=expiration_minutes),
        method="PUT",
        content_type=content_type or "application/pdf"
    )
    return url  # True GCS URL - bypasses container
```

**2. Set service account key:**
```bash
# Create/download service account key with Storage Object Creator role
# Store as Cloud Run secret:
gcloud secrets create gcs-service-account-key --data-file=key.json
gcloud run services update buildtrace-overlay \
  --update-secrets GCS_SERVICE_ACCOUNT_KEY=gcs-service-account-key:latest
```

**3. Set CORS on bucket:**
```bash
# Create cors.json:
echo '[{"origin": ["*"], "method": ["PUT"], "responseHeader": ["Content-Type"], "maxAgeSeconds": 3600}]' > cors.json
gsutil cors set cors.json gs://buildtrace-storage
```

### Result
- Files upload directly to GCS (no container memory)
- No timeouts (GCS handles large files)
- Works for all file sizes
- Container only processes after upload

### Alternative (quick test)
If you can't set up signed URLs immediately, lower the threshold in `ProcessingMonitor.js` line 28:
```javascript
const maxDirectUploadSize = 5 * 1024 * 1024; // 5MB instead of 15MB
```
This routes more files through the direct path, but they still go through the container. The signed URL approach is the proper fix.

Should I provide the complete code changes?

```python
def generate_signed_upload_url(self, path, expiration_minutes=60, content_type=None):
    if not self.bucket:
        return f"/api/upload-local/{path}"
    
    # Use GCS service account key for signing
    from google.cloud import storage
    from google.oauth2 import service_account
    import json
    
    # Get service account key from secret or env
    sa_key = json.loads(os.getenv('GCS_SERVICE_ACCOUNT_KEY', '{}'))
    credentials = service_account.Credentials.from_service_account_info(sa_key)
    client = storage.Client(credentials=credentials, project=config.CLOUD_TASKS_PROJECT)
    
    blob = client.bucket(self.bucket_name).blob(path)
    url = blob.generate_signed_url(
        version="v4",
        expiration=timedelta(minutes=expiration_minutes),
        method="PUT",
        content_type=content_type or "application/pdf"
    )
    return url  # True GCS URL - bypasses container
```

```shellscript
# Create/download service account key with Storage Object Creator role
# Store as Cloud Run secret:
gcloud secrets create gcs-service-account-key --data-file=key.json
gcloud run services update buildtrace-overlay \
  --update-secrets GCS_SERVICE_ACCOUNT_KEY=gcs-service-account-key:latest
```

```shellscript
# Create cors.json:
echo '[{"origin": ["*"], "method": ["PUT"], "responseHeader": ["Content-Type"], "maxAgeSeconds": 3600}]' > cors.json
gsutil cors set cors.json gs://buildtrace-storage
```

```javascript
const maxDirectUploadSize = 5 * 1024 * 1024; // 5MB instead of 15MB
```

---

**User**

Chunked: Large PDFs split into manageable chunks



where is this done , how and explain in detail 

---

**Cursor**


Searching for chunked processing implementation:



## Chunked Processing: Detailed Explanation

### Where it's done

**File**: `chunked_processor.py`  
**Main Function**: `process_documents()` (line 1460)  
**Called From**: `app.py` line 424 (when processing uploaded files)

### How it works

It doesn't split PDFs into smaller files. It processes pages one at a time to manage memory.

## Detailed Flow

### 1. Entry Point

```python
# app.py line 424
results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
```

### 2. Decision: Sync vs Async

**File**: `chunked_processor.py` line 42-56

```python
def should_process_sync(self, old_pdf_path, new_pdf_path) -> bool:
    # Count pages WITHOUT loading full PDF
    old_pages = self._count_pdf_pages(old_pdf_path)  # Quick count
    new_pages = self._count_pdf_pages(new_pdf_path)
    max_pages = max(old_pages, new_pages)
    
    # If â‰¤ max_sync_pages (10 pages), process synchronously
    return max_pages <= self.max_sync_pages  # Default: 10 pages
```

**Thresholds:**
- Development: â‰¤10 pages = sync
- Production: â‰¤50 pages = sync
- Above threshold = lightweight mode (still page-by-page)

### 3. PDF to Pages Conversion (The "Chunking")

**File**: `chunked_processor.py` line 863-922

```python
def _pdf_to_pages(self, pdf_path, temp_dir, prefix) -> List[str]:
    """Convert PDF to individual page images - ONE PAGE AT A TIME"""
    
    batch_size = 1  # â† KEY: Process only 1 page at a time!
    
    for start_page in range(1, total_pages + 1, batch_size):
        # Convert ONLY this single page
        images = pdf2image.convert_from_path(
            pdf_path,
            dpi=self.dpi,
            first_page=start_page,      # â† Only this page
            last_page=start_page,       # â† Only this page
            fmt='PNG'
        )
        
        # Save page immediately
        image.save(page_path, "PNG")
        page_paths.append(page_path)
        
        # Clear from memory immediately
        image.close()
        del images
        
        # Force garbage collection
        gc.collect()
```

**What this does:**
- Converts one page at a time (not the whole PDF)
- Saves each page to disk immediately
- Clears memory after each page
- Returns list of page image paths

### 4. Page-by-Page Processing

**File**: `chunked_processor.py` line 125-180

```python
# Process matching drawings sequentially
for page_num, drawing_name in enumerate(sorted(matching_drawings), 1):
    # Check memory before each page
    if not self._check_memory_usage():
        logger.error("Memory limit exceeded, stopping")
        break
    
    # Get page indices
    old_page_index = old_drawing_map[drawing_name]['page_index']
    new_page_index = new_drawing_map[drawing_name]['page_index']
    
    # Process THIS SINGLE PAGE PAIR
    page_result = self._process_single_drawing_with_ai(
        old_pages[old_page_index],  # â† Single page image
        new_pages[new_page_index],  # â† Single page image
        drawing_name,
        page_num,
        temp_dir,
        results_dir,
        session_id
    )
    
    # Force memory cleanup after each page
    self._cleanup_page_memory()  # gc.collect()
```

### 5. Single Page Processing

**File**: `chunked_processor.py` line 376-539

For each page:
1. Create overlay (SIFT alignment)
2. Run AI analysis (if enabled)
3. Upload to storage
4. Save to database
5. Clear memory

```python
def _process_single_drawing_with_ai(self, old_page_path, new_page_path, ...):
    # 1. Create overlay (loads 2 images, processes, saves)
    overlay_path, alignment_score = self._create_simple_overlay(...)
    
    # 2. AI analysis (loads overlay image)
    analysis_result = self._run_ai_analysis(overlay_path, ...)
    
    # 3. Upload to storage
    final_overlay_path = self._copy_and_upload_image(...)
    
    # 4. Save to database
    comparison_id = self._save_comparison_to_db(...)
    
    # 5. Memory automatically freed when function returns
    return result_entry
```

## Memory Management Strategy

### Why "Chunked"?

Not file chunks, but page-by-page processing:

1. PDF conversion: one page at a time (line 868: `batch_size = 1`)
2. Processing: one drawing pair at a time
3. Memory cleanup: after each page (line 924-927)
4. Memory monitoring: checks before each page (line 969-987)

### Memory Limits

**File**: `chunked_processor.py` line 969-987

```python
def _check_memory_usage(self) -> bool:
    memory_info = self.process.memory_info()
    memory_gb = memory_info.rss / (1024 ** 3)
    
    if memory_gb > self.memory_limit_gb:
        logger.warning(f"Memory exceeded: {memory_gb:.2f} GB")
        return False  # Stop processing
    return True
```

**Limits:**
- Development: 10GB
- Production: 25GB (out of 32GB available)

## Complete Flow Diagram

```
PDF Upload
    â†“
Count Pages (quick, no loading)
    â†“
â‰¤10 pages? â†’ YES â†’ Sync Processing
    â†“ NO
Lightweight Processing
    â†“
For each page (1 at a time):
    â”œâ”€ Convert PDF page â†’ PNG (1 page only)
    â”œâ”€ Save to disk
    â”œâ”€ Clear memory
    â”œâ”€ Extract drawing name
    â”œâ”€ Find matching drawing in other PDF
    â”œâ”€ Process pair:
    â”‚   â”œâ”€ Load old page image
    â”‚   â”œâ”€ Load new page image
    â”‚   â”œâ”€ SIFT alignment
    â”‚   â”œâ”€ Create overlay
    â”‚   â”œâ”€ AI analysis
    â”‚   â”œâ”€ Upload to GCS
    â”‚   â”œâ”€ Save to database
    â”‚   â””â”€ Clear memory
    â””â”€ Next page
```

## Key Points

1. Not file chunking: processes pages sequentially, not file splits
2. One page at a time: `batch_size = 1` in `_pdf_to_pages()`
3. Memory cleanup: `gc.collect()` after each page
4. Memory monitoring: checks before each page
5. Fallback: if memory limit hit, stops processing gracefully

## Why This Approach?

**Problem**: Large PDFs (100+ pages) would:
- Use too much memory if all pages loaded at once
- Cause timeouts
- Crash the container

**Solution**: Process one page at a time:
- Low memory footprint
- Can handle any PDF size
- Graceful degradation if memory limit reached

## Example

**100-page PDF:**
- Old approach: Load all 100 pages â†’ ~5GB memory â†’ crash
- Chunked approach:
  - Page 1: Load â†’ Process â†’ Clear (50MB peak)
  - Page 2: Load â†’ Process â†’ Clear (50MB peak)
  - ... continues for all 100 pages
  - Peak memory: ~50MB (one page at a time)

This is why it's called "chunked" â€” it processes in page-sized chunks rather than loading the entire PDF.

```python
# app.py line 424
results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
```

```python
def should_process_sync(self, old_pdf_path, new_pdf_path) -> bool:
    # Count pages WITHOUT loading full PDF
    old_pages = self._count_pdf_pages(old_pdf_path)  # Quick count
    new_pages = self._count_pdf_pages(new_pdf_path)
    max_pages = max(old_pages, new_pages)
    
    # If â‰¤ max_sync_pages (10 pages), process synchronously
    return max_pages <= self.max_sync_pages  # Default: 10 pages
```

```python
def _pdf_to_pages(self, pdf_path, temp_dir, prefix) -> List[str]:
    """Convert PDF to individual page images - ONE PAGE AT A TIME"""
    
    batch_size = 1  # â† KEY: Process only 1 page at a time!
    
    for start_page in range(1, total_pages + 1, batch_size):
        # Convert ONLY this single page
        images = pdf2image.convert_from_path(
            pdf_path,
            dpi=self.dpi,
            first_page=start_page,      # â† Only this page
            last_page=start_page,       # â† Only this page
            fmt='PNG'
        )
        
        # Save page immediately
        image.save(page_path, "PNG")
        page_paths.append(page_path)
        
        # Clear from memory immediately
        image.close()
        del images
        
        # Force garbage collection
        gc.collect()
```

```python
# Process matching drawings sequentially
for page_num, drawing_name in enumerate(sorted(matching_drawings), 1):
    # Check memory before each page
    if not self._check_memory_usage():
        logger.error("Memory limit exceeded, stopping")
        break
    
    # Get page indices
    old_page_index = old_drawing_map[drawing_name]['page_index']
    new_page_index = new_drawing_map[drawing_name]['page_index']
    
    # Process THIS SINGLE PAGE PAIR
    page_result = self._process_single_drawing_with_ai(
        old_pages[old_page_index],  # â† Single page image
        new_pages[new_page_index],  # â† Single page image
        drawing_name,
        page_num,
        temp_dir,
        results_dir,
        session_id
    )
    
    # Force memory cleanup after each page
    self._cleanup_page_memory()  # gc.collect()
```

```python
def _process_single_drawing_with_ai(self, old_page_path, new_page_path, ...):
    # 1. Create overlay (loads 2 images, processes, saves)
    overlay_path, alignment_score = self._create_simple_overlay(...)
    
    # 2. AI analysis (loads overlay image)
    analysis_result = self._run_ai_analysis(overlay_path, ...)
    
    # 3. Upload to storage
    final_overlay_path = self._copy_and_upload_image(...)
    
    # 4. Save to database
    comparison_id = self._save_comparison_to_db(...)
    
    # 5. Memory automatically freed when function returns
    return result_entry
```

```python
def _check_memory_usage(self) -> bool:
    memory_info = self.process.memory_info()
    memory_gb = memory_info.rss / (1024 ** 3)
    
    if memory_gb > self.memory_limit_gb:
        logger.warning(f"Memory exceeded: {memory_gb:.2f} GB")
        return False  # Stop processing
    return True
```

```plaintext
PDF Upload
    â†“
Count Pages (quick, no loading)
    â†“
â‰¤10 pages? â†’ YES â†’ Sync Processing
    â†“ NO
Lightweight Processing
    â†“
For each page (1 at a time):
    â”œâ”€ Convert PDF page â†’ PNG (1 page only)
    â”œâ”€ Save to disk
    â”œâ”€ Clear memory
    â”œâ”€ Extract drawing name
    â”œâ”€ Find matching drawing in other PDF
    â”œâ”€ Process pair:
    â”‚   â”œâ”€ Load old page image
    â”‚   â”œâ”€ Load new page image
    â”‚   â”œâ”€ SIFT alignment
    â”‚   â”œâ”€ Create overlay
    â”‚   â”œâ”€ AI analysis
    â”‚   â”œâ”€ Upload to GCS
    â”‚   â”œâ”€ Save to database
    â”‚   â””â”€ Clear memory
    â””â”€ Next page
```

---

**User**

here the drawing is not sent to gcp for chunking but chunked from the main container 

---

**Cursor**


Confirming: chunked processing runs in the main container, not in GCP. Tracing the flow:



## Current flow: chunking happens in the main container

### Exact flow

```
1. Files in GCS
   â†“
2. Container downloads PDFs to temp directory
   (app.py line 414-418: download_to_filename)
   â†“
3. Container processes in memory/disk
   (chunked_processor.py: ALL processing happens here)
   â”œâ”€ PDF â†’ Pages conversion (container memory)
   â”œâ”€ Page-by-page processing (container memory)
   â”œâ”€ SIFT alignment (container CPU/memory)
   â”œâ”€ AI analysis (container makes API calls)
   â””â”€ Results saved to container temp directory
   â†“
4. Container uploads results back to GCS
   (chunked_processor.py line 421-429: upload images)
```

### Code evidence

**Step 1: Download from GCS to container**
```python
# app.py line 414-418
storage_service.download_to_filename(old_storage_path, old_temp_path)
# â†‘ Downloads PDF from GCS to container's /tmp directory
```

**Step 2: All processing in container**
```python
# app.py line 424
results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
# â†‘ old_temp_path is a LOCAL file path in container's /tmp
```

**Step 3: PDF chunking happens in container**
```python
# chunked_processor.py line 880-888
images = pdf2image.convert_from_path(
    pdf_path,  # â† LOCAL file path in container
    dpi=self.dpi,
    first_page=start_page,
    last_page=end_page
)
# â†‘ All conversion happens in container memory
```

## Problem

- Container downloads full PDFs from GCS
- Container does all chunking/processing
- Container memory limits (4GB web, 32GB job processor)
- Container CPU does all work
- Results uploaded back to GCS

## Better approach: distributed processing

### Option 1: Cloud Functions/Cloud Run Jobs per page
```
1. Files in GCS
   â†“
2. Container triggers Cloud Functions for each page
   â†“
3. Each Cloud Function:
   â”œâ”€ Downloads single page from GCS
   â”œâ”€ Processes that page
   â””â”€ Uploads result to GCS
   â†“
4. Container aggregates results
```

### Option 2: Cloud Run Jobs
```
1. Files in GCS
   â†“
2. Container creates Cloud Run Job
   â†“
3. Job processor (32GB container):
   â”œâ”€ Downloads PDFs
   â”œâ”€ Chunks and processes
   â””â”€ Uploads results
   â†“
4. Container polls for completion
```

### Option 3: Cloud Tasks queue
```
1. Files in GCS
   â†“
2. Container creates tasks (one per page)
   â†“
3. Worker pool processes tasks in parallel
   â†“
4. Results aggregated
```

## Current architecture issue

- Main container (4GB) does all work
- Downloads full PDFs
- Processes everything locally
- Memory/CPU constrained
- Can fail on large files

## Recommendation

Use the existing job processor service (`buildtrace-job-processor`):
- 32GB memory
- 4 CPU cores
- Designed for heavy processing
- Currently exists but may not be used for chunking

**Better flow:**
```
1. Files uploaded to GCS
   â†“
2. Main container (4GB) creates processing job
   â†“
3. Job processor (32GB) handles chunking:
   â”œâ”€ Downloads PDFs
   â”œâ”€ Chunks page-by-page
   â”œâ”€ Processes each page
   â””â”€ Uploads results
   â†“
4. Main container polls for results
```

This moves heavy processing to the dedicated 32GB job processor instead of the 4GB web container.

Should I show how to route chunked processing to the job processor?

```plaintext
1. Files in GCS
   â†“
2. Container downloads PDFs to temp directory
   (app.py line 414-418: download_to_filename)
   â†“
3. Container processes in memory/disk
   (chunked_processor.py: ALL processing happens here)
   â”œâ”€ PDF â†’ Pages conversion (container memory)
   â”œâ”€ Page-by-page processing (container memory)
   â”œâ”€ SIFT alignment (container CPU/memory)
   â”œâ”€ AI analysis (container makes API calls)
   â””â”€ Results saved to container temp directory
   â†“
4. Container uploads results back to GCS
   (chunked_processor.py line 421-429: upload images)
```

```python
# app.py line 414-418
storage_service.download_to_filename(old_storage_path, old_temp_path)
# â†‘ Downloads PDF from GCS to container's /tmp directory
```

```python
# app.py line 424
results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
# â†‘ old_temp_path is a LOCAL file path in container's /tmp
```

```python
# chunked_processor.py line 880-888
images = pdf2image.convert_from_path(
    pdf_path,  # â† LOCAL file path in container
    dpi=self.dpi,
    first_page=start_page,
    last_page=end_page
)
# â†‘ All conversion happens in container memory
```

```plaintext
1. Files in GCS
   â†“
2. Container triggers Cloud Functions for each page
   â†“
3. Each Cloud Function:
   â”œâ”€ Downloads single page from GCS
   â”œâ”€ Processes that page
   â””â”€ Uploads result to GCS
   â†“
4. Container aggregates results
```

```plaintext
1. Files in GCS
   â†“
2. Container creates Cloud Run Job
   â†“
3. Job processor (32GB container):
   â”œâ”€ Downloads PDFs
   â”œâ”€ Chunks and processes
   â””â”€ Uploads results
   â†“
4. Container polls for completion
```

```plaintext
1. Files in GCS
   â†“
2. Container creates tasks (one per page)
   â†“
3. Worker pool processes tasks in parallel
   â†“
4. Results aggregated
```

```plaintext
1. Files uploaded to GCS
   â†“
2. Main container (4GB) creates processing job
   â†“
3. Job processor (32GB) handles chunking:
   â”œâ”€ Downloads PDFs
   â”œâ”€ Chunks page-by-page
   â”œâ”€ Processes each page
   â””â”€ Uploads results
   â†“
4. Main container polls for results
```

---

**User**

yes

---

**Cursor**


Examining the job processor implementation and how to route chunked processing to it:



Showing how to route chunked processing to the job processor:

## Solution: Route Chunked Processing to Job Processor

### Current Problem

**Main Container (4GB)** does all processing:
- Downloads PDFs from GCS
- Chunks and processes pages
- Runs out of memory on large files

**Job Processor (32GB)** exists but:
- Uses old `complete_drawing_pipeline` (not chunked)
- Not being used for chunked processing

### Solution: 3 Changes

---

## Change 1: Modify Main Container to Create Jobs

**File**: `app.py` - Update `/api/process-uploaded-files` endpoint

**Current Code** (line 422-424):
```python
# Process the files using chunked_process_documents
logger.info(f"Starting processing for session {session_id}")
results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
```

**New Code**:
```python
# Check if we should use job processor (for large files or production)
from gcp.database.models import ProcessingJob
import uuid

# Determine if we should use job processor
use_job_processor = (
    config.USE_BACKGROUND_PROCESSING and 
    (config.IS_PRODUCTION or config.USE_ASYNC_PROCESSING)
)

if use_job_processor:
    # Create processing job instead of processing directly
    logger.info(f"Creating processing job for session {session_id}")
    
    with get_db_session() as db:
        # Create job record
        job = ProcessingJob(
            id=str(uuid.uuid4()),
            session_id=session_id,
            job_type='comparison',
            status='pending',
            job_metadata={
                'old_storage_path': old_storage_path,
                'new_storage_path': new_storage_path,
                'old_filename': old_filename,
                'new_filename': new_filename
            }
        )
        db.add(job)
        
        # Update session status
        session = db.query(Session).filter_by(id=session_id).first()
        if session:
            session.status = 'processing'
        db.commit()
    
    logger.info(f"Job {job.id} created, will be processed by job processor")
    
    return jsonify({
        'success': True,
        'session_id': session_id,
        'status': 'queued',
        'job_id': job.id,
        'message': 'Processing queued for background job processor'
    })
else:
    # Process directly in main container (development/small files)
    logger.info(f"Processing directly in main container for session {session_id}")
    results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
    # ... rest of existing code
```

---

## Change 2: Update Job Processor to Use Chunked Processing

**File**: `gcp/infrastructure/job_processor.py`

**Current Code** (line 154-160):
```python
# Process files
logger.info("Starting drawing comparison pipeline...")
results = complete_drawing_pipeline(
    old_pdf_path=old_temp_path,
    new_pdf_path=new_temp_path,
    dpi=300,
    debug=False,
    skip_ai_analysis=False
)
```

**New Code**:
```python
# Process files using chunked processor (memory-efficient)
logger.info("Starting chunked drawing comparison pipeline...")
from chunked_processor import process_documents as chunked_process_documents

results = chunked_process_documents(
    old_pdf_path=old_temp_path,
    new_pdf_path=new_temp_path,
    session_id=job.session_id
)

# Chunked processor already saves to database, so we just need to update job status
if results.get('success'):
    # Update session with processing time
    session.total_time = results.get('summary', {}).get('total_time', 0.0)
    session.status = 'completed'
    
    job.status = 'completed'
    job.completed_at = datetime.utcnow()
    job.job_metadata = {
        **job.job_metadata,
        'overlays_created': results.get('summary', {}).get('overlays_created', 0),
        'analyses_completed': results.get('summary', {}).get('analyses_completed', 0),
        'pages_processed': results.get('pages_processed', 0)
    }
    logger.info(f"Job {job.id} completed successfully")
else:
    raise Exception(f"Chunked processing failed: {results.get('error', 'Unknown error')}")
```

**Also update** `store_processing_results` function (line 17-90) - it's not needed since `chunked_processor` already saves to database, but keep it as fallback:

```python
def store_processing_results(session_id, results, db):
    """Store processing results in the database (fallback - chunked_processor already saves)"""
    # Chunked processor already saves comparisons and analysis results
    # This is kept as fallback for non-chunked processing
    try:
        # Only update session metadata if chunked processor didn't
        session = db.query(Session).filter_by(id=session_id).first()
        if session and not session.total_time:
            session.total_time = results.get('summary', {}).get('total_time', 0.0)
            db.commit()
        logger.info("Session metadata updated")
    except Exception as e:
        logger.warning(f"Could not update session metadata: {e}")
```

---

## Change 3: Update Job Processor Entry Point

**File**: `gcp/infrastructure/job_processor.py` - Update main block

**Current Code** (line 191-193):
```python
if __name__ == "__main__":
    processor = JobProcessor()
    processor.process_jobs()
```

**New Code** (add error handling and graceful shutdown):
```python
if __name__ == "__main__":
    import signal
    
    processor = JobProcessor()
    
    def signal_handler(sig, frame):
        logger.info("Shutting down job processor...")
        processor.running = False
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        processor.process_jobs()
    except KeyboardInterrupt:
        logger.info("Job processor stopped by user")
    except Exception as e:
        logger.error(f"Job processor crashed: {e}")
        raise
```

---

## Complete Flow After Changes

```
1. Client uploads files â†’ GCS
   â†“
2. Main Container (4GB):
   â”œâ”€ Receives upload notification
   â”œâ”€ Creates ProcessingJob record (status='pending')
   â””â”€ Returns immediately: "Processing queued"
   â†“
3. Job Processor (32GB):
   â”œâ”€ Polls database for pending jobs
   â”œâ”€ Finds job â†’ Downloads PDFs from GCS
   â”œâ”€ Uses chunked_processor:
   â”‚   â”œâ”€ Converts PDF â†’ Pages (1 at a time)
   â”‚   â”œâ”€ Processes each page
   â”‚   â”œâ”€ Saves to database
   â”‚   â””â”€ Uploads results to GCS
   â”œâ”€ Updates job status â†’ 'completed'
   â””â”€ Updates session status â†’ 'completed'
   â†“
4. Main Container:
   â”œâ”€ Client polls /api/session/{id}/status
   â””â”€ Returns results when ready
```

---

## Configuration

**File**: `config.py` - Ensure these are set:

```python
# Production
USE_BACKGROUND_PROCESSING = True  # Enable job processor
USE_ASYNC_PROCESSING = True       # Use async flow

# Development (optional - can process directly)
USE_BACKGROUND_PROCESSING = False  # Process in main container
```

---

## Benefits

1. Main container (4GB) only coordinates, doesn't process
2. Job processor (32GB) handles heavy work
3. Memory-efficient: chunked processing in 32GB container
4. Scalable: multiple job processor instances can run
5. No timeouts: job processor has 60-minute timeout
6. Better reliability: job processor can retry failed jobs

---

## Testing

**Test Job Creation:**
```python
# In main container
from gcp.database.models import ProcessingJob
jobs = db.query(ProcessingJob).filter_by(status='pending').all()
print(f"Pending jobs: {len(jobs)}")
```

**Test Job Processor:**
```bash
# Check job processor logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=buildtrace-job-processor" --limit=50
```

---

## Summary

- Main container: Creates `ProcessingJob` records, returns immediately
- Job processor: Polls for jobs, uses `chunked_process_documents`, saves results
- Result: Heavy processing moves to the 32GB job processor, main container stays lightweight

Should I provide the complete modified code files?

```python
# Process the files using chunked_process_documents
logger.info(f"Starting processing for session {session_id}")
results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
```

```python
# Check if we should use job processor (for large files or production)
from gcp.database.models import ProcessingJob
import uuid

# Determine if we should use job processor
use_job_processor = (
    config.USE_BACKGROUND_PROCESSING and 
    (config.IS_PRODUCTION or config.USE_ASYNC_PROCESSING)
)

if use_job_processor:
    # Create processing job instead of processing directly
    logger.info(f"Creating processing job for session {session_id}")
    
    with get_db_session() as db:
        # Create job record
        job = ProcessingJob(
            id=str(uuid.uuid4()),
            session_id=session_id,
            job_type='comparison',
            status='pending',
            job_metadata={
                'old_storage_path': old_storage_path,
                'new_storage_path': new_storage_path,
                'old_filename': old_filename,
                'new_filename': new_filename
            }
        )
        db.add(job)
        
        # Update session status
        session = db.query(Session).filter_by(id=session_id).first()
        if session:
            session.status = 'processing'
        db.commit()
    
    logger.info(f"Job {job.id} created, will be processed by job processor")
    
    return jsonify({
        'success': True,
        'session_id': session_id,
        'status': 'queued',
        'job_id': job.id,
        'message': 'Processing queued for background job processor'
    })
else:
    # Process directly in main container (development/small files)
    logger.info(f"Processing directly in main container for session {session_id}")
    results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
    # ... rest of existing code
```

```python
# Process files
logger.info("Starting drawing comparison pipeline...")
results = complete_drawing_pipeline(
    old_pdf_path=old_temp_path,
    new_pdf_path=new_temp_path,
    dpi=300,
    debug=False,
    skip_ai_analysis=False
)
```

```python
# Process files using chunked processor (memory-efficient)
logger.info("Starting chunked drawing comparison pipeline...")
from chunked_processor import process_documents as chunked_process_documents

results = chunked_process_documents(
    old_pdf_path=old_temp_path,
    new_pdf_path=new_temp_path,
    session_id=job.session_id
)

# Chunked processor already saves to database, so we just need to update job status
if results.get('success'):
    # Update session with processing time
    session.total_time = results.get('summary', {}).get('total_time', 0.0)
    session.status = 'completed'
    
    job.status = 'completed'
    job.completed_at = datetime.utcnow()
    job.job_metadata = {
        **job.job_metadata,
        'overlays_created': results.get('summary', {}).get('overlays_created', 0),
        'analyses_completed': results.get('summary', {}).get('analyses_completed', 0),
        'pages_processed': results.get('pages_processed', 0)
    }
    logger.info(f"Job {job.id} completed successfully")
else:
    raise Exception(f"Chunked processing failed: {results.get('error', 'Unknown error')}")
```

```python
def store_processing_results(session_id, results, db):
    """Store processing results in the database (fallback - chunked_processor already saves)"""
    # Chunked processor already saves comparisons and analysis results
    # This is kept as fallback for non-chunked processing
    try:
        # Only update session metadata if chunked processor didn't
        session = db.query(Session).filter_by(id=session_id).first()
        if session and not session.total_time:
            session.total_time = results.get('summary', {}).get('total_time', 0.0)
            db.commit()
        logger.info("Session metadata updated")
    except Exception as e:
        logger.warning(f"Could not update session metadata: {e}")
```

```python
if __name__ == "__main__":
    processor = JobProcessor()
    processor.process_jobs()
```

```python
if __name__ == "__main__":
    import signal
    
    processor = JobProcessor()
    
    def signal_handler(sig, frame):
        logger.info("Shutting down job processor...")
        processor.running = False
    
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        processor.process_jobs()
    except KeyboardInterrupt:
        logger.info("Job processor stopped by user")
    except Exception as e:
        logger.error(f"Job processor crashed: {e}")
        raise
```

```plaintext
1. Client uploads files â†’ GCS
   â†“
2. Main Container (4GB):
   â”œâ”€ Receives upload notification
   â”œâ”€ Creates ProcessingJob record (status='pending')
   â””â”€ Returns immediately: "Processing queued"
   â†“
3. Job Processor (32GB):
   â”œâ”€ Polls database for pending jobs
   â”œâ”€ Finds job â†’ Downloads PDFs from GCS
   â”œâ”€ Uses chunked_processor:
   â”‚   â”œâ”€ Converts PDF â†’ Pages (1 at a time)
   â”‚   â”œâ”€ Processes each page
   â”‚   â”œâ”€ Saves to database
   â”‚   â””â”€ Uploads results to GCS
   â”œâ”€ Updates job status â†’ 'completed'
   â””â”€ Updates session status â†’ 'completed'
   â†“
4. Main Container:
   â”œâ”€ Client polls /api/session/{id}/status
   â””â”€ Returns results when ready
```

```python
# Production
USE_BACKGROUND_PROCESSING = True  # Enable job processor
USE_ASYNC_PROCESSING = True       # Use async flow

# Development (optional - can process directly)
USE_BACKGROUND_PROCESSING = False  # Process in main container
```

```python
# In main container
from gcp.database.models import ProcessingJob
jobs = db.query(ProcessingJob).filter_by(status='pending').all()
print(f"Pending jobs: {len(jobs)}")
```

```shellscript
# Check job processor logs
gcloud logging read "resource.type=cloud_run_revision AND resource.labels.service_name=buildtrace-job-processor" --limit=50
```

---

**User**

a scalable system would be one that uplaods to gcs and then processes in chunks all the time, or it wrong? evaluate and idscuss

---

**Cursor**


Evaluating scalable architectures and tradeoffs:



## Evaluation: Always chunked vs hybrid approach

### Your suggestion: upload to GCS + always process in chunks

**Pros:**
- Consistent behavior
- Handles any file size
- Predictable memory usage
- Simpler logic (no size checks)

**Cons:**
- Overhead for small files (1â€“2 pages)
- Slower for small files (sequential processing)
- Higher latency (page-by-page adds up)
- More database writes per page

### Current approach: hybrid (sync for small, chunked for large)

**Pros:**
- Fast for small files (parallel processing possible)
- Efficient resource use
- Lower latency for typical cases
- Fewer database operations for small files

**Cons:**
- More complex logic
- Two code paths to maintain
- Threshold decisions needed

## Scalability analysis

### True scalability: distributed processing

The most scalable approach is not just â€œalways chunked,â€ but distributed processing:

```
Current (Even with Always Chunked):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Container â”‚ Downloads PDF â†’ Chunks â†’ Processes sequentially
â”‚   (32GB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  One bottleneck

True Scalability:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GCS       â”‚ PDF stored
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestratorâ”‚ Creates tasks (one per page)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Workerâ”‚ â”‚Workerâ”‚Workerâ”‚ â”‚Workerâ”‚  â”‚ Process pages in parallel
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Results   â”‚ Aggregated
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture comparison

### Option 1: Always chunked (single container)

```
Flow:
1. Upload to GCS âœ…
2. Download to container
3. Process page-by-page sequentially
4. Upload results

Scalability: âš ï¸ Limited
- Single container bottleneck
- Sequential processing
- Can't parallelize across containers
- Memory constrained (even 32GB has limits)
```

### Option 2: Distributed chunked (your suggestion improved)

```
Flow:
1. Upload to GCS âœ…
2. Extract page list (without downloading)
3. Create tasks (one per page pair)
4. Workers process pages in parallel
5. Aggregate results

Scalability: âœ… Excellent
- Parallel processing
- Auto-scaling workers
- No single bottleneck
- Handles any size
```

### Option 3: Hybrid with smart routing

```
Flow:
1. Upload to GCS âœ…
2. Quick page count check
3. Small (â‰¤5 pages): Process in single container (fast)
4. Large (>5 pages): Distributed processing (scalable)

Scalability: âœ… Good
- Fast for common case
- Scalable for large files
- Best of both worlds
```

## Recommendation: distributed chunked processing

### Architecture

```
1. Upload to GCS (direct from client)
   â†“
2. Main Container:
   â”œâ”€ Extracts page list (lightweight, no download)
   â”œâ”€ Creates Cloud Tasks (one per page pair)
   â””â”€ Returns immediately
   â†“
3. Worker Pool (auto-scaling):
   â”œâ”€ Each worker processes one page pair
   â”œâ”€ Downloads only needed pages from GCS
   â”œâ”€ Processes independently
   â””â”€ Uploads result to GCS
   â†“
4. Aggregator:
   â”œâ”€ Collects all page results
   â”œâ”€ Creates summary
   â””â”€ Updates database
```

### Implementation

**Step 1: Extract page list without downloading**
```python
def extract_page_list(pdf_gcs_path):
    """Extract page count and drawing names from PDF in GCS"""
    # Use PyMuPDF to read PDF metadata only
    # Returns: [(page_num, drawing_name), ...]
    # No full PDF download needed
```

**Step 2: Create distributed tasks**
```python
def create_page_tasks(session_id, old_pdf_path, new_pdf_path):
    """Create one Cloud Task per page pair"""
    old_pages = extract_page_list(old_pdf_path)  # Lightweight
    new_pages = extract_page_list(new_pdf_path)
    
    matching_pages = match_pages(old_pages, new_pages)
    
    for old_page_num, new_page_num, drawing_name in matching_pages:
        create_cloud_task(
            endpoint='/api/process-page',
            payload={
                'session_id': session_id,
                'old_pdf_path': old_pdf_path,
                'new_pdf_path': new_pdf_path,
                'old_page_num': old_page_num,
                'new_page_num': new_page_num,
                'drawing_name': drawing_name
            }
        )
```

**Step 3: Worker processes single page**
```python
@app.route('/api/process-page', methods=['POST'])
def process_single_page():
    """Process one page pair - runs in worker container"""
    data = request.json
    old_pdf_path = data['old_pdf_path']
    page_num = data['old_page_num']
    
    # Download ONLY this page from GCS
    old_page_image = download_single_page_from_pdf(old_pdf_path, page_num)
    new_page_image = download_single_page_from_pdf(new_pdf_path, page_num)
    
    # Process this single page
    result = process_page_pair(old_page_image, new_page_image, ...)
    
    # Upload result
    upload_result_to_gcs(result)
    
    return jsonify({'success': True})
```

## Comparison table

| Approach | Scalability | Latency | Complexity | Cost |
|----------|------------|---------|------------|------|
| Always chunked (single container) | âš ï¸ Limited | âš ï¸ High (sequential) | âœ… Simple | âœ… Low |
| Hybrid (current) | âš ï¸ Limited | âœ… Good | âš ï¸ Medium | âœ… Low |
| Distributed chunked | âœ… Excellent | âœ… Good (parallel) | âš ï¸ Complex | âš ï¸ Higher |
| Hybrid distributed | âœ… Excellent | âœ… Best | âš ï¸ Most complex | âš ï¸ Medium |

## Final recommendation

For maximum scalability:
1. Upload to GCS (direct from client)
2. Always use distributed processing (not just chunked)
3. Process pages in parallel across workers
4. Optional: keep fast path for very small files (1â€“2 pages)

### Why not always chunked in a single container?

- Sequential bottleneck: pages processed one at a time
- Memory limits: even 32GB can be exceeded
- No horizontal scaling: can't add more containers
- Single point of failure

### Why distributed chunked?

- Parallel processing: multiple pages simultaneously
- Auto-scaling: workers scale with load
- Fault tolerance: one page failure doesn't stop others
- True scalability: handles any file size

## Practical implementation path

**Phase 1 (Quick win):**
- Always use chunked processing (even for small files)
- Route to 32GB job processor
- Better than current, but still limited

**Phase 2 (True scalability):**
- Implement distributed processing
- One task per page pair
- Auto-scaling worker pool
- Maximum scalability

Should I provide the complete distributed processing implementation?

```plaintext
Current (Even with Always Chunked):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Container â”‚ Downloads PDF â†’ Chunks â†’ Processes sequentially
â”‚   (32GB)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
  One bottleneck

True Scalability:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   GCS       â”‚ PDF stored
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Orchestratorâ”‚ Creates tasks (one per page)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚Workerâ”‚ â”‚Workerâ”‚Workerâ”‚ â”‚Workerâ”‚  â”‚ Process pages in parallel
â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜
     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Results   â”‚ Aggregated
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

```plaintext
Flow:
1. Upload to GCS âœ…
2. Download to container
3. Process page-by-page sequentially
4. Upload results

Scalability: âš ï¸ Limited
- Single container bottleneck
- Sequential processing
- Can't parallelize across containers
- Memory constrained (even 32GB has limits)
```

```plaintext
Flow:
1. Upload to GCS âœ…
2. Extract page list (without downloading)
3. Create tasks (one per page pair)
4. Workers process pages in parallel
5. Aggregate results

Scalability: âœ… Excellent
- Parallel processing
- Auto-scaling workers
- No single bottleneck
- Handles any size
```

```plaintext
Flow:
1. Upload to GCS âœ…
2. Quick page count check
3. Small (â‰¤5 pages): Process in single container (fast)
4. Large (>5 pages): Distributed processing (scalable)

Scalability: âœ… Good
- Fast for common case
- Scalable for large files
- Best of both worlds
```

```plaintext
1. Upload to GCS (direct from client)
   â†“
2. Main Container:
   â”œâ”€ Extracts page list (lightweight, no download)
   â”œâ”€ Creates Cloud Tasks (one per page pair)
   â””â”€ Returns immediately
   â†“
3. Worker Pool (auto-scaling):
   â”œâ”€ Each worker processes one page pair
   â”œâ”€ Downloads only needed pages from GCS
   â”œâ”€ Processes independently
   â””â”€ Uploads result to GCS
   â†“
4. Aggregator:
   â”œâ”€ Collects all page results
   â”œâ”€ Creates summary
   â””â”€ Updates database
```

```python
def extract_page_list(pdf_gcs_path):
    """Extract page count and drawing names from PDF in GCS"""
    # Use PyMuPDF to read PDF metadata only
    # Returns: [(page_num, drawing_name), ...]
    # No full PDF download needed
```

```python
def create_page_tasks(session_id, old_pdf_path, new_pdf_path):
    """Create one Cloud Task per page pair"""
    old_pages = extract_page_list(old_pdf_path)  # Lightweight
    new_pages = extract_page_list(new_pdf_path)
    
    matching_pages = match_pages(old_pages, new_pages)
    
    for old_page_num, new_page_num, drawing_name in matching_pages:
        create_cloud_task(
            endpoint='/api/process-page',
            payload={
                'session_id': session_id,
                'old_pdf_path': old_pdf_path,
                'new_pdf_path': new_pdf_path,
                'old_page_num': old_page_num,
                'new_page_num': new_page_num,
                'drawing_name': drawing_name
            }
        )
```

```python
@app.route('/api/process-page', methods=['POST'])
def process_single_page():
    """Process one page pair - runs in worker container"""
    data = request.json
    old_pdf_path = data['old_pdf_path']
    page_num = data['old_page_num']
    
    # Download ONLY this page from GCS
    old_page_image = download_single_page_from_pdf(old_pdf_path, page_num)
    new_page_image = download_single_page_from_pdf(new_pdf_path, page_num)
    
    # Process this single page
    result = process_page_pair(old_page_image, new_page_image, ...)
    
    # Upload result
    upload_result_to_gcs(result)
    
    return jsonify({'success': True})
```

---

**User**

even though current chunking has pone flaw that is loading the large file intp memory 

---

**Cursor**


Confirming the flaw: the entire PDF is downloaded before chunking. Tracing the code:



## Critical flaw: full PDF download before chunking

### The problem

**Current flow:**
```
1. PDF in GCS (e.g., 100MB, 200 pages)
   â†“
2. Container downloads ENTIRE PDF to /tmp
   (app.py line 414-418: download_to_filename)
   â†“
3. Full PDF now in container disk/memory
   â†“
4. THEN chunked processing starts
   (chunked_processor.py: processes page-by-page)
```

**Code evidence:**

```python
# app.py line 414-418
storage_service.download_to_filename(old_storage_path, old_temp_path)
# â†‘ Downloads ENTIRE 100MB PDF to container

# chunked_processor.py line 880
images = pdf2image.convert_from_path(
    pdf_path,  # â† Requires full PDF file on disk
    first_page=start_page,
    last_page=end_page
)
# â†‘ Even though it processes one page, it needs the full PDF file
```

### Why this is a problem

1. Memory/disk usage: full PDF must be downloaded first
   - 100MB PDF â†’ 100MB in container before processing starts
   - 500MB PDF â†’ 500MB in container
2. `pdf2image` limitation: requires a local file path
   - Cannot stream from GCS
   - Must have the full PDF file accessible
3. Time waste: download happens before any processing
   - Large file download can take minutes
   - Processing can't start until download completes

## Solution: stream pages directly from GCS

### Option 1: Use PyMuPDF with GCS streaming (recommended)

**Why PyMuPDF (fitz):**
- Can work with file-like objects (streams)
- Can read specific pages without loading the full PDF
- More memory-efficient than pdf2image

**Implementation:**

```python
def _pdf_to_pages_from_gcs(self, gcs_path: str, temp_dir: str, prefix: str) -> List[str]:
    """Convert PDF pages directly from GCS without downloading full PDF"""
    import fitz  # PyMuPDF
    from gcp.storage import storage_service
    import io
    
    page_paths = []
    
    # Download PDF metadata only (page count)
    total_pages = self._count_pdf_pages_from_gcs(gcs_path)
    
    # Process each page individually
    for page_num in range(1, total_pages + 1):
        try:
            # Stream ONLY this page from GCS
            pdf_stream = storage_service.download_file(gcs_path)  # Still downloads full PDF
            # â†‘ Problem: Still downloads full PDF
            
            # Better: Use range requests to get specific page
            # (Requires GCS range request support)
            
        except Exception as e:
            logger.error(f"Error processing page {page_num}: {e}")
            continue
    
    return page_paths
```

**Problem:** GCS doesn't easily support extracting a single page from a PDF without reading the file structure.

### Option 2: Pre-split PDF into pages in GCS (best for scalability)

**Architecture:**
```
1. Upload PDF to GCS
   â†“
2. Split PDF into individual pages in GCS
   (One Cloud Function/Job per page extraction)
   â†“
3. Store pages in GCS:
   gs://bucket/sessions/{id}/pages/
     â”œâ”€â”€ old_page_001.png
     â”œâ”€â”€ old_page_002.png
     â”œâ”€â”€ new_page_001.png
     â””â”€â”€ new_page_002.png
   â†“
4. Process pages independently (no full PDF download)
```

**Implementation:**

```python
def split_pdf_to_pages_in_gcs(pdf_gcs_path: str, session_id: str):
    """Split PDF into individual pages and store in GCS"""
    import fitz
    from gcp.storage import storage_service
    
    # Download PDF metadata only (to get page count)
    pdf_bytes = storage_service.download_file(pdf_gcs_path)
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    
    total_pages = len(doc)
    
    # Extract each page and upload to GCS
    for page_num in range(total_pages):
        page = doc[page_num]
        
        # Render page to image
        pix = page.get_pixmap(dpi=300)
        page_image_bytes = pix.tobytes("png")
        
        # Upload page directly to GCS
        page_path = f"sessions/{session_id}/pages/old_page_{page_num+1:03d}.png"
        storage_service.upload_file(
            page_image_bytes,
            page_path,
            content_type="image/png"
        )
        
        # Clear from memory immediately
        pix = None
        page = None
    
    doc.close()
    return total_pages
```

### Option 3: Stream with range requests (complex but efficient)

**Use HTTP range requests to read PDF structure:**

```python
def extract_page_from_gcs_range(gcs_path: str, page_num: int):
    """Extract single page using HTTP range requests"""
    # This is complex because PDF structure is non-linear
    # Would need to:
    # 1. Read PDF header/catalog (small range)
    # 2. Find page object location
    # 3. Read page object (another range)
    # 4. Read page content (another range)
    # Very complex, not recommended
```

## Recommended solution: pre-split architecture

### Flow

```
1. Client uploads PDF â†’ GCS
   â†“
2. Trigger page extraction job
   â†“
3. Split PDF into pages in GCS (parallel):
   â”œâ”€ Cloud Function 1: Extract page 1 â†’ GCS
   â”œâ”€ Cloud Function 2: Extract page 2 â†’ GCS
   â”œâ”€ Cloud Function 3: Extract page 3 â†’ GCS
   â””â”€ ... (all in parallel)
   â†“
4. Process pages independently:
   â”œâ”€ Worker 1: Download page_001.png â†’ Process â†’ Upload result
   â”œâ”€ Worker 2: Download page_002.png â†’ Process â†’ Upload result
   â””â”€ ... (all in parallel, no full PDF needed)
```

### Benefits

1. No full PDF download: only individual pages
2. True parallelization: pages processed simultaneously
3. Memory efficient: each worker handles one page
4. Scalable: auto-scaling workers
5. Fault tolerant: one page failure doesn't affect others

### Implementation sketch

**Step 1: Split PDF on upload**
```python
@app.route('/api/split-pdf', methods=['POST'])
def split_pdf_to_pages():
    """Split uploaded PDF into individual pages in GCS"""
    session_id = request.json['session_id']
    pdf_gcs_path = request.json['pdf_path']
    
    # Create Cloud Tasks for each page extraction
    total_pages = get_pdf_page_count_from_gcs(pdf_gcs_path)
    
    for page_num in range(1, total_pages + 1):
        create_cloud_task(
            endpoint='/api/extract-page',
            payload={
                'session_id': session_id,
                'pdf_gcs_path': pdf_gcs_path,
                'page_num': page_num
            }
        )
```

**Step 2: Extract single page**
```python
@app.route('/api/extract-page', methods=['POST'])
def extract_single_page():
    """Extract one page from PDF in GCS"""
    pdf_gcs_path = request.json['pdf_gcs_path']
    page_num = request.json['page_num']
    
    # Download PDF (still needed, but only once per page extraction)
    # OR use PyMuPDF with streaming
    pdf_bytes = storage_service.download_file(pdf_gcs_path)
    
    # Extract only this page
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    page = doc[page_num - 1]
    pix = page.get_pixmap(dpi=300)
    page_image = pix.tobytes("png")
    doc.close()
    
    # Upload page to GCS
    page_path = f"sessions/{session_id}/pages/page_{page_num:03d}.png"
    storage_service.upload_file(page_image, page_path)
    
    return jsonify({'success': True})
```

**Step 3: Process pages independently**
```python
@app.route('/api/process-page', methods=['POST'])
def process_single_page():
    """Process one page pair - no full PDF needed"""
    old_page_gcs_path = request.json['old_page_path']  # Already extracted
    new_page_gcs_path = request.json['new_page_path']  # Already extracted
    
    # Download only these 2 page images (small, ~5MB each)
    old_page = storage_service.download_file(old_page_gcs_path)
    new_page = storage_service.download_file(new_page_gcs_path)
    
    # Process (no full PDF in memory)
    result = process_page_pair(old_page, new_page)
    
    return jsonify({'success': True})
```

## Comparison

| Approach | Full PDF Download | Memory Usage | Scalability |
|----------|------------------|--------------|-------------|
| Current chunked | Yes (before chunking) | High (full PDF) | Limited |
| Stream with PyMuPDF | Partial (structure only) | Medium | Better |
| Pre-split pages | No (only pages) | Low (one page) | Excellent |

## Final recommendation

1. Pre-split PDFs into pages in GCS on upload
2. Process pages independently (no full PDF download)
3. True parallel processing across workers
4. Maximum scalability and memory efficiency

This removes the flaw: no full PDF download, only individual pages.

Should I provide the complete pre-split implementation?

```plaintext
1. PDF in GCS (e.g., 100MB, 200 pages)
   â†“
2. Container downloads ENTIRE PDF to /tmp
   (app.py line 414-418: download_to_filename)
   â†“
3. Full PDF now in container disk/memory
   â†“
4. THEN chunked processing starts
   (chunked_processor.py: processes page-by-page)
```

```python
# app.py line 414-418
storage_service.download_to_filename(old_storage_path, old_temp_path)
# â†‘ Downloads ENTIRE 100MB PDF to container

# chunked_processor.py line 880
images = pdf2image.convert_from_path(
    pdf_path,  # â† Requires full PDF file on disk
    first_page=start_page,
    last_page=end_page
)
# â†‘ Even though it processes one page, it needs the full PDF file
```

```python
def _pdf_to_pages_from_gcs(self, gcs_path: str, temp_dir: str, prefix: str) -> List[str]:
    """Convert PDF pages directly from GCS without downloading full PDF"""
    import fitz  # PyMuPDF
    from gcp.storage import storage_service
    import io
    
    page_paths = []
    
    # Download PDF metadata only (page count)
    total_pages = self._count_pdf_pages_from_gcs(gcs_path)
    
    # Process each page individually
    for page_num in range(1, total_pages + 1):
        try:
            # Stream ONLY this page from GCS
            pdf_stream = storage_service.download_file(gcs_path)  # Still downloads full PDF
            # â†‘ Problem: Still downloads full PDF
            
            # Better: Use range requests to get specific page
            # (Requires GCS range request support)
            
        except Exception as e:
            logger.error(f"Error processing page {page_num}: {e}")
            continue
    
    return page_paths
```

```plaintext
1. Upload PDF to GCS
   â†“
2. Split PDF into individual pages in GCS
   (One Cloud Function/Job per page extraction)
   â†“
3. Store pages in GCS:
   gs://bucket/sessions/{id}/pages/
     â”œâ”€â”€ old_page_001.png
     â”œâ”€â”€ old_page_002.png
     â”œâ”€â”€ new_page_001.png
     â””â”€â”€ new_page_002.png
   â†“
4. Process pages independently (no full PDF download)
```

```python
def split_pdf_to_pages_in_gcs(pdf_gcs_path: str, session_id: str):
    """Split PDF into individual pages and store in GCS"""
    import fitz
    from gcp.storage import storage_service
    
    # Download PDF metadata only (to get page count)
    pdf_bytes = storage_service.download_file(pdf_gcs_path)
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    
    total_pages = len(doc)
    
    # Extract each page and upload to GCS
    for page_num in range(total_pages):
        page = doc[page_num]
        
        # Render page to image
        pix = page.get_pixmap(dpi=300)
        page_image_bytes = pix.tobytes("png")
        
        # Upload page directly to GCS
        page_path = f"sessions/{session_id}/pages/old_page_{page_num+1:03d}.png"
        storage_service.upload_file(
            page_image_bytes,
            page_path,
            content_type="image/png"
        )
        
        # Clear from memory immediately
        pix = None
        page = None
    
    doc.close()
    return total_pages
```

```python
def extract_page_from_gcs_range(gcs_path: str, page_num: int):
    """Extract single page using HTTP range requests"""
    # This is complex because PDF structure is non-linear
    # Would need to:
    # 1. Read PDF header/catalog (small range)
    # 2. Find page object location
    # 3. Read page object (another range)
    # 4. Read page content (another range)
    # Very complex, not recommended
```

```plaintext
1. Client uploads PDF â†’ GCS
   â†“
2. Trigger page extraction job
   â†“
3. Split PDF into pages in GCS (parallel):
   â”œâ”€ Cloud Function 1: Extract page 1 â†’ GCS
   â”œâ”€ Cloud Function 2: Extract page 2 â†’ GCS
   â”œâ”€ Cloud Function 3: Extract page 3 â†’ GCS
   â””â”€ ... (all in parallel)
   â†“
4. Process pages independently:
   â”œâ”€ Worker 1: Download page_001.png â†’ Process â†’ Upload result
   â”œâ”€ Worker 2: Download page_002.png â†’ Process â†’ Upload result
   â””â”€ ... (all in parallel, no full PDF needed)
```

```python
@app.route('/api/split-pdf', methods=['POST'])
def split_pdf_to_pages():
    """Split uploaded PDF into individual pages in GCS"""
    session_id = request.json['session_id']
    pdf_gcs_path = request.json['pdf_path']
    
    # Create Cloud Tasks for each page extraction
    total_pages = get_pdf_page_count_from_gcs(pdf_gcs_path)
    
    for page_num in range(1, total_pages + 1):
        create_cloud_task(
            endpoint='/api/extract-page',
            payload={
                'session_id': session_id,
                'pdf_gcs_path': pdf_gcs_path,
                'page_num': page_num
            }
        )
```

```python
@app.route('/api/extract-page', methods=['POST'])
def extract_single_page():
    """Extract one page from PDF in GCS"""
    pdf_gcs_path = request.json['pdf_gcs_path']
    page_num = request.json['page_num']
    
    # Download PDF (still needed, but only once per page extraction)
    # OR use PyMuPDF with streaming
    pdf_bytes = storage_service.download_file(pdf_gcs_path)
    
    # Extract only this page
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    page = doc[page_num - 1]
    pix = page.get_pixmap(dpi=300)
    page_image = pix.tobytes("png")
    doc.close()
    
    # Upload page to GCS
    page_path = f"sessions/{session_id}/pages/page_{page_num:03d}.png"
    storage_service.upload_file(page_image, page_path)
    
    return jsonify({'success': True})
```

```python
@app.route('/api/process-page', methods=['POST'])
def process_single_page():
    """Process one page pair - no full PDF needed"""
    old_page_gcs_path = request.json['old_page_path']  # Already extracted
    new_page_gcs_path = request.json['new_page_path']  # Already extracted
    
    # Download only these 2 page images (small, ~5MB each)
    old_page = storage_service.download_file(old_page_gcs_path)
    new_page = storage_service.download_file(new_page_gcs_path)
    
    # Process (no full PDF in memory)
    result = process_page_pair(old_page, new_page)
    
    return jsonify({'success': True})
```

---

**User**

how can this pre split be handles 

---

**Cursor**


Outlining the pre-split implementation:



## Pre-Split Implementation Guide

### Architecture Overview

```
1. Upload PDF â†’ GCS
   â†“
2. Split PDF into pages â†’ Store pages in GCS
   â†“
3. Extract drawing names from pages
   â†“
4. Match pages by drawing name
   â†“
5. Process page pairs independently (parallel)
   â†“
6. Aggregate results
```

---

## Implementation: Step-by-Step

### Step 1: Create PDF Splitter Service

**New File**: `gcp/infrastructure/pdf_splitter.py`

```python
#!/usr/bin/env python3
"""
PDF Splitter Service
Splits PDFs into individual page images and stores in GCS
"""

import os
import io
import logging
import fitz  # PyMuPDF
from typing import List, Dict
from gcp.storage import storage_service
from gcp.database import get_db_session
from gcp.database.models import Drawing

logger = logging.getLogger(__name__)

class PDFSplitter:
    """Splits PDFs into individual page images"""
    
    def __init__(self, dpi=300):
        self.dpi = dpi
    
    def split_pdf_to_pages(self, pdf_gcs_path: str, session_id: str, 
                          drawing_type: str) -> List[Dict]:
        """
        Split PDF into individual pages and store in GCS
        
        Args:
            pdf_gcs_path: GCS path to PDF (e.g., gs://bucket/sessions/{id}/old_file.pdf)
            session_id: Session ID
            drawing_type: 'old' or 'new'
            
        Returns:
            List of page info: [{'page_num': 1, 'gcs_path': '...', 'drawing_name': 'A-101'}, ...]
        """
        pages_info = []
        
        try:
            # Download PDF from GCS (we need it to split, but process efficiently)
            logger.info(f"Splitting PDF: {pdf_gcs_path}")
            pdf_bytes = storage_service.download_file(pdf_gcs_path)
            
            # Open PDF from bytes (memory efficient)
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            total_pages = len(doc)
            
            logger.info(f"PDF has {total_pages} pages, splitting into individual images...")
            
            # Extract drawing names first (lightweight operation)
            drawing_names = self._extract_drawing_names_from_pdf(doc)
            
            # Process each page
            for page_num in range(total_pages):
                try:
                    page = doc[page_num]
                    
                    # Render page to image
                    zoom = self.dpi / 72.0  # Convert DPI to zoom factor
                    mat = fitz.Matrix(zoom, zoom)
                    pix = page.get_pixmap(matrix=mat, alpha=False)
                    
                    # Convert to PNG bytes
                    page_image_bytes = pix.tobytes("png")
                    
                    # Get drawing name for this page
                    drawing_name = drawing_names.get(page_num + 1, f"Page_{page_num+1:03d}")
                    
                    # Upload page to GCS
                    page_gcs_path = (
                        f"sessions/{session_id}/pages/{drawing_type}/"
                        f"{drawing_name}.png"
                    )
                    
                    storage_service.upload_file(
                        io.BytesIO(page_image_bytes),
                        page_gcs_path,
                        content_type="image/png"
                    )
                    
                    pages_info.append({
                        'page_num': page_num + 1,
                        'gcs_path': page_gcs_path,
                        'drawing_name': drawing_name,
                        'size_bytes': len(page_image_bytes)
                    })
                    
                    # Clear from memory immediately
                    pix = None
                    page = None
                    
                    logger.info(f"Extracted page {page_num + 1}/{total_pages}: {drawing_name}")
                    
                except Exception as e:
                    logger.error(f"Error extracting page {page_num + 1}: {e}")
                    continue
            
            doc.close()
            logger.info(f"Successfully split PDF into {len(pages_info)} pages")
            
            return pages_info
            
        except Exception as e:
            logger.error(f"Error splitting PDF {pdf_gcs_path}: {e}")
            raise
    
    def _extract_drawing_names_from_pdf(self, doc: fitz.Document) -> Dict[int, str]:
        """Extract drawing names from PDF pages (lightweight, no rendering)"""
        drawing_names = {}
        import re
        
        DRAWING_RE = re.compile(r"\b([A-Z])[-\s]?(\d{1,4}(?:\.\d{1,2})?)(?:-([A-Z0-9]{1,8}))?\b")
        
        for page_num in range(len(doc)):
            try:
                page = doc[page_num]
                
                # Extract text (lightweight, no rendering)
                text = page.get_text()
                
                # Find drawing name
                match = DRAWING_RE.search(text)
                if match:
                    letter = match.group(1)
                    number = match.group(2)
                    suffix = match.group(3)
                    separator = text[match.start()+1:match.start(2)]
                    drawing_name = letter + separator + number
                    if suffix:
                        drawing_name += '-' + suffix
                    drawing_names[page_num + 1] = drawing_name
                else:
                    drawing_names[page_num + 1] = f"Page_{page_num+1:03d}"
                    
            except Exception as e:
                logger.warning(f"Could not extract drawing name for page {page_num + 1}: {e}")
                drawing_names[page_num + 1] = f"Page_{page_num+1:03d}"
        
        return drawing_names
```

---

### Step 2: Update Upload Handler to Split PDFs

**File**: `app.py` - Modify `/api/process-uploaded-files`

**Replace** (line 400-424):
```python
# Download files from storage to temporary locations for processing
old_temp_path = None
new_temp_path = None

try:
    # Create temporary files
    old_temp_fd, old_temp_path = tempfile.mkstemp(suffix='.pdf', prefix='old_')
    new_temp_fd, new_temp_path = tempfile.mkstemp(suffix='.pdf', prefix='new_')
    
    # Close file descriptors
    os.close(old_temp_fd)
    os.close(new_temp_fd)
    
    # Download files from storage
    if not storage_service.download_to_filename(old_storage_path, old_temp_path):
        raise Exception(f"Failed to download old file from {old_storage_path}")
    
    if not storage_service.download_to_filename(new_storage_path, new_temp_path):
        raise Exception(f"Failed to download new file from {new_storage_path}")
    
    logger.info(f"Downloaded files to temporary locations: {old_temp_path}, {new_temp_path}")
    
    # Process the files using chunked_process_documents
    logger.info(f"Starting processing for session {session_id}")
    results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
```

**With**:
```python
# Split PDFs into pages in GCS (no full download needed for processing)
from gcp.infrastructure.pdf_splitter import PDFSplitter

splitter = PDFSplitter(dpi=300)

logger.info(f"Splitting PDFs into pages for session {session_id}")

# Split old PDF
old_pages = splitter.split_pdf_to_pages(
    pdf_gcs_path=old_storage_path,
    session_id=session_id,
    drawing_type='old'
)

# Split new PDF
new_pages = splitter.split_pdf_to_pages(
    pdf_gcs_path=new_storage_path,
    session_id=session_id,
    drawing_type='new'
)

logger.info(f"Split complete: {len(old_pages)} old pages, {len(new_pages)} new pages")

# Store page metadata in database
if config.USE_DATABASE:
    with get_db_session() as db:
        for page_info in old_pages:
            drawing = Drawing(
                id=str(uuid.uuid4()),
                session_id=session_id,
                drawing_type='old',
                filename=f"{page_info['drawing_name']}.png",
                original_filename=files['old'].get('filename', 'old_file.pdf'),
                storage_path=page_info['gcs_path'],
                drawing_name=page_info['drawing_name'],
                page_number=page_info['page_num']
            )
            db.add(drawing)
        
        for page_info in new_pages:
            drawing = Drawing(
                id=str(uuid.uuid4()),
                session_id=session_id,
                drawing_type='new',
                filename=f"{page_info['drawing_name']}.png",
                original_filename=files['new'].get('filename', 'new_file.pdf'),
                storage_path=page_info['gcs_path'],
                drawing_name=page_info['drawing_name'],
                page_number=page_info['page_num']
            )
            db.add(drawing)
        
        db.commit()

# Create processing tasks for each page pair
from gcp.tasks import task_processor

# Match pages by drawing name
old_pages_map = {p['drawing_name']: p for p in old_pages}
new_pages_map = {p['drawing_name']: p for p in new_pages}
matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())

logger.info(f"Found {len(matching_drawings)} matching drawings to process")

# Create Cloud Task for each page pair
for drawing_name in matching_drawings:
    task_processor.create_page_processing_task(
        session_id=session_id,
        old_page_gcs_path=old_pages_map[drawing_name]['gcs_path'],
        new_page_gcs_path=new_pages_map[drawing_name]['gcs_path'],
        drawing_name=drawing_name
    )

return jsonify({
    'success': True,
    'session_id': session_id,
    'status': 'processing',
    'pages_split': len(old_pages) + len(new_pages),
    'matching_pairs': len(matching_drawings),
    'message': f'Split into {len(old_pages) + len(new_pages)} pages, processing {len(matching_drawings)} pairs'
})
```

---

### Step 3: Add Page Processing Task Method

**File**: `gcp/tasks/task_processor.py` - Add new method

```python
def create_page_processing_task(self, session_id: str, old_page_gcs_path: str, 
                                new_page_gcs_path: str, drawing_name: str):
    """Create a Cloud Task for processing a single page pair"""
    try:
        parent = self.client.queue_path(self.project, self.location, self.queue)
        
        task = {
            'http_request': {
                'http_method': tasks_v2.HttpMethod.POST,
                'url': f'{self.service_url}/api/process-page-pair',
                'headers': {
                    'Content-Type': 'application/json',
                },
                'body': json.dumps({
                    'session_id': session_id,
                    'old_page_gcs_path': old_page_gcs_path,
                    'new_page_gcs_path': new_page_gcs_path,
                    'drawing_name': drawing_name,
                    'timestamp': datetime.utcnow().isoformat()
                }).encode('utf-8')
            }
        }
        
        task['dispatch_deadline'] = '1800s'  # 30 minutes
        
        response = self.client.create_task(parent=parent, task=task)
        logger.info(f"Created page processing task for {drawing_name}: {response.name}")
        return response.name
        
    except Exception as e:
        logger.error(f"Failed to create page task for {drawing_name}: {e}")
        raise
```

---

### Step 4: Create Page Pair Processing Endpoint

**File**: `app.py` - Add new endpoint

```python
@app.route('/api/process-page-pair', methods=['POST'])
def process_page_pair():
    """Process a single page pair - no full PDF needed"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_page_gcs_path = data['old_page_gcs_path']
        new_page_gcs_path = data['new_page_gcs_path']
        drawing_name = data['drawing_name']
        
        logger.info(f"Processing page pair: {drawing_name}")
        
        # Download ONLY these two page images (small, ~5MB each)
        old_page_bytes = storage_service.download_file(old_page_gcs_path)
        new_page_bytes = storage_service.download_file(new_page_gcs_path)
        
        # Save to temp files for processing
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            old_page_path = os.path.join(temp_dir, f"{drawing_name}_old.png")
            new_page_path = os.path.join(temp_dir, f"{drawing_name}_new.png")
            
            with open(old_page_path, 'wb') as f:
                f.write(old_page_bytes)
            with open(new_page_path, 'wb') as f:
                f.write(new_page_bytes)
            
            # Process this single page pair
            from chunked_processor import ChunkedProcessor
            processor = ChunkedProcessor(session_id=session_id)
            
            result = processor._process_single_drawing_with_ai(
                old_page_path=old_page_path,
                new_page_path=new_page_path,
                drawing_name=drawing_name,
                page_number=1,
                temp_dir=temp_dir,
                results_dir=f"uploads/sessions/{session_id}/results",
                session_id=session_id
            )
            
            logger.info(f"Completed processing {drawing_name}: {result.get('success', False)}")
            
            return jsonify({
                'success': result.get('success', False),
                'drawing_name': drawing_name,
                'result': result
            })
            
    except Exception as e:
        logger.error(f"Error processing page pair: {e}")
        return jsonify({'error': str(e)}), 500
```

---

### Step 5: Parallel Processing with Cloud Tasks

**File**: `app.py` - Update split logic to create tasks in parallel

```python
# After splitting PDFs (in process_uploaded_files)

# Create tasks for all matching page pairs
from gcp.tasks import task_processor
from concurrent.futures import ThreadPoolExecutor

def create_task_for_pair(drawing_name):
    """Create Cloud Task for one page pair"""
    try:
        return task_processor.create_page_processing_task(
            session_id=session_id,
            old_page_gcs_path=old_pages_map[drawing_name]['gcs_path'],
            new_page_gcs_path=new_pages_map[drawing_name]['gcs_path'],
            drawing_name=drawing_name
        )
    except Exception as e:
        logger.error(f"Failed to create task for {drawing_name}: {e}")
        return None

# Create tasks in parallel (faster)
with ThreadPoolExecutor(max_workers=10) as executor:
    task_futures = [
        executor.submit(create_task_for_pair, drawing_name)
        for drawing_name in matching_drawings
    ]
    
    tasks_created = [f.result() for f in task_futures if f.result()]

logger.info(f"Created {len(tasks_created)} processing tasks")
```

---

## Complete Flow

```
1. Client uploads PDFs â†’ GCS
   gs://bucket/sessions/{id}/old_file.pdf
   gs://bucket/sessions/{id}/new_file.pdf
   â†“
2. Main Container: Split PDFs
   â”œâ”€ Download PDF (only for splitting)
   â”œâ”€ Extract pages â†’ PNG images
   â”œâ”€ Upload pages to GCS:
   â”‚   gs://bucket/sessions/{id}/pages/old/A-101.png
   â”‚   gs://bucket/sessions/{id}/pages/old/A-102.png
   â”‚   gs://bucket/sessions/{id}/pages/new/A-101.png
   â”‚   gs://bucket/sessions/{id}/pages/new/A-102.png
   â””â”€ Store metadata in database
   â†“
3. Main Container: Create Tasks
   â”œâ”€ Match pages by drawing name
   â”œâ”€ Create Cloud Task for each pair:
   â”‚   Task 1: Process A-101 (old + new)
   â”‚   Task 2: Process A-102 (old + new)
   â”‚   ...
   â””â”€ Return immediately
   â†“
4. Worker Pool (auto-scaling):
   â”œâ”€ Worker 1: Downloads A-101_old.png + A-101_new.png â†’ Processes â†’ Uploads result
   â”œâ”€ Worker 2: Downloads A-102_old.png + A-102_new.png â†’ Processes â†’ Uploads result
   â””â”€ ... (all in parallel, no full PDFs)
   â†“
5. Results Aggregated:
   â”œâ”€ All page results in database
   â””â”€ Session marked complete
```

---

## Benefits

1. No full PDF download for processing: only individual pages
2. True parallel processing: pages processed simultaneously
3. Memory efficient: each worker handles ~10MB (2 pages) vs 100MB+ (full PDF)
4. Scalable: auto-scaling workers
5. Fault tolerant: one page failure doesn't affect others

---

## Storage Structure

```
gs://buildtrace-storage/
â””â”€â”€ sessions/
    â””â”€â”€ {session_id}/
        â”œâ”€â”€ old_file.pdf          # Original PDF
        â”œâ”€â”€ new_file.pdf          # Original PDF
        â””â”€â”€ pages/
            â”œâ”€â”€ old/
            â”‚   â”œâ”€â”€ A-101.png     # Individual pages
            â”‚   â”œâ”€â”€ A-102.png
            â”‚   â””â”€â”€ ...
            â””â”€â”€ new/
                â”œâ”€â”€ A-101.png
                â”œâ”€â”€ A-102.png
                â””â”€â”€ ...
```

---

## Configuration

**File**: `config.py` - Add feature flag

```python
# Enable pre-split processing
USE_PRE_SPLIT_PROCESSING = os.getenv('USE_PRE_SPLIT_PROCESSING', 'true').lower() == 'true'
```

---

## Migration Path

**Phase 1**: Add pre-split as optional feature
- Keep current chunked processing as fallback
- Test with feature flag

**Phase 2**: Make pre-split default
- Use for all files >10 pages
- Keep chunked for very small files

**Phase 3**: Always use pre-split
- Remove chunked processing code
- Simplify architecture

This removes the flaw: no full PDF download, only individual pages processed independently.

Should I provide the complete code files for this implementation?

```plaintext
1. Upload PDF â†’ GCS
   â†“
2. Split PDF into pages â†’ Store pages in GCS
   â†“
3. Extract drawing names from pages
   â†“
4. Match pages by drawing name
   â†“
5. Process page pairs independently (parallel)
   â†“
6. Aggregate results
```

```python
#!/usr/bin/env python3
"""
PDF Splitter Service
Splits PDFs into individual page images and stores in GCS
"""

import os
import io
import logging
import fitz  # PyMuPDF
from typing import List, Dict
from gcp.storage import storage_service
from gcp.database import get_db_session
from gcp.database.models import Drawing

logger = logging.getLogger(__name__)

class PDFSplitter:
    """Splits PDFs into individual page images"""
    
    def __init__(self, dpi=300):
        self.dpi = dpi
    
    def split_pdf_to_pages(self, pdf_gcs_path: str, session_id: str, 
                          drawing_type: str) -> List[Dict]:
        """
        Split PDF into individual pages and store in GCS
        
        Args:
            pdf_gcs_path: GCS path to PDF (e.g., gs://bucket/sessions/{id}/old_file.pdf)
            session_id: Session ID
            drawing_type: 'old' or 'new'
            
        Returns:
            List of page info: [{'page_num': 1, 'gcs_path': '...', 'drawing_name': 'A-101'}, ...]
        """
        pages_info = []
        
        try:
            # Download PDF from GCS (we need it to split, but process efficiently)
            logger.info(f"Splitting PDF: {pdf_gcs_path}")
            pdf_bytes = storage_service.download_file(pdf_gcs_path)
            
            # Open PDF from bytes (memory efficient)
            doc = fitz.open(stream=pdf_bytes, filetype="pdf")
            total_pages = len(doc)
            
            logger.info(f"PDF has {total_pages} pages, splitting into individual images...")
            
            # Extract drawing names first (lightweight operation)
            drawing_names = self._extract_drawing_names_from_pdf(doc)
            
            # Process each page
            for page_num in range(total_pages):
                try:
                    page = doc[page_num]
                    
                    # Render page to image
                    zoom = self.dpi / 72.0  # Convert DPI to zoom factor
                    mat = fitz.Matrix(zoom, zoom)
                    pix = page.get_pixmap(matrix=mat, alpha=False)
                    
                    # Convert to PNG bytes
                    page_image_bytes = pix.tobytes("png")
                    
                    # Get drawing name for this page
                    drawing_name = drawing_names.get(page_num + 1, f"Page_{page_num+1:03d}")
                    
                    # Upload page to GCS
                    page_gcs_path = (
                        f"sessions/{session_id}/pages/{drawing_type}/"
                        f"{drawing_name}.png"
                    )
                    
                    storage_service.upload_file(
                        io.BytesIO(page_image_bytes),
                        page_gcs_path,
                        content_type="image/png"
                    )
                    
                    pages_info.append({
                        'page_num': page_num + 1,
                        'gcs_path': page_gcs_path,
                        'drawing_name': drawing_name,
                        'size_bytes': len(page_image_bytes)
                    })
                    
                    # Clear from memory immediately
                    pix = None
                    page = None
                    
                    logger.info(f"Extracted page {page_num + 1}/{total_pages}: {drawing_name}")
                    
                except Exception as e:
                    logger.error(f"Error extracting page {page_num + 1}: {e}")
                    continue
            
            doc.close()
            logger.info(f"Successfully split PDF into {len(pages_info)} pages")
            
            return pages_info
            
        except Exception as e:
            logger.error(f"Error splitting PDF {pdf_gcs_path}: {e}")
            raise
    
    def _extract_drawing_names_from_pdf(self, doc: fitz.Document) -> Dict[int, str]:
        """Extract drawing names from PDF pages (lightweight, no rendering)"""
        drawing_names = {}
        import re
        
        DRAWING_RE = re.compile(r"\b([A-Z])[-\s]?(\d{1,4}(?:\.\d{1,2})?)(?:-([A-Z0-9]{1,8}))?\b")
        
        for page_num in range(len(doc)):
            try:
                page = doc[page_num]
                
                # Extract text (lightweight, no rendering)
                text = page.get_text()
                
                # Find drawing name
                match = DRAWING_RE.search(text)
                if match:
                    letter = match.group(1)
                    number = match.group(2)
                    suffix = match.group(3)
                    separator = text[match.start()+1:match.start(2)]
                    drawing_name = letter + separator + number
                    if suffix:
                        drawing_name += '-' + suffix
                    drawing_names[page_num + 1] = drawing_name
                else:
                    drawing_names[page_num + 1] = f"Page_{page_num+1:03d}"
                    
            except Exception as e:
                logger.warning(f"Could not extract drawing name for page {page_num + 1}: {e}")
                drawing_names[page_num + 1] = f"Page_{page_num+1:03d}"
        
        return drawing_names
```

```python
# Download files from storage to temporary locations for processing
old_temp_path = None
new_temp_path = None

try:
    # Create temporary files
    old_temp_fd, old_temp_path = tempfile.mkstemp(suffix='.pdf', prefix='old_')
    new_temp_fd, new_temp_path = tempfile.mkstemp(suffix='.pdf', prefix='new_')
    
    # Close file descriptors
    os.close(old_temp_fd)
    os.close(new_temp_fd)
    
    # Download files from storage
    if not storage_service.download_to_filename(old_storage_path, old_temp_path):
        raise Exception(f"Failed to download old file from {old_storage_path}")
    
    if not storage_service.download_to_filename(new_storage_path, new_temp_path):
        raise Exception(f"Failed to download new file from {new_storage_path}")
    
    logger.info(f"Downloaded files to temporary locations: {old_temp_path}, {new_temp_path}")
    
    # Process the files using chunked_process_documents
    logger.info(f"Starting processing for session {session_id}")
    results = chunked_process_documents(old_temp_path, new_temp_path, session_id)
```

```python
# Split PDFs into pages in GCS (no full download needed for processing)
from gcp.infrastructure.pdf_splitter import PDFSplitter

splitter = PDFSplitter(dpi=300)

logger.info(f"Splitting PDFs into pages for session {session_id}")

# Split old PDF
old_pages = splitter.split_pdf_to_pages(
    pdf_gcs_path=old_storage_path,
    session_id=session_id,
    drawing_type='old'
)

# Split new PDF
new_pages = splitter.split_pdf_to_pages(
    pdf_gcs_path=new_storage_path,
    session_id=session_id,
    drawing_type='new'
)

logger.info(f"Split complete: {len(old_pages)} old pages, {len(new_pages)} new pages")

# Store page metadata in database
if config.USE_DATABASE:
    with get_db_session() as db:
        for page_info in old_pages:
            drawing = Drawing(
                id=str(uuid.uuid4()),
                session_id=session_id,
                drawing_type='old',
                filename=f"{page_info['drawing_name']}.png",
                original_filename=files['old'].get('filename', 'old_file.pdf'),
                storage_path=page_info['gcs_path'],
                drawing_name=page_info['drawing_name'],
                page_number=page_info['page_num']
            )
            db.add(drawing)
        
        for page_info in new_pages:
            drawing = Drawing(
                id=str(uuid.uuid4()),
                session_id=session_id,
                drawing_type='new',
                filename=f"{page_info['drawing_name']}.png",
                original_filename=files['new'].get('filename', 'new_file.pdf'),
                storage_path=page_info['gcs_path'],
                drawing_name=page_info['drawing_name'],
                page_number=page_info['page_num']
            )
            db.add(drawing)
        
        db.commit()

# Create processing tasks for each page pair
from gcp.tasks import task_processor

# Match pages by drawing name
old_pages_map = {p['drawing_name']: p for p in old_pages}
new_pages_map = {p['drawing_name']: p for p in new_pages}
matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())

logger.info(f"Found {len(matching_drawings)} matching drawings to process")

# Create Cloud Task for each page pair
for drawing_name in matching_drawings:
    task_processor.create_page_processing_task(
        session_id=session_id,
        old_page_gcs_path=old_pages_map[drawing_name]['gcs_path'],
        new_page_gcs_path=new_pages_map[drawing_name]['gcs_path'],
        drawing_name=drawing_name
    )

return jsonify({
    'success': True,
    'session_id': session_id,
    'status': 'processing',
    'pages_split': len(old_pages) + len(new_pages),
    'matching_pairs': len(matching_drawings),
    'message': f'Split into {len(old_pages) + len(new_pages)} pages, processing {len(matching_drawings)} pairs'
})
```

```python
def create_page_processing_task(self, session_id: str, old_page_gcs_path: str, 
                                new_page_gcs_path: str, drawing_name: str):
    """Create a Cloud Task for processing a single page pair"""
    try:
        parent = self.client.queue_path(self.project, self.location, self.queue)
        
        task = {
            'http_request': {
                'http_method': tasks_v2.HttpMethod.POST,
                'url': f'{self.service_url}/api/process-page-pair',
                'headers': {
                    'Content-Type': 'application/json',
                },
                'body': json.dumps({
                    'session_id': session_id,
                    'old_page_gcs_path': old_page_gcs_path,
                    'new_page_gcs_path': new_page_gcs_path,
                    'drawing_name': drawing_name,
                    'timestamp': datetime.utcnow().isoformat()
                }).encode('utf-8')
            }
        }
        
        task['dispatch_deadline'] = '1800s'  # 30 minutes
        
        response = self.client.create_task(parent=parent, task=task)
        logger.info(f"Created page processing task for {drawing_name}: {response.name}")
        return response.name
        
    except Exception as e:
        logger.error(f"Failed to create page task for {drawing_name}: {e}")
        raise
```

```python
@app.route('/api/process-page-pair', methods=['POST'])
def process_page_pair():
    """Process a single page pair - no full PDF needed"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_page_gcs_path = data['old_page_gcs_path']
        new_page_gcs_path = data['new_page_gcs_path']
        drawing_name = data['drawing_name']
        
        logger.info(f"Processing page pair: {drawing_name}")
        
        # Download ONLY these two page images (small, ~5MB each)
        old_page_bytes = storage_service.download_file(old_page_gcs_path)
        new_page_bytes = storage_service.download_file(new_page_gcs_path)
        
        # Save to temp files for processing
        import tempfile
        with tempfile.TemporaryDirectory() as temp_dir:
            old_page_path = os.path.join(temp_dir, f"{drawing_name}_old.png")
            new_page_path = os.path.join(temp_dir, f"{drawing_name}_new.png")
            
            with open(old_page_path, 'wb') as f:
                f.write(old_page_bytes)
            with open(new_page_path, 'wb') as f:
                f.write(new_page_bytes)
            
            # Process this single page pair
            from chunked_processor import ChunkedProcessor
            processor = ChunkedProcessor(session_id=session_id)
            
            result = processor._process_single_drawing_with_ai(
                old_page_path=old_page_path,
                new_page_path=new_page_path,
                drawing_name=drawing_name,
                page_number=1,
                temp_dir=temp_dir,
                results_dir=f"uploads/sessions/{session_id}/results",
                session_id=session_id
            )
            
            logger.info(f"Completed processing {drawing_name}: {result.get('success', False)}")
            
            return jsonify({
                'success': result.get('success', False),
                'drawing_name': drawing_name,
                'result': result
            })
            
    except Exception as e:
        logger.error(f"Error processing page pair: {e}")
        return jsonify({'error': str(e)}), 500
```

```python
# After splitting PDFs (in process_uploaded_files)

# Create tasks for all matching page pairs
from gcp.tasks import task_processor
from concurrent.futures import ThreadPoolExecutor

def create_task_for_pair(drawing_name):
    """Create Cloud Task for one page pair"""
    try:
        return task_processor.create_page_processing_task(
            session_id=session_id,
            old_page_gcs_path=old_pages_map[drawing_name]['gcs_path'],
            new_page_gcs_path=new_pages_map[drawing_name]['gcs_path'],
            drawing_name=drawing_name
        )
    except Exception as e:
        logger.error(f"Failed to create task for {drawing_name}: {e}")
        return None

# Create tasks in parallel (faster)
with ThreadPoolExecutor(max_workers=10) as executor:
    task_futures = [
        executor.submit(create_task_for_pair, drawing_name)
        for drawing_name in matching_drawings
    ]
    
    tasks_created = [f.result() for f in task_futures if f.result()]

logger.info(f"Created {len(tasks_created)} processing tasks")
```

```plaintext
1. Client uploads PDFs â†’ GCS
   gs://bucket/sessions/{id}/old_file.pdf
   gs://bucket/sessions/{id}/new_file.pdf
   â†“
2. Main Container: Split PDFs
   â”œâ”€ Download PDF (only for splitting)
   â”œâ”€ Extract pages â†’ PNG images
   â”œâ”€ Upload pages to GCS:
   â”‚   gs://bucket/sessions/{id}/pages/old/A-101.png
   â”‚   gs://bucket/sessions/{id}/pages/old/A-102.png
   â”‚   gs://bucket/sessions/{id}/pages/new/A-101.png
   â”‚   gs://bucket/sessions/{id}/pages/new/A-102.png
   â””â”€ Store metadata in database
   â†“
3. Main Container: Create Tasks
   â”œâ”€ Match pages by drawing name
   â”œâ”€ Create Cloud Task for each pair:
   â”‚   Task 1: Process A-101 (old + new)
   â”‚   Task 2: Process A-102 (old + new)
   â”‚   ...
   â””â”€ Return immediately
   â†“
4. Worker Pool (auto-scaling):
   â”œâ”€ Worker 1: Downloads A-101_old.png + A-101_new.png â†’ Processes â†’ Uploads result
   â”œâ”€ Worker 2: Downloads A-102_old.png + A-102_new.png â†’ Processes â†’ Uploads result
   â””â”€ ... (all in parallel, no full PDFs)
   â†“
5. Results Aggregated:
   â”œâ”€ All page results in database
   â””â”€ Session marked complete
```

```plaintext
gs://buildtrace-storage/
â””â”€â”€ sessions/
    â””â”€â”€ {session_id}/
        â”œâ”€â”€ old_file.pdf          # Original PDF
        â”œâ”€â”€ new_file.pdf          # Original PDF
        â””â”€â”€ pages/
            â”œâ”€â”€ old/
            â”‚   â”œâ”€â”€ A-101.png     # Individual pages
            â”‚   â”œâ”€â”€ A-102.png
            â”‚   â””â”€â”€ ...
            â””â”€â”€ new/
                â”œâ”€â”€ A-101.png
                â”œâ”€â”€ A-102.png
                â””â”€â”€ ...
```

```python
# Enable pre-split processing
USE_PRE_SPLIT_PROCESSING = os.getenv('USE_PRE_SPLIT_PROCESSING', 'true').lower() == 'true'
```

---

**User**

but to split it how will it happen, would it not require loading the entire pdf

---

**Cursor**


Clarifying the tradeoff: splitting requires reading the PDF, but we can minimize memory use. Explaining the options:



## The Reality: Splitting Requires PDF Access

### The Problem

To split a PDF, you need to read it:
- PDF structure (catalog/index) â€” small (~few KB)
- Page content â€” must be read to extract pages
- PyMuPDF needs the PDF file/bytes to extract pages

So yes, splitting requires loading/reading the PDF.

---

## The Key Difference: One-Time vs Repeated

### Current Approach (Repeated Download)
```
For EACH processing attempt:
1. Download full PDF (100MB) âŒ
2. Process pages sequentially
3. If retry needed â†’ Download again âŒ
```

### Pre-Split Approach (One-Time Download)
```
ONCE at upload:
1. Download full PDF (100MB) âœ… (one-time cost)
2. Split into pages â†’ Store in GCS
3. Delete PDF from container âœ…

For processing (can happen multiple times):
4. Download individual pages (~5MB each) âœ…
5. Process independently
6. No full PDF needed âœ…
```

---

## Better Solution: Split During Upload

### Architecture: Split Immediately on Upload

```
1. Client uploads PDF â†’ GCS
   â†“
2. IMMEDIATELY trigger split (before storing full PDF)
   â†“
3. Split PDF â†’ Extract pages â†’ Store pages in GCS
   â†“
4. Optionally: Delete original PDF (or keep as backup)
   â†“
5. Processing uses pages only (no full PDF)
```

### Implementation: Split in Upload Handler

**File**: `app.py` - Modify upload endpoint

```python
@app.route('/api/process-uploaded-files', methods=['POST'])
def process_uploaded_files():
    """Process files - split immediately on upload"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        files = data['files']
        
        old_storage_path = files['old']['storage_path']
        new_storage_path = files['new']['storage_path']
        
        # OPTION 1: Split in same request (if PDFs are small)
        # OPTION 2: Create split tasks (if PDFs are large)
        
        # Check PDF size first
        old_size = storage_service.get_file_size(old_storage_path)
        new_size = storage_service.get_file_size(new_storage_path)
        total_size = old_size + new_size
        
        if total_size > 50 * 1024 * 1024:  # > 50MB
            # Large PDFs: Split in background
            return split_pdfs_in_background(session_id, old_storage_path, new_storage_path)
        else:
            # Small PDFs: Split immediately
            return split_pdfs_immediately(session_id, old_storage_path, new_storage_path)
```

---

## Solution: Streaming Split (Minimal Memory)

### Use PyMuPDF with Streaming

**Key Insight**: PyMuPDF can work with file-like objects, but still needs to read the PDF structure.

**However**, we can minimize memory:

```python
def split_pdf_streaming(pdf_gcs_path: str, session_id: str, drawing_type: str):
    """Split PDF with minimal memory footprint"""
    from gcp.storage import storage_service
    import fitz
    import io
    
    # Download PDF as stream (not all at once if possible)
    # Note: GCS download still gets full file, but we process incrementally
    pdf_bytes = storage_service.download_file(pdf_gcs_path)
    
    # Open PDF from bytes (PyMuPDF reads structure, not all pages)
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    
    # Process pages one at a time (minimal memory)
    for page_num in range(len(doc)):
        page = doc[page_num]  # Only this page loaded
        
        # Render this page
        pix = page.get_pixmap(dpi=300)
        page_bytes = pix.tobytes("png")
        
        # Upload immediately
        page_path = f"sessions/{session_id}/pages/{drawing_type}/page_{page_num+1:03d}.png"
        storage_service.upload_file(io.BytesIO(page_bytes), page_path)
        
        # Clear immediately
        pix = None
        page = None
        
        # Force cleanup
        import gc
        gc.collect()
    
    doc.close()
    # PDF bytes can be garbage collected now
```

**Memory Usage:**
- PDF bytes in memory: ~100MB (temporary, during split)
- Each page during processing: ~5MB
- Peak memory: ~105MB (vs 200MB+ if keeping full PDF)

---

## Best Approach: Hybrid Strategy

### Strategy: Split Smartly Based on Size

```python
def handle_pdf_upload(pdf_gcs_path: str, session_id: str, drawing_type: str):
    """Smart PDF handling based on size"""
    
    # Get PDF metadata (lightweight, no full download)
    pdf_size = storage_service.get_file_size(pdf_gcs_path)
    page_count = get_pdf_page_count_from_gcs(pdf_gcs_path)  # Lightweight
    
    if page_count <= 5 and pdf_size < 10 * 1024 * 1024:  # Small PDF
        # Don't split - process directly
        # Full PDF is small enough
        return process_directly(pdf_gcs_path, session_id, drawing_type)
    
    elif page_count <= 20:  # Medium PDF
        # Split in same container (one-time cost)
        return split_and_process(pdf_gcs_path, session_id, drawing_type)
    
    else:  # Large PDF (>20 pages)
        # Split in dedicated job processor (32GB)
        return create_split_job(pdf_gcs_path, session_id, drawing_type)
```

---

## Alternative: Client-Side Split (Best for Scalability)

### Most Scalable: Split on Client Before Upload

**Architecture:**
```
1. Client (Browser):
   â”œâ”€ User selects PDF
   â”œâ”€ JavaScript splits PDF into pages
   â”œâ”€ Uploads individual pages directly to GCS
   â””â”€ No full PDF ever reaches server
   â†“
2. Server:
   â”œâ”€ Receives page list
   â”œâ”€ Matches pages
   â””â”€ Processes independently
```

**Implementation** (JavaScript):
```javascript
// Use pdf-lib or similar in browser
import * as pdfjsLib from 'pdfjs-dist';

async function splitPdfInBrowser(pdfFile) {
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const pages = [];
    
    for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
        const page = await pdf.getPage(pageNum);
        const viewport = page.getViewport({scale: 2.0});
        
        // Render to canvas
        const canvas = document.createElement('canvas');
        canvas.width = viewport.width;
        canvas.height = viewport.height;
        await page.render({canvasContext: canvas.getContext('2d'), viewport}).promise;
        
        // Convert to blob
        canvas.toBlob((blob) => {
            // Upload this page directly to GCS
            uploadPageToGCS(blob, `page_${pageNum}.png`);
        });
    }
}
```

**Benefits:**
- No server memory usage
- No PDF download to server
- Client does the work
- Server only processes pages

**Drawbacks:**
- Requires JavaScript PDF library
- Client-side processing
- Browser memory limits

---

## Recommended Solution: Split in Job Processor

### Best Balance: Split in 32GB Container

**Flow:**
```
1. Upload PDF â†’ GCS
   â†“
2. Main Container (4GB):
   â”œâ”€ Creates split job
   â””â”€ Returns immediately
   â†“
3. Job Processor (32GB):
   â”œâ”€ Downloads PDF (one-time)
   â”œâ”€ Splits into pages
   â”œâ”€ Uploads pages to GCS
   â””â”€ Creates processing tasks
   â†“
4. Workers:
   â”œâ”€ Process pages independently
   â””â”€ No full PDF needed
```

**Code:**

```python
# In job processor
def split_pdf_job(pdf_gcs_path: str, session_id: str, drawing_type: str):
    """Split PDF in 32GB container - can handle large PDFs"""
    from gcp.infrastructure.pdf_splitter import PDFSplitter
    
    splitter = PDFSplitter(dpi=300)
    
    # Download PDF (one-time, in 32GB container)
    pages = splitter.split_pdf_to_pages(pdf_gcs_path, session_id, drawing_type)
    
    # Create processing tasks for each page
    for page_info in pages:
        create_page_processing_task(page_info)
    
    return pages
```

**Why This Works:**
- Split happens in 32GB container (can handle large PDFs)
- One-time download (not repeated)
- After split, only pages needed (small)
- Processing is distributed

---

## Summary: The Tradeoff

| Approach | PDF Download | Memory During Split | Memory During Processing |
|----------|--------------|---------------------|--------------------------|
| Current | Every time | Full PDF | Full PDF |
| Pre-split (same container) | Once | Full PDF (temporary) | Pages only |
| Pre-split (job processor) | Once (in 32GB) | Full PDF (temporary) | Pages only |
| Client-side split | Never | None | Pages only |

**Recommendation:**
- Use job processor for splitting (32GB handles large PDFs)
- Split is one-time cost (not repeated)
- After split, processing uses only pages (scalable)

The flaw is acknowledged: splitting requires the PDF. The benefit is it's one-time, and processing afterward uses only pages.

Should I provide the complete job-processor-based split implementation?

```plaintext
For EACH processing attempt:
1. Download full PDF (100MB) âŒ
2. Process pages sequentially
3. If retry needed â†’ Download again âŒ
```

```plaintext
ONCE at upload:
1. Download full PDF (100MB) âœ… (one-time cost)
2. Split into pages â†’ Store in GCS
3. Delete PDF from container âœ…

For processing (can happen multiple times):
4. Download individual pages (~5MB each) âœ…
5. Process independently
6. No full PDF needed âœ…
```

```plaintext
1. Client uploads PDF â†’ GCS
   â†“
2. IMMEDIATELY trigger split (before storing full PDF)
   â†“
3. Split PDF â†’ Extract pages â†’ Store pages in GCS
   â†“
4. Optionally: Delete original PDF (or keep as backup)
   â†“
5. Processing uses pages only (no full PDF)
```

```python
@app.route('/api/process-uploaded-files', methods=['POST'])
def process_uploaded_files():
    """Process files - split immediately on upload"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        files = data['files']
        
        old_storage_path = files['old']['storage_path']
        new_storage_path = files['new']['storage_path']
        
        # OPTION 1: Split in same request (if PDFs are small)
        # OPTION 2: Create split tasks (if PDFs are large)
        
        # Check PDF size first
        old_size = storage_service.get_file_size(old_storage_path)
        new_size = storage_service.get_file_size(new_storage_path)
        total_size = old_size + new_size
        
        if total_size > 50 * 1024 * 1024:  # > 50MB
            # Large PDFs: Split in background
            return split_pdfs_in_background(session_id, old_storage_path, new_storage_path)
        else:
            # Small PDFs: Split immediately
            return split_pdfs_immediately(session_id, old_storage_path, new_storage_path)
```

```python
def split_pdf_streaming(pdf_gcs_path: str, session_id: str, drawing_type: str):
    """Split PDF with minimal memory footprint"""
    from gcp.storage import storage_service
    import fitz
    import io
    
    # Download PDF as stream (not all at once if possible)
    # Note: GCS download still gets full file, but we process incrementally
    pdf_bytes = storage_service.download_file(pdf_gcs_path)
    
    # Open PDF from bytes (PyMuPDF reads structure, not all pages)
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    
    # Process pages one at a time (minimal memory)
    for page_num in range(len(doc)):
        page = doc[page_num]  # Only this page loaded
        
        # Render this page
        pix = page.get_pixmap(dpi=300)
        page_bytes = pix.tobytes("png")
        
        # Upload immediately
        page_path = f"sessions/{session_id}/pages/{drawing_type}/page_{page_num+1:03d}.png"
        storage_service.upload_file(io.BytesIO(page_bytes), page_path)
        
        # Clear immediately
        pix = None
        page = None
        
        # Force cleanup
        import gc
        gc.collect()
    
    doc.close()
    # PDF bytes can be garbage collected now
```

```python
def handle_pdf_upload(pdf_gcs_path: str, session_id: str, drawing_type: str):
    """Smart PDF handling based on size"""
    
    # Get PDF metadata (lightweight, no full download)
    pdf_size = storage_service.get_file_size(pdf_gcs_path)
    page_count = get_pdf_page_count_from_gcs(pdf_gcs_path)  # Lightweight
    
    if page_count <= 5 and pdf_size < 10 * 1024 * 1024:  # Small PDF
        # Don't split - process directly
        # Full PDF is small enough
        return process_directly(pdf_gcs_path, session_id, drawing_type)
    
    elif page_count <= 20:  # Medium PDF
        # Split in same container (one-time cost)
        return split_and_process(pdf_gcs_path, session_id, drawing_type)
    
    else:  # Large PDF (>20 pages)
        # Split in dedicated job processor (32GB)
        return create_split_job(pdf_gcs_path, session_id, drawing_type)
```

```plaintext
1. Client (Browser):
   â”œâ”€ User selects PDF
   â”œâ”€ JavaScript splits PDF into pages
   â”œâ”€ Uploads individual pages directly to GCS
   â””â”€ No full PDF ever reaches server
   â†“
2. Server:
   â”œâ”€ Receives page list
   â”œâ”€ Matches pages
   â””â”€ Processes independently
```

```javascript
// Use pdf-lib or similar in browser
import * as pdfjsLib from 'pdfjs-dist';

async function splitPdfInBrowser(pdfFile) {
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const pages = [];
    
    for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
        const page = await pdf.getPage(pageNum);
        const viewport = page.getViewport({scale: 2.0});
        
        // Render to canvas
        const canvas = document.createElement('canvas');
        canvas.width = viewport.width;
        canvas.height = viewport.height;
        await page.render({canvasContext: canvas.getContext('2d'), viewport}).promise;
        
        // Convert to blob
        canvas.toBlob((blob) => {
            // Upload this page directly to GCS
            uploadPageToGCS(blob, `page_${pageNum}.png`);
        });
    }
}
```

```plaintext
1. Upload PDF â†’ GCS
   â†“
2. Main Container (4GB):
   â”œâ”€ Creates split job
   â””â”€ Returns immediately
   â†“
3. Job Processor (32GB):
   â”œâ”€ Downloads PDF (one-time)
   â”œâ”€ Splits into pages
   â”œâ”€ Uploads pages to GCS
   â””â”€ Creates processing tasks
   â†“
4. Workers:
   â”œâ”€ Process pages independently
   â””â”€ No full PDF needed
```

```python
# In job processor
def split_pdf_job(pdf_gcs_path: str, session_id: str, drawing_type: str):
    """Split PDF in 32GB container - can handle large PDFs"""
    from gcp.infrastructure.pdf_splitter import PDFSplitter
    
    splitter = PDFSplitter(dpi=300)
    
    # Download PDF (one-time, in 32GB container)
    pages = splitter.split_pdf_to_pages(pdf_gcs_path, session_id, drawing_type)
    
    # Create processing tasks for each page
    for page_info in pages:
        create_page_processing_task(page_info)
    
    return pages
```

---

**User**

Client-side split

explain mroe 

---

**Cursor**


Explaining client-side PDF splitting: how it works, implementation, and tradeoffs.



## Client-Side PDF Split: Detailed Explanation

### Concept

Split the PDF in the browser before uploading. The server receives individual page images, not the full PDF.

```
Traditional Flow:
Client â†’ Upload PDF (100MB) â†’ Server â†’ Split â†’ Process

Client-Side Split Flow:
Client â†’ Split PDF â†’ Upload Pages (5MB each) â†’ Server â†’ Process
```

---

## How It Works

### Step 1: Load PDF in Browser

**JavaScript Libraries:**
1. **pdf.js** (Mozilla) - Most popular, well-supported
2. **pdf-lib** - Modern, TypeScript-friendly
3. **react-pdf** - If using React

**Using pdf.js:**

```javascript
// Load PDF.js library
<script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>

// Or via npm
import * as pdfjsLib from 'pdfjs-dist';
pdfjsLib.GlobalWorkerOptions.workerSrc = 
    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
```

### Step 2: Split PDF into Pages

**Complete Implementation:**

```javascript
/**
 * PDF Splitter Module
 * Splits PDF into individual page images in the browser
 */
class PDFSplitter {
    constructor() {
        this.pdfjsLib = null;
        this.initializePDFJS();
    }

    async initializePDFJS() {
        // Load PDF.js dynamically
        if (typeof pdfjsLib === 'undefined') {
            await this.loadPDFJSLibrary();
        }
        this.pdfjsLib = pdfjsLib;
    }

    async loadPDFJSLibrary() {
        return new Promise((resolve, reject) => {
            const script = document.createElement('script');
            script.src = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js';
            script.onload = () => {
                // Load worker
                pdfjsLib.GlobalWorkerOptions.workerSrc = 
                    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
                resolve();
            };
            script.onerror = reject;
            document.head.appendChild(script);
        });
    }

    /**
     * Split PDF file into individual page images
     * @param {File} pdfFile - PDF file object
     * @param {string} sessionId - Session ID
     * @param {string} drawingType - 'old' or 'new'
     * @param {Function} onProgress - Progress callback (pageNum, totalPages)
     * @returns {Promise<Array>} Array of page info objects
     */
    async splitPDFToPages(pdfFile, sessionId, drawingType, onProgress = null) {
        try {
            console.log(`ðŸ“„ Splitting PDF: ${pdfFile.name} (${(pdfFile.size / 1024 / 1024).toFixed(2)} MB)`);
            
            // Load PDF from file
            const arrayBuffer = await pdfFile.arrayBuffer();
            const pdf = await this.pdfjsLib.getDocument({data: arrayBuffer}).promise;
            
            const totalPages = pdf.numPages;
            console.log(`ðŸ“„ PDF has ${totalPages} pages`);
            
            const pages = [];
            const drawingNames = await this.extractDrawingNames(pdf);
            
            // Process each page
            for (let pageNum = 1; pageNum <= totalPages; pageNum++) {
                try {
                    if (onProgress) {
                        onProgress(pageNum, totalPages);
                    }
                    
                    // Get page
                    const page = await pdf.getPage(pageNum);
                    
                    // Render page to canvas
                    const viewport = page.getViewport({scale: 2.0}); // 2x for 300 DPI equivalent
                    const canvas = document.createElement('canvas');
                    const context = canvas.getContext('2d');
                    canvas.width = viewport.width;
                    canvas.height = viewport.height;
                    
                    // Render page
                    await page.render({
                        canvasContext: context,
                        viewport: viewport
                    }).promise;
                    
                    // Convert canvas to blob (PNG)
                    const blob = await new Promise((resolve) => {
                        canvas.toBlob(resolve, 'image/png', 1.0);
                    });
                    
                    // Get drawing name for this page
                    const drawingName = drawingNames[pageNum] || `Page_${pageNum.toString().padStart(3, '0')}`;
                    
                    pages.push({
                        pageNum: pageNum,
                        drawingName: drawingName,
                        blob: blob,
                        size: blob.size
                    });
                    
                    console.log(`âœ… Extracted page ${pageNum}/${totalPages}: ${drawingName} (${(blob.size / 1024).toFixed(2)} KB)`);
                    
                    // Clean up
                    canvas.width = 0;
                    canvas.height = 0;
                    
                } catch (error) {
                    console.error(`âŒ Error extracting page ${pageNum}:`, error);
                    // Continue with next page
                }
            }
            
            console.log(`âœ… Split complete: ${pages.length} pages extracted`);
            return pages;
            
        } catch (error) {
            console.error('âŒ Error splitting PDF:', error);
            throw new Error(`Failed to split PDF: ${error.message}`);
        }
    }

    /**
     * Extract drawing names from PDF pages (text extraction)
     */
    async extractDrawingNames(pdf) {
        const drawingNames = {};
        const drawingNameRegex = /\b([A-Z])[-\s]?(\d{1,4}(?:\.\d{1,2})?)(?:-([A-Z0-9]{1,8}))?\b/;
        
        for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
            try {
                const page = await pdf.getPage(pageNum);
                const textContent = await page.getTextContent();
                
                // Combine all text items
                const fullText = textContent.items
                    .map(item => item.str)
                    .join(' ');
                
                // Find drawing name
                const match = fullText.match(drawingNameRegex);
                if (match) {
                    const letter = match[1];
                    const number = match[2];
                    const suffix = match[3];
                    const separator = match[0].includes('-') ? '-' : 
                                    match[0].includes(' ') ? ' ' : '';
                    drawingNames[pageNum] = letter + separator + number + (suffix ? '-' + suffix : '');
                } else {
                    drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
                }
            } catch (error) {
                console.warn(`Could not extract drawing name for page ${pageNum}:`, error);
                drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
            }
        }
        
        return drawingNames;
    }
}
```

### Step 3: Upload Pages Directly to GCS

**Integration with Existing Upload Flow:**

```javascript
// Modify FileUploader.js
class FileUploader {
    constructor() {
        this.oldFile = null;
        this.newFile = null;
        this.pdfSplitter = new PDFSplitter();
        this.useClientSideSplit = true; // Feature flag
    }

    async handleFileSelect(file, type) {
        // Validate file
        if (!Utils.isValidFileType(file, this.acceptedTypes)) {
            Utils.showToast('Invalid file type', 'error');
            return;
        }

        // Check if PDF and client-side split enabled
        if (file.type === 'application/pdf' && this.useClientSideSplit) {
            // Show splitting progress
            this.showSplittingProgress(type);
            
            try {
                // Split PDF in browser
                const pages = await this.pdfSplitter.splitPDFToPages(
                    file,
                    null, // sessionId will be generated later
                    type,
                    (pageNum, totalPages) => {
                        this.updateSplittingProgress(type, pageNum, totalPages);
                    }
                );
                
                // Store split pages instead of original file
                if (type === 'old') {
                    this.oldFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                } else {
                    this.newFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                }
                
                this.updateFileInfo(type, file, pages.length);
                
            } catch (error) {
                Utils.showToast(`Failed to split PDF: ${error.message}`, 'error');
                // Fallback to original file
                if (type === 'old') {
                    this.oldFile = file;
                } else {
                    this.newFile = file;
                }
            }
        } else {
            // Non-PDF or split disabled - use original file
            if (type === 'old') {
                this.oldFile = file;
            } else {
                this.newFile = file;
            }
        }
        
        this.updateCompareButton();
    }

    showSplittingProgress(type) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            fileInfo.innerHTML = `
                <div class="splitting-progress">
                    <div class="spinner"></div>
                    <span>Splitting PDF into pages...</span>
                </div>
            `;
        }
    }

    updateSplittingProgress(type, currentPage, totalPages) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            const progress = (currentPage / totalPages) * 100;
            fileInfo.querySelector('.splitting-progress span').textContent = 
                `Splitting PDF: ${currentPage}/${totalPages} pages (${Math.round(progress)}%)`;
        }
    }
}
```

### Step 4: Upload Pages to GCS

**Modify ProcessingMonitor.js:**

```javascript
async processFilesWithClientSideSplit(oldFile, newFile, onComplete) {
    try {
        // Get signed upload URLs for pages
        const sessionId = await this.createSession();
        
        // Upload all pages
        const uploadPromises = [];
        
        // Upload old pages
        if (oldFile.isSplit && oldFile.pages) {
            for (const page of oldFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'old', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        // Upload new pages
        if (newFile.isSplit && newFile.pages) {
            for (const page of newFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'new', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        await Promise.all(uploadPromises);
        
        // Notify server to process pages
        const response = await apiClient.processSplitPages(sessionId, {
            oldPages: oldFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            })),
            newPages: newFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            }))
        });
        
        return response;
        
    } catch (error) {
        console.error('Client-side split processing failed:', error);
        throw error;
    }
}

async getPageUploadUrl(sessionId, drawingType, drawingName) {
    const response = await apiClient.post('/api/get-page-upload-url', {
        session_id: sessionId,
        drawing_type: drawingType,
        drawing_name: drawingName
    });
    return response.upload_url;
}

async uploadPageToGCS(blob, uploadUrl, drawingName) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                console.log(`Uploading ${drawingName}: ${Math.round(e.loaded / e.total * 100)}%`);
            }
        };
        
        xhr.onload = () => {
            if (xhr.status >= 200 && xhr.status < 300) {
                console.log(`âœ… ${drawingName} uploaded`);
                resolve();
            } else {
                reject(new Error(`Upload failed: ${xhr.status}`));
            }
        };
        
        xhr.onerror = () => reject(new Error('Upload failed'));
        
        xhr.open('PUT', uploadUrl);
        xhr.setRequestHeader('Content-Type', 'image/png');
        xhr.send(blob);
    });
}
```

---

## Server-Side Changes

### New Endpoint: Get Page Upload URLs

**File**: `app.py`

```python
@app.route('/api/get-page-upload-url', methods=['POST'])
def get_page_upload_url():
    """Get signed URL for uploading a single page"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        drawing_type = data['drawing_type']  # 'old' or 'new'
        drawing_name = data['drawing_name']
        
        # Generate storage path
        page_path = f"sessions/{session_id}/pages/{drawing_type}/{drawing_name}.png"
        
        # Generate signed upload URL
        upload_url = storage_service.generate_signed_upload_url(
            page_path,
            expiration_minutes=60,
            content_type='image/png'
        )
        
        return jsonify({
            'upload_url': upload_url,
            'storage_path': page_path
        })
        
    except Exception as e:
        logger.error(f"Error generating page upload URL: {e}")
        return jsonify({'error': str(e)}), 500
```

### New Endpoint: Process Split Pages

**File**: `app.py`

```python
@app.route('/api/process-split-pages', methods=['POST'])
def process_split_pages():
    """Process pages that were split on client side"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_pages = data['old_pages']  # [{'drawingName': 'A-101', 'pageNum': 1}, ...]
        new_pages = data['new_pages']
        
        # Match pages by drawing name
        old_pages_map = {p['drawingName']: p for p in old_pages}
        new_pages_map = {p['drawingName']: p for p in new_pages}
        matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())
        
        # Create processing tasks for each matching pair
        from gcp.tasks import task_processor
        
        for drawing_name in matching_drawings:
            old_page_path = f"sessions/{session_id}/pages/old/{drawing_name}.png"
            new_page_path = f"sessions/{session_id}/pages/new/{drawing_name}.png"
            
            task_processor.create_page_processing_task(
                session_id=session_id,
                old_page_gcs_path=old_page_path,
                new_page_gcs_path=new_page_path,
                drawing_name=drawing_name
            )
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'matching_pairs': len(matching_drawings),
            'message': f'Processing {len(matching_drawings)} page pairs'
        })
        
    except Exception as e:
        logger.error(f"Error processing split pages: {e}")
        return jsonify({'error': str(e)}), 500
```

---

## Complete Flow

```
1. User selects PDF (100MB, 200 pages)
   â†“
2. Browser loads PDF.js library
   â†“
3. Browser splits PDF:
   â”œâ”€ Page 1 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ Page 2 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ ...
   â””â”€ Page 200 â†’ Canvas â†’ PNG blob (5MB)
   â†“
4. Browser uploads pages directly to GCS:
   â”œâ”€ Upload A-101.png â†’ GCS
   â”œâ”€ Upload A-102.png â†’ GCS
   â””â”€ ... (all in parallel)
   â†“
5. Server receives page list:
   â”œâ”€ Matches pages by drawing name
   â”œâ”€ Creates processing tasks
   â””â”€ Returns immediately
   â†“
6. Workers process pages:
   â”œâ”€ Download single page (5MB) â†’ Process â†’ Upload result
   â””â”€ No full PDF ever on server
```

---

## Benefits

1. No server memory usage: server never sees full PDF
2. No server download: pages uploaded directly to GCS
3. Parallel uploads: multiple pages upload simultaneously
4. Better UX: progress shown during split
5. Offloads work: browser does the splitting
6. True scalability: server only processes small pages

---

## Limitations & Considerations

### Browser Memory Limits

**Problem:**
- Large PDFs (500MB+) may exceed browser memory
- Mobile browsers have stricter limits
- Can cause browser crashes

**Solution:**
```javascript
// Check file size before splitting
if (pdfFile.size > 100 * 1024 * 1024) { // > 100MB
    // Fallback to server-side split
    return this.fallbackToServerSplit(pdfFile);
}
```

### Browser Compatibility

**Supported:**
- Chrome/Edge: âœ… Excellent
- Firefox: âœ… Excellent
- Safari: âœ… Good (iOS 11+)
- Mobile: âš ï¸ Limited (memory constraints)

**Fallback:**
```javascript
// Detect browser capabilities
if (!this.isClientSideSplitSupported()) {
    // Use server-side split
    return this.useServerSideSplit();
}

isClientSideSplitSupported() {
    // Check for required APIs
    return (
        typeof FileReader !== 'undefined' &&
        typeof Blob !== 'undefined' &&
        typeof Canvas !== 'undefined' &&
        // Check if PDF.js can load
        this.canLoadPDFJS()
    );
}
```

### Performance

**Browser Processing:**
- 100-page PDF: ~30-60 seconds in browser
- Uses client CPU/memory
- Blocks UI (should use Web Workers)

**Web Worker Implementation:**

```javascript
// pdf-splitter-worker.js
self.onmessage = async function(e) {
    const { pdfFile, pageNum } = e.data;
    
    // Load and render page in worker
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const page = await pdf.getPage(pageNum);
    // ... render to canvas ...
    
    // Send blob back to main thread
    self.postMessage({
        pageNum: pageNum,
        blob: pageBlob
    });
};
```

---

## Hybrid Approach (Recommended)

### Smart Client/Server Split

```javascript
class SmartPDFHandler {
    async handlePDF(file, type) {
        // Check file size
        if (file.size < 10 * 1024 * 1024) { // < 10MB
            // Small PDF: Split in browser
            return await this.splitInBrowser(file, type);
        } else if (file.size < 50 * 1024 * 1024) { // < 50MB
            // Medium PDF: Check browser capability
            if (this.isBrowserCapable()) {
                return await this.splitInBrowser(file, type);
            } else {
                return await this.splitOnServer(file, type);
            }
        } else {
            // Large PDF: Always split on server (32GB container)
            return await this.splitOnServer(file, type);
        }
    }
    
    isBrowserCapable() {
        // Check available memory, browser type, etc.
        const memory = navigator.deviceMemory || 4; // GB
        return memory >= 8 && this.isModernBrowser();
    }
}
```

---

## Implementation Checklist

1. Add PDF.js library (CDN or npm)
2. Create `PDFSplitter` class
3. Modify `FileUploader` to split PDFs
4. Add server endpoints for page upload URLs
5. Update upload flow to handle split pages
6. Add fallback for unsupported browsers
7. Add progress indicators
8. Test with various PDF sizes

---

## Summary

**Client-Side Split:**
- âœ… Most scalable (no server PDF handling)
- âœ… Best for small-medium PDFs (<50MB)
- âš ï¸ Browser memory limits for large PDFs
- âš ï¸ Requires modern browser
- âœ… Excellent UX (immediate feedback)

**Best Strategy:**
- Small PDFs (<10MB): Client-side split
- Medium PDFs (10-50MB): Client-side if capable, else server
- Large PDFs (>50MB): Server-side split in 32GB container

This gives maximum scalability while handling edge cases.

Should I provide the complete client-side split implementation code?

```plaintext
Traditional Flow:
Client â†’ Upload PDF (100MB) â†’ Server â†’ Split â†’ Process

Client-Side Split Flow:
Client â†’ Split PDF â†’ Upload Pages (5MB each) â†’ Server â†’ Process
```

```javascript
// Load PDF.js library
<script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>

// Or via npm
import * as pdfjsLib from 'pdfjs-dist';
pdfjsLib.GlobalWorkerOptions.workerSrc = 
    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
```

```javascript
/**
 * PDF Splitter Module
 * Splits PDF into individual page images in the browser
 */
class PDFSplitter {
    constructor() {
        this.pdfjsLib = null;
        this.initializePDFJS();
    }

    async initializePDFJS() {
        // Load PDF.js dynamically
        if (typeof pdfjsLib === 'undefined') {
            await this.loadPDFJSLibrary();
        }
        this.pdfjsLib = pdfjsLib;
    }

    async loadPDFJSLibrary() {
        return new Promise((resolve, reject) => {
            const script = document.createElement('script');
            script.src = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js';
            script.onload = () => {
                // Load worker
                pdfjsLib.GlobalWorkerOptions.workerSrc = 
                    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
                resolve();
            };
            script.onerror = reject;
            document.head.appendChild(script);
        });
    }

    /**
     * Split PDF file into individual page images
     * @param {File} pdfFile - PDF file object
     * @param {string} sessionId - Session ID
     * @param {string} drawingType - 'old' or 'new'
     * @param {Function} onProgress - Progress callback (pageNum, totalPages)
     * @returns {Promise<Array>} Array of page info objects
     */
    async splitPDFToPages(pdfFile, sessionId, drawingType, onProgress = null) {
        try {
            console.log(`ðŸ“„ Splitting PDF: ${pdfFile.name} (${(pdfFile.size / 1024 / 1024).toFixed(2)} MB)`);
            
            // Load PDF from file
            const arrayBuffer = await pdfFile.arrayBuffer();
            const pdf = await this.pdfjsLib.getDocument({data: arrayBuffer}).promise;
            
            const totalPages = pdf.numPages;
            console.log(`ðŸ“„ PDF has ${totalPages} pages`);
            
            const pages = [];
            const drawingNames = await this.extractDrawingNames(pdf);
            
            // Process each page
            for (let pageNum = 1; pageNum <= totalPages; pageNum++) {
                try {
                    if (onProgress) {
                        onProgress(pageNum, totalPages);
                    }
                    
                    // Get page
                    const page = await pdf.getPage(pageNum);
                    
                    // Render page to canvas
                    const viewport = page.getViewport({scale: 2.0}); // 2x for 300 DPI equivalent
                    const canvas = document.createElement('canvas');
                    const context = canvas.getContext('2d');
                    canvas.width = viewport.width;
                    canvas.height = viewport.height;
                    
                    // Render page
                    await page.render({
                        canvasContext: context,
                        viewport: viewport
                    }).promise;
                    
                    // Convert canvas to blob (PNG)
                    const blob = await new Promise((resolve) => {
                        canvas.toBlob(resolve, 'image/png', 1.0);
                    });
                    
                    // Get drawing name for this page
                    const drawingName = drawingNames[pageNum] || `Page_${pageNum.toString().padStart(3, '0')}`;
                    
                    pages.push({
                        pageNum: pageNum,
                        drawingName: drawingName,
                        blob: blob,
                        size: blob.size
                    });
                    
                    console.log(`âœ… Extracted page ${pageNum}/${totalPages}: ${drawingName} (${(blob.size / 1024).toFixed(2)} KB)`);
                    
                    // Clean up
                    canvas.width = 0;
                    canvas.height = 0;
                    
                } catch (error) {
                    console.error(`âŒ Error extracting page ${pageNum}:`, error);
                    // Continue with next page
                }
            }
            
            console.log(`âœ… Split complete: ${pages.length} pages extracted`);
            return pages;
            
        } catch (error) {
            console.error('âŒ Error splitting PDF:', error);
            throw new Error(`Failed to split PDF: ${error.message}`);
        }
    }

    /**
     * Extract drawing names from PDF pages (text extraction)
     */
    async extractDrawingNames(pdf) {
        const drawingNames = {};
        const drawingNameRegex = /\b([A-Z])[-\s]?(\d{1,4}(?:\.\d{1,2})?)(?:-([A-Z0-9]{1,8}))?\b/;
        
        for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
            try {
                const page = await pdf.getPage(pageNum);
                const textContent = await page.getTextContent();
                
                // Combine all text items
                const fullText = textContent.items
                    .map(item => item.str)
                    .join(' ');
                
                // Find drawing name
                const match = fullText.match(drawingNameRegex);
                if (match) {
                    const letter = match[1];
                    const number = match[2];
                    const suffix = match[3];
                    const separator = match[0].includes('-') ? '-' : 
                                    match[0].includes(' ') ? ' ' : '';
                    drawingNames[pageNum] = letter + separator + number + (suffix ? '-' + suffix : '');
                } else {
                    drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
                }
            } catch (error) {
                console.warn(`Could not extract drawing name for page ${pageNum}:`, error);
                drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
            }
        }
        
        return drawingNames;
    }
}
```

```javascript
// Modify FileUploader.js
class FileUploader {
    constructor() {
        this.oldFile = null;
        this.newFile = null;
        this.pdfSplitter = new PDFSplitter();
        this.useClientSideSplit = true; // Feature flag
    }

    async handleFileSelect(file, type) {
        // Validate file
        if (!Utils.isValidFileType(file, this.acceptedTypes)) {
            Utils.showToast('Invalid file type', 'error');
            return;
        }

        // Check if PDF and client-side split enabled
        if (file.type === 'application/pdf' && this.useClientSideSplit) {
            // Show splitting progress
            this.showSplittingProgress(type);
            
            try {
                // Split PDF in browser
                const pages = await this.pdfSplitter.splitPDFToPages(
                    file,
                    null, // sessionId will be generated later
                    type,
                    (pageNum, totalPages) => {
                        this.updateSplittingProgress(type, pageNum, totalPages);
                    }
                );
                
                // Store split pages instead of original file
                if (type === 'old') {
                    this.oldFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                } else {
                    this.newFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                }
                
                this.updateFileInfo(type, file, pages.length);
                
            } catch (error) {
                Utils.showToast(`Failed to split PDF: ${error.message}`, 'error');
                // Fallback to original file
                if (type === 'old') {
                    this.oldFile = file;
                } else {
                    this.newFile = file;
                }
            }
        } else {
            // Non-PDF or split disabled - use original file
            if (type === 'old') {
                this.oldFile = file;
            } else {
                this.newFile = file;
            }
        }
        
        this.updateCompareButton();
    }

    showSplittingProgress(type) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            fileInfo.innerHTML = `
                <div class="splitting-progress">
                    <div class="spinner"></div>
                    <span>Splitting PDF into pages...</span>
                </div>
            `;
        }
    }

    updateSplittingProgress(type, currentPage, totalPages) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            const progress = (currentPage / totalPages) * 100;
            fileInfo.querySelector('.splitting-progress span').textContent = 
                `Splitting PDF: ${currentPage}/${totalPages} pages (${Math.round(progress)}%)`;
        }
    }
}
```

```javascript
async processFilesWithClientSideSplit(oldFile, newFile, onComplete) {
    try {
        // Get signed upload URLs for pages
        const sessionId = await this.createSession();
        
        // Upload all pages
        const uploadPromises = [];
        
        // Upload old pages
        if (oldFile.isSplit && oldFile.pages) {
            for (const page of oldFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'old', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        // Upload new pages
        if (newFile.isSplit && newFile.pages) {
            for (const page of newFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'new', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        await Promise.all(uploadPromises);
        
        // Notify server to process pages
        const response = await apiClient.processSplitPages(sessionId, {
            oldPages: oldFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            })),
            newPages: newFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            }))
        });
        
        return response;
        
    } catch (error) {
        console.error('Client-side split processing failed:', error);
        throw error;
    }
}

async getPageUploadUrl(sessionId, drawingType, drawingName) {
    const response = await apiClient.post('/api/get-page-upload-url', {
        session_id: sessionId,
        drawing_type: drawingType,
        drawing_name: drawingName
    });
    return response.upload_url;
}

async uploadPageToGCS(blob, uploadUrl, drawingName) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                console.log(`Uploading ${drawingName}: ${Math.round(e.loaded / e.total * 100)}%`);
            }
        };
        
        xhr.onload = () => {
            if (xhr.status >= 200 && xhr.status < 300) {
                console.log(`âœ… ${drawingName} uploaded`);
                resolve();
            } else {
                reject(new Error(`Upload failed: ${xhr.status}`));
            }
        };
        
        xhr.onerror = () => reject(new Error('Upload failed'));
        
        xhr.open('PUT', uploadUrl);
        xhr.setRequestHeader('Content-Type', 'image/png');
        xhr.send(blob);
    });
}
```

```python
@app.route('/api/get-page-upload-url', methods=['POST'])
def get_page_upload_url():
    """Get signed URL for uploading a single page"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        drawing_type = data['drawing_type']  # 'old' or 'new'
        drawing_name = data['drawing_name']
        
        # Generate storage path
        page_path = f"sessions/{session_id}/pages/{drawing_type}/{drawing_name}.png"
        
        # Generate signed upload URL
        upload_url = storage_service.generate_signed_upload_url(
            page_path,
            expiration_minutes=60,
            content_type='image/png'
        )
        
        return jsonify({
            'upload_url': upload_url,
            'storage_path': page_path
        })
        
    except Exception as e:
        logger.error(f"Error generating page upload URL: {e}")
        return jsonify({'error': str(e)}), 500
```

```python
@app.route('/api/process-split-pages', methods=['POST'])
def process_split_pages():
    """Process pages that were split on client side"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_pages = data['old_pages']  # [{'drawingName': 'A-101', 'pageNum': 1}, ...]
        new_pages = data['new_pages']
        
        # Match pages by drawing name
        old_pages_map = {p['drawingName']: p for p in old_pages}
        new_pages_map = {p['drawingName']: p for p in new_pages}
        matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())
        
        # Create processing tasks for each matching pair
        from gcp.tasks import task_processor
        
        for drawing_name in matching_drawings:
            old_page_path = f"sessions/{session_id}/pages/old/{drawing_name}.png"
            new_page_path = f"sessions/{session_id}/pages/new/{drawing_name}.png"
            
            task_processor.create_page_processing_task(
                session_id=session_id,
                old_page_gcs_path=old_page_path,
                new_page_gcs_path=new_page_path,
                drawing_name=drawing_name
            )
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'matching_pairs': len(matching_drawings),
            'message': f'Processing {len(matching_drawings)} page pairs'
        })
        
    except Exception as e:
        logger.error(f"Error processing split pages: {e}")
        return jsonify({'error': str(e)}), 500
```

```plaintext
1. User selects PDF (100MB, 200 pages)
   â†“
2. Browser loads PDF.js library
   â†“
3. Browser splits PDF:
   â”œâ”€ Page 1 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ Page 2 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ ...
   â””â”€ Page 200 â†’ Canvas â†’ PNG blob (5MB)
   â†“
4. Browser uploads pages directly to GCS:
   â”œâ”€ Upload A-101.png â†’ GCS
   â”œâ”€ Upload A-102.png â†’ GCS
   â””â”€ ... (all in parallel)
   â†“
5. Server receives page list:
   â”œâ”€ Matches pages by drawing name
   â”œâ”€ Creates processing tasks
   â””â”€ Returns immediately
   â†“
6. Workers process pages:
   â”œâ”€ Download single page (5MB) â†’ Process â†’ Upload result
   â””â”€ No full PDF ever on server
```

```javascript
// Check file size before splitting
if (pdfFile.size > 100 * 1024 * 1024) { // > 100MB
    // Fallback to server-side split
    return this.fallbackToServerSplit(pdfFile);
}
```

```javascript
// Detect browser capabilities
if (!this.isClientSideSplitSupported()) {
    // Use server-side split
    return this.useServerSideSplit();
}

isClientSideSplitSupported() {
    // Check for required APIs
    return (
        typeof FileReader !== 'undefined' &&
        typeof Blob !== 'undefined' &&
        typeof Canvas !== 'undefined' &&
        // Check if PDF.js can load
        this.canLoadPDFJS()
    );
}
```

```javascript
// pdf-splitter-worker.js
self.onmessage = async function(e) {
    const { pdfFile, pageNum } = e.data;
    
    // Load and render page in worker
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const page = await pdf.getPage(pageNum);
    // ... render to canvas ...
    
    // Send blob back to main thread
    self.postMessage({
        pageNum: pageNum,
        blob: pageBlob
    });
};
```

```javascript
class SmartPDFHandler {
    async handlePDF(file, type) {
        // Check file size
        if (file.size < 10 * 1024 * 1024) { // < 10MB
            // Small PDF: Split in browser
            return await this.splitInBrowser(file, type);
        } else if (file.size < 50 * 1024 * 1024) { // < 50MB
            // Medium PDF: Check browser capability
            if (this.isBrowserCapable()) {
                return await this.splitInBrowser(file, type);
            } else {
                return await this.splitOnServer(file, type);
            }
        } else {
            // Large PDF: Always split on server (32GB container)
            return await this.splitOnServer(file, type);
        }
    }
    
    isBrowserCapable() {
        // Check available memory, browser type, etc.
        const memory = navigator.deviceMemory || 4; // GB
        return memory >= 8 && this.isModernBrowser();
    }
}
```

---

**User**

put this into an aditional document called selfpjourney 

---

**Cursor**


Creating the "selfpjourney" document with the client-side split explanation. Providing the content to create the file:



Content for `SELFJOURNEY.md` in the documentation folder:

```markdown
# Self Journey: Client-Side PDF Split Architecture

This document details the client-side PDF splitting approach for BuildTrace, explaining how PDFs can be split in the browser before upload to achieve maximum scalability.

## Table of Contents

1. [Concept Overview](#concept-overview)
2. [How It Works](#how-it-works)
3. [Complete Implementation](#complete-implementation)
4. [Server-Side Changes](#server-side-changes)
5. [Benefits & Limitations](#benefits--limitations)
6. [Hybrid Approach](#hybrid-approach)
7. [Implementation Checklist](#implementation-checklist)

---

## Concept Overview

### The Problem

Traditional flow requires the server to download and split large PDFs:
```
Client â†’ Upload PDF (100MB) â†’ Server â†’ Download â†’ Split â†’ Process
```

This creates:
- High server memory usage
- Long processing times
- Single point of failure
- Limited scalability

### The Solution: Client-Side Split

Split PDFs in the browser before uploading:
```
Client â†’ Split PDF â†’ Upload Pages (5MB each) â†’ Server â†’ Process
```

**Key Benefits:**
- Server never sees full PDF
- Pages uploaded directly to GCS
- True parallel processing
- Maximum scalability

---

## How It Works

### Step 1: Load PDF in Browser

**JavaScript Libraries:**
1. **pdf.js** (Mozilla) - Most popular, well-supported
2. **pdf-lib** - Modern, TypeScript-friendly
3. **react-pdf** - If using React

**Using pdf.js:**

```javascript
// Load PDF.js library
<script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>

// Or via npm
import * as pdfjsLib from 'pdfjs-dist';
pdfjsLib.GlobalWorkerOptions.workerSrc = 
    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
```

### Step 2: Split PDF into Pages

**Complete Implementation:**

```javascript
/**
 * PDF Splitter Module
 * Splits PDF into individual page images in the browser
 */
class PDFSplitter {
    constructor() {
        this.pdfjsLib = null;
        this.initializePDFJS();
    }

    async initializePDFJS() {
        // Load PDF.js dynamically
        if (typeof pdfjsLib === 'undefined') {
            await this.loadPDFJSLibrary();
        }
        this.pdfjsLib = pdfjsLib;
    }

    async loadPDFJSLibrary() {
        return new Promise((resolve, reject) => {
            const script = document.createElement('script');
            script.src = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js';
            script.onload = () => {
                // Load worker
                pdfjsLib.GlobalWorkerOptions.workerSrc = 
                    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
                resolve();
            };
            script.onerror = reject;
            document.head.appendChild(script);
        });
    }

    /**
     * Split PDF file into individual page images
     * @param {File} pdfFile - PDF file object
     * @param {string} sessionId - Session ID
     * @param {string} drawingType - 'old' or 'new'
     * @param {Function} onProgress - Progress callback (pageNum, totalPages)
     * @returns {Promise<Array>} Array of page info objects
     */
    async splitPDFToPages(pdfFile, sessionId, drawingType, onProgress = null) {
        try {
            console.log(`ðŸ“„ Splitting PDF: ${pdfFile.name} (${(pdfFile.size / 1024 / 1024).toFixed(2)} MB)`);
            
            // Load PDF from file
            const arrayBuffer = await pdfFile.arrayBuffer();
            const pdf = await this.pdfjsLib.getDocument({data: arrayBuffer}).promise;
            
            const totalPages = pdf.numPages;
            console.log(`ðŸ“„ PDF has ${totalPages} pages`);
            
            const pages = [];
            const drawingNames = await this.extractDrawingNames(pdf);
            
            // Process each page
            for (let pageNum = 1; pageNum <= totalPages; pageNum++) {
                try {
                    if (onProgress) {
                        onProgress(pageNum, totalPages);
                    }
                    
                    // Get page
                    const page = await pdf.getPage(pageNum);
                    
                    // Render page to canvas
                    const viewport = page.getViewport({scale: 2.0}); // 2x for 300 DPI equivalent
                    const canvas = document.createElement('canvas');
                    const context = canvas.getContext('2d');
                    canvas.width = viewport.width;
                    canvas.height = viewport.height;
                    
                    // Render page
                    await page.render({
                        canvasContext: context,
                        viewport: viewport
                    }).promise;
                    
                    // Convert canvas to blob (PNG)
                    const blob = await new Promise((resolve) => {
                        canvas.toBlob(resolve, 'image/png', 1.0);
                    });
                    
                    // Get drawing name for this page
                    const drawingName = drawingNames[pageNum] || `Page_${pageNum.toString().padStart(3, '0')}`;
                    
                    pages.push({
                        pageNum: pageNum,
                        drawingName: drawingName,
                        blob: blob,
                        size: blob.size
                    });
                    
                    console.log(`âœ… Extracted page ${pageNum}/${totalPages}: ${drawingName} (${(blob.size / 1024).toFixed(2)} KB)`);
                    
                    // Clean up
                    canvas.width = 0;
                    canvas.height = 0;
                    
                } catch (error) {
                    console.error(`âŒ Error extracting page ${pageNum}:`, error);
                    // Continue with next page
                }
            }
            
            console.log(`âœ… Split complete: ${pages.length} pages extracted`);
            return pages;
            
        } catch (error) {
            console.error('âŒ Error splitting PDF:', error);
            throw new Error(`Failed to split PDF: ${error.message}`);
        }
    }

    /**
     * Extract drawing names from PDF pages (text extraction)
     */
    async extractDrawingNames(pdf) {
        const drawingNames = {};
        const drawingNameRegex = /\b([A-Z])[-\s]?(\d{1,4}(?:\.\d{1,2})?)(?:-([A-Z0-9]{1,8}))?\b/;
        
        for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
            try {
                const page = await pdf.getPage(pageNum);
                const textContent = await page.getTextContent();
                
                // Combine all text items
                const fullText = textContent.items
                    .map(item => item.str)
                    .join(' ');
                
                // Find drawing name
                const match = fullText.match(drawingNameRegex);
                if (match) {
                    const letter = match[1];
                    const number = match[2];
                    const suffix = match[3];
                    const separator = match[0].includes('-') ? '-' : 
                                    match[0].includes(' ') ? ' ' : '';
                    drawingNames[pageNum] = letter + separator + number + (suffix ? '-' + suffix : '');
                } else {
                    drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
                }
            } catch (error) {
                console.warn(`Could not extract drawing name for page ${pageNum}:`, error);
                drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
            }
        }
        
        return drawingNames;
    }
}
```

### Step 3: Upload Pages Directly to GCS

**Integration with Existing Upload Flow:**

```javascript
// Modify FileUploader.js
class FileUploader {
    constructor() {
        this.oldFile = null;
        this.newFile = null;
        this.pdfSplitter = new PDFSplitter();
        this.useClientSideSplit = true; // Feature flag
    }

    async handleFileSelect(file, type) {
        // Validate file
        if (!Utils.isValidFileType(file, this.acceptedTypes)) {
            Utils.showToast('Invalid file type', 'error');
            return;
        }

        // Check if PDF and client-side split enabled
        if (file.type === 'application/pdf' && this.useClientSideSplit) {
            // Show splitting progress
            this.showSplittingProgress(type);
            
            try {
                // Split PDF in browser
                const pages = await this.pdfSplitter.splitPDFToPages(
                    file,
                    null, // sessionId will be generated later
                    type,
                    (pageNum, totalPages) => {
                        this.updateSplittingProgress(type, pageNum, totalPages);
                    }
                );
                
                // Store split pages instead of original file
                if (type === 'old') {
                    this.oldFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                } else {
                    this.newFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                }
                
                this.updateFileInfo(type, file, pages.length);
                
            } catch (error) {
                Utils.showToast(`Failed to split PDF: ${error.message}`, 'error');
                // Fallback to original file
                if (type === 'old') {
                    this.oldFile = file;
                } else {
                    this.newFile = file;
                }
            }
        } else {
            // Non-PDF or split disabled - use original file
            if (type === 'old') {
                this.oldFile = file;
            } else {
                this.newFile = file;
            }
        }
        
        this.updateCompareButton();
    }

    showSplittingProgress(type) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            fileInfo.innerHTML = `
                <div class="splitting-progress">
                    <div class="spinner"></div>
                    <span>Splitting PDF into pages...</span>
                </div>
            `;
        }
    }

    updateSplittingProgress(type, currentPage, totalPages) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            const progress = (currentPage / totalPages) * 100;
            fileInfo.querySelector('.splitting-progress span').textContent = 
                `Splitting PDF: ${currentPage}/${totalPages} pages (${Math.round(progress)}%)`;
        }
    }
}
```

### Step 4: Upload Pages to GCS

**Modify ProcessingMonitor.js:**

```javascript
async processFilesWithClientSideSplit(oldFile, newFile, onComplete) {
    try {
        // Get signed upload URLs for pages
        const sessionId = await this.createSession();
        
        // Upload all pages
        const uploadPromises = [];
        
        // Upload old pages
        if (oldFile.isSplit && oldFile.pages) {
            for (const page of oldFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'old', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        // Upload new pages
        if (newFile.isSplit && newFile.pages) {
            for (const page of newFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'new', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        await Promise.all(uploadPromises);
        
        // Notify server to process pages
        const response = await apiClient.processSplitPages(sessionId, {
            oldPages: oldFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            })),
            newPages: newFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            }))
        });
        
        return response;
        
    } catch (error) {
        console.error('Client-side split processing failed:', error);
        throw error;
    }
}

async getPageUploadUrl(sessionId, drawingType, drawingName) {
    const response = await apiClient.post('/api/get-page-upload-url', {
        session_id: sessionId,
        drawing_type: drawingType,
        drawing_name: drawingName
    });
    return response.upload_url;
}

async uploadPageToGCS(blob, uploadUrl, drawingName) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                console.log(`Uploading ${drawingName}: ${Math.round(e.loaded / e.total * 100)}%`);
            }
        };
        
        xhr.onload = () => {
            if (xhr.status >= 200 && xhr.status < 300) {
                console.log(`âœ… ${drawingName} uploaded`);
                resolve();
            } else {
                reject(new Error(`Upload failed: ${xhr.status}`));
            }
        };
        
        xhr.onerror = () => reject(new Error('Upload failed'));
        
        xhr.open('PUT', uploadUrl);
        xhr.setRequestHeader('Content-Type', 'image/png');
        xhr.send(blob);
    });
}
```

---

## Server-Side Changes

### New Endpoint: Get Page Upload URLs

**File**: `app.py`

```python
@app.route('/api/get-page-upload-url', methods=['POST'])
def get_page_upload_url():
    """Get signed URL for uploading a single page"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        drawing_type = data['drawing_type']  # 'old' or 'new'
        drawing_name = data['drawing_name']
        
        # Generate storage path
        page_path = f"sessions/{session_id}/pages/{drawing_type}/{drawing_name}.png"
        
        # Generate signed upload URL
        upload_url = storage_service.generate_signed_upload_url(
            page_path,
            expiration_minutes=60,
            content_type='image/png'
        )
        
        return jsonify({
            'upload_url': upload_url,
            'storage_path': page_path
        })
        
    except Exception as e:
        logger.error(f"Error generating page upload URL: {e}")
        return jsonify({'error': str(e)}), 500
```

### New Endpoint: Process Split Pages

**File**: `app.py`

```python
@app.route('/api/process-split-pages', methods=['POST'])
def process_split_pages():
    """Process pages that were split on client side"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_pages = data['old_pages']  # [{'drawingName': 'A-101', 'pageNum': 1}, ...]
        new_pages = data['new_pages']
        
        # Match pages by drawing name
        old_pages_map = {p['drawingName']: p for p in old_pages}
        new_pages_map = {p['drawingName']: p for p in new_pages}
        matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())
        
        # Create processing tasks for each matching pair
        from gcp.tasks import task_processor
        
        for drawing_name in matching_drawings:
            old_page_path = f"sessions/{session_id}/pages/old/{drawing_name}.png"
            new_page_path = f"sessions/{session_id}/pages/new/{drawing_name}.png"
            
            task_processor.create_page_processing_task(
                session_id=session_id,
                old_page_gcs_path=old_page_path,
                new_page_gcs_path=new_page_path,
                drawing_name=drawing_name
            )
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'matching_pairs': len(matching_drawings),
            'message': f'Processing {len(matching_drawings)} page pairs'
        })
        
    except Exception as e:
        logger.error(f"Error processing split pages: {e}")
        return jsonify({'error': str(e)}), 500
```

---

## Complete Flow

```
1. User selects PDF (100MB, 200 pages)
   â†“
2. Browser loads PDF.js library
   â†“
3. Browser splits PDF:
   â”œâ”€ Page 1 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ Page 2 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ ...
   â””â”€ Page 200 â†’ Canvas â†’ PNG blob (5MB)
   â†“
4. Browser uploads pages directly to GCS:
   â”œâ”€ Upload A-101.png â†’ GCS
   â”œâ”€ Upload A-102.png â†’ GCS
   â””â”€ ... (all in parallel)
   â†“
5. Server receives page list:
   â”œâ”€ Matches pages by drawing name
   â”œâ”€ Creates processing tasks
   â””â”€ Returns immediately
   â†“
6. Workers process pages:
   â”œâ”€ Download single page (5MB) â†’ Process â†’ Upload result
   â””â”€ No full PDF ever on server
```

---

## Benefits & Limitations

### Benefits

1. âœ… **No Server Memory Usage**: Server never sees full PDF
2. âœ… **No Server Download**: Pages uploaded directly to GCS
3. âœ… **Parallel Uploads**: Multiple pages upload simultaneously
4. âœ… **Better UX**: Progress shown during split
5. âœ… **Offloads Work**: Browser does the splitting
6. âœ… **True Scalability**: Server only processes small pages

### Limitations & Considerations

#### Browser Memory Limits

**Problem:**
- Large PDFs (500MB+) may exceed browser memory
- Mobile browsers have stricter limits
- Can cause browser crashes

**Solution:**
```javascript
// Check file size before splitting
if (pdfFile.size > 100 * 1024 * 1024) { // > 100MB
    // Fallback to server-side split
    return this.fallbackToServerSplit(pdfFile);
}
```

#### Browser Compatibility

**Supported:**
- Chrome/Edge: âœ… Excellent
- Firefox: âœ… Excellent
- Safari: âœ… Good (iOS 11+)
- Mobile: âš ï¸ Limited (memory constraints)

**Fallback:**
```javascript
// Detect browser capabilities
if (!this.isClientSideSplitSupported()) {
    // Use server-side split
    return this.useServerSideSplit();
}

isClientSideSplitSupported() {
    // Check for required APIs
    return (
        typeof FileReader !== 'undefined' &&
        typeof Blob !== 'undefined' &&
        typeof Canvas !== 'undefined' &&
        // Check if PDF.js can load
        this.canLoadPDFJS()
    );
}
```

#### Performance

**Browser Processing:**
- 100-page PDF: ~30-60 seconds in browser
- Uses client CPU/memory
- Blocks UI (should use Web Workers)

**Web Worker Implementation:**

```javascript
// pdf-splitter-worker.js
self.onmessage = async function(e) {
    const { pdfFile, pageNum } = e.data;
    
    // Load and render page in worker
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const page = await pdf.getPage(pageNum);
    // ... render to canvas ...
    
    // Send blob back to main thread
    self.postMessage({
        pageNum: pageNum,
        blob: pageBlob
    });
};
```

---

## Hybrid Approach (Recommended)

### Smart Client/Server Split

```javascript
class SmartPDFHandler {
    async handlePDF(file, type) {
        // Check file size
        if (file.size < 10 * 1024 * 1024) { // < 10MB
            // Small PDF: Split in browser
            return await this.splitInBrowser(file, type);
        } else if (file.size < 50 * 1024 * 1024) { // < 50MB
            // Medium PDF: Check browser capability
            if (this.isBrowserCapable()) {
                return await this.splitInBrowser(file, type);
            } else {
                return await this.splitOnServer(file, type);
            }
        } else {
            // Large PDF: Always split on server (32GB container)
            return await this.splitOnServer(file, type);
        }
    }
    
    isBrowserCapable() {
        // Check available memory, browser type, etc.
        const memory = navigator.deviceMemory || 4; // GB
        return memory >= 8 && this.isModernBrowser();
    }
}
```

### Strategy Matrix

| PDF Size | Approach | Reason |
|----------|----------|--------|
| < 10MB | Client-side split | Fast, no server load |
| 10-50MB | Client-side if capable, else server | Balance performance and reliability |
| > 50MB | Server-side split (32GB container) | Browser memory limits |

---

## Implementation Checklist

- [ ] Add PDF.js library (CDN or npm)
- [ ] Create `PDFSplitter` class
- [ ] Modify `FileUploader` to split PDFs
- [ ] Add server endpoints for page upload URLs
- [ ] Update upload flow to handle split pages
- [ ] Add fallback for unsupported browsers
- [ ] Add progress indicators
- [ ] Test with various PDF sizes
- [ ] Implement Web Workers for large PDFs
- [ ] Add error handling and retry logic
- [ ] Update documentation

---

## Summary

**Client-Side Split:**
- âœ… Most scalable (no server PDF handling)
- âœ… Best for small-medium PDFs (<50MB)
- âš ï¸ Browser memory limits for large PDFs
- âš ï¸ Requires modern browser
- âœ… Excellent UX (immediate feedback)

**Best Strategy:**
- Small PDFs (<10MB): Client-side split
- Medium PDFs (10-50MB): Client-side if capable, else server
- Large PDFs (>50MB): Server-side split in 32GB container

This approach provides maximum scalability while gracefully handling edge cases and browser limitations.

---

## Related Documentation

- [ARCHITECTURE.md](./ARCHITECTURE.md) - System architecture overview
- [PIPELINE.md](./PIPELINE.md) - Processing pipeline details
- [STORAGE.md](./STORAGE.md) - Storage architecture
- [DEPLOYMENT.md](./DEPLOYMENT.md) - Deployment strategies
```

Save this as `/Users/ashishrajshekhar/Desktop/Job_interview_tasks/Job_trial/documentation/SELFJOURNEY.md`.

The document covers:
- Client-side PDF splitting concept
- Implementation code
- Server-side changes
- Benefits and limitations
- Hybrid approach recommendations
- Implementation checklist

Should I add anything else to this document?

```markdown
# Self Journey: Client-Side PDF Split Architecture

This document details the client-side PDF splitting approach for BuildTrace, explaining how PDFs can be split in the browser before upload to achieve maximum scalability.

## Table of Contents

1. [Concept Overview](#concept-overview)
2. [How It Works](#how-it-works)
3. [Complete Implementation](#complete-implementation)
4. [Server-Side Changes](#server-side-changes)
5. [Benefits & Limitations](#benefits--limitations)
6. [Hybrid Approach](#hybrid-approach)
7. [Implementation Checklist](#implementation-checklist)

---

## Concept Overview

### The Problem

Traditional flow requires the server to download and split large PDFs:
```

```plaintext

This creates:
- High server memory usage
- Long processing times
- Single point of failure
- Limited scalability

### The Solution: Client-Side Split

Split PDFs in the browser before uploading:
```

```plaintext

**Key Benefits:**
- Server never sees full PDF
- Pages uploaded directly to GCS
- True parallel processing
- Maximum scalability

---

## How It Works

### Step 1: Load PDF in Browser

**JavaScript Libraries:**
1. **pdf.js** (Mozilla) - Most popular, well-supported
2. **pdf-lib** - Modern, TypeScript-friendly
3. **react-pdf** - If using React

**Using pdf.js:**
avascript
// Load PDF.js library
<script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script>

// Or via npm
import * as pdfjsLib from 'pdfjs-dist';
pdfjsLib.GlobalWorkerOptions.workerSrc = 
    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';### Step 2: Split PDF into Pages

**Complete Implementation:**

/**
 * PDF Splitter Module
 * Splits PDF into individual page images in the browser
 */
class PDFSplitter {
    constructor() {
        this.pdfjsLib = null;
        this.initializePDFJS();
    }

    async initializePDFJS() {
        // Load PDF.js dynamically
        if (typeof pdfjsLib === 'undefined') {
            await this.loadPDFJSLibrary();
        }
        this.pdfjsLib = pdfjsLib;
    }

    async loadPDFJSLibrary() {
        return new Promise((resolve, reject) => {
            const script = document.createElement('script');
            script.src = 'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js';
            script.onload = () => {
                // Load worker
                pdfjsLib.GlobalWorkerOptions.workerSrc = 
                    'https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.worker.min.js';
                resolve();
            };
            script.onerror = reject;
            document.head.appendChild(script);
        });
    }

    /**
     * Split PDF file into individual page images
     * @param {File} pdfFile - PDF file object
     * @param {string} sessionId - Session ID
     * @param {string} drawingType - 'old' or 'new'
     * @param {Function} onProgress - Progress callback (pageNum, totalPages)
     * @returns {Promise<Array>} Array of page info objects
     */
    async splitPDFToPages(pdfFile, sessionId, drawingType, onProgress = null) {
        try {
            console.log(`ðŸ“„ Splitting PDF: ${pdfFile.name} (${(pdfFile.size / 1024 / 1024).toFixed(2)} MB)`);
            
            // Load PDF from file
            const arrayBuffer = await pdfFile.arrayBuffer();
            const pdf = await this.pdfjsLib.getDocument({data: arrayBuffer}).promise;
            
            const totalPages = pdf.numPages;
            console.log(`ðŸ“„ PDF has ${totalPages} pages`);
            
            const pages = [];
            const drawingNames = await this.extractDrawingNames(pdf);
            
            // Process each page
            for (let pageNum = 1; pageNum <= totalPages; pageNum++) {
                try {
                    if (onProgress) {
                        onProgress(pageNum, totalPages);
                    }
                    
                    // Get page
                    const page = await pdf.getPage(pageNum);
                    
                    // Render page to canvas
                    const viewport = page.getViewport({scale: 2.0}); // 2x for 300 DPI equivalent
                    const canvas = document.createElement('canvas');
                    const context = canvas.getContext('2d');
                    canvas.width = viewport.width;
                    canvas.height = viewport.height;
                    
                    // Render page
                    await page.render({
                        canvasContext: context,
                        viewport: viewport
                    }).promise;
                    
                    // Convert canvas to blob (PNG)
                    const blob = await new Promise((resolve) => {
                        canvas.toBlob(resolve, 'image/png', 1.0);
                    });
                    
                    // Get drawing name for this page
                    const drawingName = drawingNames[pageNum] || `Page_${pageNum.toString().padStart(3, '0')}`;
                    
                    pages.push({
                        pageNum: pageNum,
                        drawingName: drawingName,
                        blob: blob,
                        size: blob.size
                    });
                    
                    console.log(`âœ… Extracted page ${pageNum}/${totalPages}: ${drawingName} (${(blob.size / 1024).toFixed(2)} KB)`);
                    
                    // Clean up
                    canvas.width = 0;
                    canvas.height = 0;
                    
                } catch (error) {
                    console.error(`âŒ Error extracting page ${pageNum}:`, error);
                    // Continue with next page
                }
            }
            
            console.log(`âœ… Split complete: ${pages.length} pages extracted`);
            return pages;
            
        } catch (error) {
            console.error('âŒ Error splitting PDF:', error);
            throw new Error(`Failed to split PDF: ${error.message}`);
        }
    }

    /**
     * Extract drawing names from PDF pages (text extraction)
     */
    async extractDrawingNames(pdf) {
        const drawingNames = {};
        const drawingNameRegex = /\b([A-Z])[-\s]?(\d{1,4}(?:\.\d{1,2})?)(?:-([A-Z0-9]{1,8}))?\b/;
        
        for (let pageNum = 1; pageNum <= pdf.numPages; pageNum++) {
            try {
                const page = await pdf.getPage(pageNum);
                const textContent = await page.getTextContent();
                
                // Combine all text items
                const fullText = textContent.items
                    .map(item => item.str)
                    .join(' ');
                
                // Find drawing name
                const match = fullText.match(drawingNameRegex);
                if (match) {
                    const letter = match[1];
                    const number = match[2];
                    const suffix = match[3];
                    const separator = match[0].includes('-') ? '-' : 
                                    match[0].includes(' ') ? ' ' : '';
                    drawingNames[pageNum] = letter + separator + number + (suffix ? '-' + suffix : '');
                } else {
                    drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
                }
            } catch (error) {
                console.warn(`Could not extract drawing name for page ${pageNum}:`, error);
                drawingNames[pageNum] = `Page_${pageNum.toString().padStart(3, '0')}`;
            }
        }
        
        return drawingNames;
    }
}### Step 3: Upload Pages Directly to GCS

**Integration with Existing Upload Flow:**

// Modify FileUploader.js
class FileUploader {
    constructor() {
        this.oldFile = null;
        this.newFile = null;
        this.pdfSplitter = new PDFSplitter();
        this.useClientSideSplit = true; // Feature flag
    }

    async handleFileSelect(file, type) {
        // Validate file
        if (!Utils.isValidFileType(file, this.acceptedTypes)) {
            Utils.showToast('Invalid file type', 'error');
            return;
        }

        // Check if PDF and client-side split enabled
        if (file.type === 'application/pdf' && this.useClientSideSplit) {
            // Show splitting progress
            this.showSplittingProgress(type);
            
            try {
                // Split PDF in browser
                const pages = await this.pdfSplitter.splitPDFToPages(
                    file,
                    null, // sessionId will be generated later
                    type,
                    (pageNum, totalPages) => {
                        this.updateSplittingProgress(type, pageNum, totalPages);
                    }
                );
                
                // Store split pages instead of original file
                if (type === 'old') {
                    this.oldFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                } else {
                    this.newFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                }
                
                this.updateFileInfo(type, file, pages.length);
                
            } catch (error) {
                Utils.showToast(`Failed to split PDF: ${error.message}`, 'error');
                // Fallback to original file
                if (type === 'old') {
                    this.oldFile = file;
                } else {
                    this.newFile = file;
                }
            }
        } else {
            // Non-PDF or split disabled - use original file
            if (type === 'old') {
                this.oldFile = file;
            } else {
                this.newFile = file;
            }
        }
        
        this.updateCompareButton();
    }

    showSplittingProgress(type) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            fileInfo.innerHTML = `
                <div class="splitting-progress">
                    <div class="spinner"></div>
                    <span>Splitting PDF into pages...</span>
                </div>
            `;
        }
    }

    updateSplittingProgress(type, currentPage, totalPages) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            const progress = (currentPage / totalPages) * 100;
            fileInfo.querySelector('.splitting-progress span').textContent = 
                `Splitting PDF: ${currentPage}/${totalPages} pages (${Math.round(progress)}%)`;
        }
    }
}### Step 4: Upload Pages to GCS

**Modify ProcessingMonitor.js:**
ascript
async processFilesWithClientSideSplit(oldFile, newFile, onComplete) {
    try {
        // Get signed upload URLs for pages
        const sessionId = await this.createSession();
        
        // Upload all pages
        const uploadPromises = [];
        
        // Upload old pages
        if (oldFile.isSplit && oldFile.pages) {
            for (const page of oldFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'old', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        // Upload new pages
        if (newFile.isSplit && newFile.pages) {
            for (const page of newFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'new', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        await Promise.all(uploadPromises);
        
        // Notify server to process pages
        const response = await apiClient.processSplitPages(sessionId, {
            oldPages: oldFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            })),
            newPages: newFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            }))
        });
        
        return response;
        
    } catch (error) {
        console.error('Client-side split processing failed:', error);
        throw error;
    }
}

async getPageUploadUrl(sessionId, drawingType, drawingName) {
    const response = await apiClient.post('/api/get-page-upload-url', {
        session_id: sessionId,
        drawing_type: drawingType,
        drawing_name: drawingName
    });
    return response.upload_url;
}

async uploadPageToGCS(blob, uploadUrl, drawingName) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                console.log(`Uploading ${drawingName}: ${Math.round(e.loaded / e.total * 100)}%`);
            }
        };
        
        xhr.onload = () => {
            if (xhr.status >= 200 && xhr.status < 300) {
                console.log(`âœ… ${drawingName} uploaded`);
                resolve();
            } else {
                reject(new Error(`Upload failed: ${xhr.status}`));
            }
        };
        
        xhr.onerror = () => reject(new Error('Upload failed'));
        
        xhr.open('PUT', uploadUrl);
        xhr.setRequestHeader('Content-Type', 'image/png');
        xhr.send(blob);
    });
}---

## Server-Side Changes

### New Endpoint: Get Page Upload URLs

**File**: `app.py`

@app.route('/api/get-page-upload-url', methods=['POST'])
def get_page_upload_url():
    """Get signed URL for uploading a single page"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        drawing_type = data['drawing_type']  # 'old' or 'new'
        drawing_name = data['drawing_name']
        
        # Generate storage path
        page_path = f"sessions/{session_id}/pages/{drawing_type}/{drawing_name}.png"
        
        # Generate signed upload URL
        upload_url = storage_service.generate_signed_upload_url(
            page_path,
            expiration_minutes=60,
            content_type='image/png'
        )
        
        return jsonify({
            'upload_url': upload_url,
            'storage_path': page_path
        })
        
    except Exception as e:
        logger.error(f"Error generating page upload URL: {e}")
        return jsonify({'error': str(e)}), 500
### New Endpoint: Process Split Pages

**File**: `app.py`

@app.route('/api/process-split-pages', methods=['POST'])
def process_split_pages():
    """Process pages that were split on client side"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_pages = data['old_pages']  # [{'drawingName': 'A-101', 'pageNum': 1}, ...]
        new_pages = data['new_pages']
        
        # Match pages by drawing name
        old_pages_map = {p['drawingName']: p for p in old_pages}
        new_pages_map = {p['drawingName']: p for p in new_pages}
        matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())
        
        # Create processing tasks for each matching pair
        from gcp.tasks import task_processor
        
        for drawing_name in matching_drawings:
            old_page_path = f"sessions/{session_id}/pages/old/{drawing_name}.png"
            new_page_path = f"sessions/{session_id}/pages/new/{drawing_name}.png"
            
            task_processor.create_page_processing_task(
                session_id=session_id,
                old_page_gcs_path=old_page_path,
                new_page_gcs_path=new_page_path,
                drawing_name=drawing_name
            )
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'matching_pairs': len(matching_drawings),
            'message': f'Processing {len(matching_drawings)} page pairs'
        })
        
    except Exception as e:
        logger.error(f"Error processing split pages: {e}")
        return jsonify({'error': str(e)}), 500---

## Complete Flow

```

```javascript

---

## Benefits & Limitations

### Benefits

1. âœ… **No Server Memory Usage**: Server never sees full PDF
2. âœ… **No Server Download**: Pages uploaded directly to GCS
3. âœ… **Parallel Uploads**: Multiple pages upload simultaneously
4. âœ… **Better UX**: Progress shown during split
5. âœ… **Offloads Work**: Browser does the splitting
6. âœ… **True Scalability**: Server only processes small pages

### Limitations & Considerations

#### Browser Memory Limits

**Problem:**
- Large PDFs (500MB+) may exceed browser memory
- Mobile browsers have stricter limits
- Can cause browser crashes

**Solution:**avascript
// Check file size before splitting
if (pdfFile.size > 100 * 1024 * 1024) { // > 100MB
    // Fallback to server-side split
    return this.fallbackToServerSplit(pdfFile);
}#### Browser Compatibility

**Supported:**
- Chrome/Edge: âœ… Excellent
- Firefox: âœ… Excellent
- Safari: âœ… Good (iOS 11+)
- Mobile: âš ï¸ Limited (memory constraints)

**Fallback:**ipt
// Detect browser capabilities
if (!this.isClientSideSplitSupported()) {
    // Use server-side split
    return this.useServerSideSplit();
}

isClientSideSplitSupported() {
    // Check for required APIs
    return (
        typeof FileReader !== 'undefined' &&
        typeof Blob !== 'undefined' &&
        typeof Canvas !== 'undefined' &&
        // Check if PDF.js can load
        this.canLoadPDFJS()
    );
}#### Performance

**Browser Processing:**
- 100-page PDF: ~30-60 seconds in browser
- Uses client CPU/memory
- Blocks UI (should use Web Workers)

**Web Worker Implementation:**
t
// pdf-splitter-worker.js
self.onmessage = async function(e) {
    const { pdfFile, pageNum } = e.data;
    
    // Load and render page in worker
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const page = await pdf.getPage(pageNum);
    // ... render to canvas ...
    
    // Send blob back to main thread
    self.postMessage({
        pageNum: pageNum,
        blob: pageBlob
    });
};---

## Hybrid Approach (Recommended)

### Smart Client/Server Split
cript
class SmartPDFHandler {
    async handlePDF(file, type) {
        // Check file size
        if (file.size < 10 * 1024 * 1024) { // < 10MB
            // Small PDF: Split in browser
            return await this.splitInBrowser(file, type);
        } else if (file.size < 50 * 1024 * 1024) { // < 50MB
            // Medium PDF: Check browser capability
            if (this.isBrowserCapable()) {
                return await this.splitInBrowser(file, type);
            } else {
                return await this.splitOnServer(file, type);
            }
        } else {
            // Large PDF: Always split on server (32GB container)
            return await this.splitOnServer(file, type);
        }
    }
    
    isBrowserCapable() {
        // Check available memory, browser type, etc.
        const memory = navigator.deviceMemory || 4; // GB
        return memory >= 8 && this.isModernBrowser();
    }
}### Strategy Matrix

| PDF Size | Approach | Reason |
|----------|----------|--------|
| < 10MB | Client-side split | Fast, no server load |
| 10-50MB | Client-side if capable, else server | Balance performance and reliability |
| > 50MB | Server-side split (32GB container) | Browser memory limits |

---

## Implementation Checklist

- [ ] Add PDF.js library (CDN or npm)
- [ ] Create `PDFSplitter` class
- [ ] Modify `FileUploader` to split PDFs
- [ ] Add server endpoints for page upload URLs
- [ ] Update upload flow to handle split pages
- [ ] Add fallback for unsupported browsers
- [ ] Add progress indicators
- [ ] Test with various PDF sizes
- [ ] Implement Web Workers for large PDFs
- [ ] Add error handling and retry logic
- [ ] Update documentation

---

## Summary

**Client-Side Split:**
- âœ… Most scalable (no server PDF handling)
- âœ… Best for small-medium PDFs (<50MB)
- âš ï¸ Browser memory limits for large PDFs
- âš ï¸ Requires modern browser
- âœ… Excellent UX (immediate feedback)

**Best Strategy:**
- Small PDFs (<10MB): Client-side split
- Medium PDFs (10-50MB): Client-side if capable, else server
- Large PDFs (>50MB): Server-side split in 32GB container

This approach provides maximum scalability while gracefully handling edge cases and browser limitations.

---

## Related Documentation

- [ARCHITECTURE.md](./ARCHITECTURE.md) - System architecture overview
- [PIPELINE.md](./PIPELINE.md) - Processing pipeline details
- [STORAGE.md](./STORAGE.md) - Storage architecture
- [DEPLOYMENT.md](./DEPLOYMENT.md) - Deployment strategies
```

```plaintext

---

## Benefits & Limitations

### Benefits

1. âœ… **No Server Memory Usage**: Server never sees full PDF
2. âœ… **No Server Download**: Pages uploaded directly to GCS
3. âœ… **Parallel Uploads**: Multiple pages upload simultaneously
4. âœ… **Better UX**: Progress shown during split
5. âœ… **Offloads Work**: Browser does the splitting
6. âœ… **True Scalability**: Server only processes small pages

### Limitations & Considerations

#### Browser Memory Limits

**Problem:**
- Large PDFs (500MB+) may exceed browser memory
- Mobile browsers have stricter limits
- Can cause browser crashes

**Solution:**avascript
// Check file size before splitting
if (pdfFile.size > 100 * 1024 * 1024) { // > 100MB
    // Fallback to server-side split
    return this.fallbackToServerSplit(pdfFile);
}#### Browser Compatibility

**Supported:**
- Chrome/Edge: âœ… Excellent
- Firefox: âœ… Excellent
- Safari: âœ… Good (iOS 11+)
- Mobile: âš ï¸ Limited (memory constraints)

**Fallback:**ipt
// Detect browser capabilities
if (!this.isClientSideSplitSupported()) {
    // Use server-side split
    return this.useServerSideSplit();
}

isClientSideSplitSupported() {
    // Check for required APIs
    return (
        typeof FileReader !== 'undefined' &&
        typeof Blob !== 'undefined' &&
        typeof Canvas !== 'undefined' &&
        // Check if PDF.js can load
        this.canLoadPDFJS()
    );
}#### Performance

**Browser Processing:**
- 100-page PDF: ~30-60 seconds in browser
- Uses client CPU/memory
- Blocks UI (should use Web Workers)

**Web Worker Implementation:**
t
// pdf-splitter-worker.js
self.onmessage = async function(e) {
    const { pdfFile, pageNum } = e.data;
    
    // Load and render page in worker
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const page = await pdf.getPage(pageNum);
    // ... render to canvas ...
    
    // Send blob back to main thread
    self.postMessage({
        pageNum: pageNum,
        blob: pageBlob
    });
};---

## Hybrid Approach (Recommended)

### Smart Client/Server Split
cript
class SmartPDFHandler {
    async handlePDF(file, type) {
        // Check file size
        if (file.size < 10 * 1024 * 1024) { // < 10MB
            // Small PDF: Split in browser
            return await this.splitInBrowser(file, type);
        } else if (file.size < 50 * 1024 * 1024) { // < 50MB
            // Medium PDF: Check browser capability
            if (this.isBrowserCapable()) {
                return await this.splitInBrowser(file, type);
            } else {
                return await this.splitOnServer(file, type);
            }
        } else {
            // Large PDF: Always split on server (32GB container)
            return await this.splitOnServer(file, type);
        }
    }
    
    isBrowserCapable() {
        // Check available memory, browser type, etc.
        const memory = navigator.deviceMemory || 4; // GB
        return memory >= 8 && this.isModernBrowser();
    }
}### Strategy Matrix

| PDF Size | Approach | Reason |
|----------|----------|--------|
| < 10MB | Client-side split | Fast, no server load |
| 10-50MB | Client-side if capable, else server | Balance performance and reliability |
| > 50MB | Server-side split (32GB container) | Browser memory limits |

---

## Implementation Checklist

- [ ] Add PDF.js library (CDN or npm)
- [ ] Create `PDFSplitter` class
- [ ] Modify `FileUploader` to split PDFs
- [ ] Add server endpoints for page upload URLs
- [ ] Update upload flow to handle split pages
- [ ] Add fallback for unsupported browsers
- [ ] Add progress indicators
- [ ] Test with various PDF sizes
- [ ] Implement Web Workers for large PDFs
- [ ] Add error handling and retry logic
- [ ] Update documentation

---

## Summary

**Client-Side Split:**
- âœ… Most scalable (no server PDF handling)
- âœ… Best for small-medium PDFs (<50MB)
- âš ï¸ Browser memory limits for large PDFs
- âš ï¸ Requires modern browser
- âœ… Excellent UX (immediate feedback)

**Best Strategy:**
- Small PDFs (<10MB): Client-side split
- Medium PDFs (10-50MB): Client-side if capable, else server
- Large PDFs (>50MB): Server-side split in 32GB container

This approach provides maximum scalability while gracefully handling edge cases and browser limitations.

---

## Related Documentation

- [ARCHITECTURE.md](./ARCHITECTURE.md) - System architecture overview
- [PIPELINE.md](./PIPELINE.md) - Processing pipeline details
- [STORAGE.md](./STORAGE.md) - Storage architecture
- [DEPLOYMENT.md](./DEPLOYMENT.md) - Deployment strategies
```

```javascript
// Modify FileUploader.js
class FileUploader {
    constructor() {
        this.oldFile = null;
        this.newFile = null;
        this.pdfSplitter = new PDFSplitter();
        this.useClientSideSplit = true; // Feature flag
    }

    async handleFileSelect(file, type) {
        // Validate file
        if (!Utils.isValidFileType(file, this.acceptedTypes)) {
            Utils.showToast('Invalid file type', 'error');
            return;
        }

        // Check if PDF and client-side split enabled
        if (file.type === 'application/pdf' && this.useClientSideSplit) {
            // Show splitting progress
            this.showSplittingProgress(type);
            
            try {
                // Split PDF in browser
                const pages = await this.pdfSplitter.splitPDFToPages(
                    file,
                    null, // sessionId will be generated later
                    type,
                    (pageNum, totalPages) => {
                        this.updateSplittingProgress(type, pageNum, totalPages);
                    }
                );
                
                // Store split pages instead of original file
                if (type === 'old') {
                    this.oldFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                } else {
                    this.newFile = {
                        originalFile: file,
                        pages: pages,
                        isSplit: true
                    };
                }
                
                this.updateFileInfo(type, file, pages.length);
                
            } catch (error) {
                Utils.showToast(`Failed to split PDF: ${error.message}`, 'error');
                // Fallback to original file
                if (type === 'old') {
                    this.oldFile = file;
                } else {
                    this.newFile = file;
                }
            }
        } else {
            // Non-PDF or split disabled - use original file
            if (type === 'old') {
                this.oldFile = file;
            } else {
                this.newFile = file;
            }
        }
        
        this.updateCompareButton();
    }

    showSplittingProgress(type) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            fileInfo.innerHTML = `
                <div class="splitting-progress">
                    <div class="spinner"></div>
                    <span>Splitting PDF into pages...</span>
                </div>
            `;
        }
    }

    updateSplittingProgress(type, currentPage, totalPages) {
        const fileInfo = document.getElementById(`${type}-file-info`);
        if (fileInfo) {
            const progress = (currentPage / totalPages) * 100;
            fileInfo.querySelector('.splitting-progress span').textContent = 
                `Splitting PDF: ${currentPage}/${totalPages} pages (${Math.round(progress)}%)`;
        }
    }
}
```

```javascript
async processFilesWithClientSideSplit(oldFile, newFile, onComplete) {
    try {
        // Get signed upload URLs for pages
        const sessionId = await this.createSession();
        
        // Upload all pages
        const uploadPromises = [];
        
        // Upload old pages
        if (oldFile.isSplit && oldFile.pages) {
            for (const page of oldFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'old', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        // Upload new pages
        if (newFile.isSplit && newFile.pages) {
            for (const page of newFile.pages) {
                const uploadUrl = await this.getPageUploadUrl(
                    sessionId, 
                    'new', 
                    page.drawingName
                );
                uploadPromises.push(
                    this.uploadPageToGCS(page.blob, uploadUrl, page.drawingName)
                );
            }
        }
        
        await Promise.all(uploadPromises);
        
        // Notify server to process pages
        const response = await apiClient.processSplitPages(sessionId, {
            oldPages: oldFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            })),
            newPages: newFile.pages.map(p => ({
                drawingName: p.drawingName,
                pageNum: p.pageNum
            }))
        });
        
        return response;
        
    } catch (error) {
        console.error('Client-side split processing failed:', error);
        throw error;
    }
}

async getPageUploadUrl(sessionId, drawingType, drawingName) {
    const response = await apiClient.post('/api/get-page-upload-url', {
        session_id: sessionId,
        drawing_type: drawingType,
        drawing_name: drawingName
    });
    return response.upload_url;
}

async uploadPageToGCS(blob, uploadUrl, drawingName) {
    return new Promise((resolve, reject) => {
        const xhr = new XMLHttpRequest();
        
        xhr.upload.onprogress = (e) => {
            if (e.lengthComputable) {
                console.log(`Uploading ${drawingName}: ${Math.round(e.loaded / e.total * 100)}%`);
            }
        };
        
        xhr.onload = () => {
            if (xhr.status >= 200 && xhr.status < 300) {
                console.log(`âœ… ${drawingName} uploaded`);
                resolve();
            } else {
                reject(new Error(`Upload failed: ${xhr.status}`));
            }
        };
        
        xhr.onerror = () => reject(new Error('Upload failed'));
        
        xhr.open('PUT', uploadUrl);
        xhr.setRequestHeader('Content-Type', 'image/png');
        xhr.send(blob);
    });
}
```

```python
@app.route('/api/get-page-upload-url', methods=['POST'])
def get_page_upload_url():
    """Get signed URL for uploading a single page"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        drawing_type = data['drawing_type']  # 'old' or 'new'
        drawing_name = data['drawing_name']
        
        # Generate storage path
        page_path = f"sessions/{session_id}/pages/{drawing_type}/{drawing_name}.png"
        
        # Generate signed upload URL
        upload_url = storage_service.generate_signed_upload_url(
            page_path,
            expiration_minutes=60,
            content_type='image/png'
        )
        
        return jsonify({
            'upload_url': upload_url,
            'storage_path': page_path
        })
        
    except Exception as e:
        logger.error(f"Error generating page upload URL: {e}")
        return jsonify({'error': str(e)}), 500
```

```python
@app.route('/api/process-split-pages', methods=['POST'])
def process_split_pages():
    """Process pages that were split on client side"""
    try:
        data = request.get_json()
        session_id = data['session_id']
        old_pages = data['old_pages']  # [{'drawingName': 'A-101', 'pageNum': 1}, ...]
        new_pages = data['new_pages']
        
        # Match pages by drawing name
        old_pages_map = {p['drawingName']: p for p in old_pages}
        new_pages_map = {p['drawingName']: p for p in new_pages}
        matching_drawings = set(old_pages_map.keys()) & set(new_pages_map.keys())
        
        # Create processing tasks for each matching pair
        from gcp.tasks import task_processor
        
        for drawing_name in matching_drawings:
            old_page_path = f"sessions/{session_id}/pages/old/{drawing_name}.png"
            new_page_path = f"sessions/{session_id}/pages/new/{drawing_name}.png"
            
            task_processor.create_page_processing_task(
                session_id=session_id,
                old_page_gcs_path=old_page_path,
                new_page_gcs_path=new_page_path,
                drawing_name=drawing_name
            )
        
        return jsonify({
            'success': True,
            'session_id': session_id,
            'matching_pairs': len(matching_drawings),
            'message': f'Processing {len(matching_drawings)} page pairs'
        })
        
    except Exception as e:
        logger.error(f"Error processing split pages: {e}")
        return jsonify({'error': str(e)}), 500
```

```plaintext
1. User selects PDF (100MB, 200 pages)
   â†“
2. Browser loads PDF.js library
   â†“
3. Browser splits PDF:
   â”œâ”€ Page 1 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ Page 2 â†’ Canvas â†’ PNG blob (5MB)
   â”œâ”€ ...
   â””â”€ Page 200 â†’ Canvas â†’ PNG blob (5MB)
   â†“
4. Browser uploads pages directly to GCS:
   â”œâ”€ Upload A-101.png â†’ GCS
   â”œâ”€ Upload A-102.png â†’ GCS
   â””â”€ ... (all in parallel)
   â†“
5. Server receives page list:
   â”œâ”€ Matches pages by drawing name
   â”œâ”€ Creates processing tasks
   â””â”€ Returns immediately
   â†“
6. Workers process pages:
   â”œâ”€ Download single page (5MB) â†’ Process â†’ Upload result
   â””â”€ No full PDF ever on server
```

```javascript
// Check file size before splitting
if (pdfFile.size > 100 * 1024 * 1024) { // > 100MB
    // Fallback to server-side split
    return this.fallbackToServerSplit(pdfFile);
}
```

```javascript
// Detect browser capabilities
if (!this.isClientSideSplitSupported()) {
    // Use server-side split
    return this.useServerSideSplit();
}

isClientSideSplitSupported() {
    // Check for required APIs
    return (
        typeof FileReader !== 'undefined' &&
        typeof Blob !== 'undefined' &&
        typeof Canvas !== 'undefined' &&
        // Check if PDF.js can load
        this.canLoadPDFJS()
    );
}
```

```javascript
// pdf-splitter-worker.js
self.onmessage = async function(e) {
    const { pdfFile, pageNum } = e.data;
    
    // Load and render page in worker
    const pdf = await pdfjsLib.getDocument({data: pdfFile}).promise;
    const page = await pdf.getPage(pageNum);
    // ... render to canvas ...
    
    // Send blob back to main thread
    self.postMessage({
        pageNum: pageNum,
        blob: pageBlob
    });
};
```

```javascript
class SmartPDFHandler {
    async handlePDF(file, type) {
        // Check file size
        if (file.size < 10 * 1024 * 1024) { // < 10MB
            // Small PDF: Split in browser
            return await this.splitInBrowser(file, type);
        } else if (file.size < 50 * 1024 * 1024) { // < 50MB
            // Medium PDF: Check browser capability
            if (this.isBrowserCapable()) {
                return await this.splitInBrowser(file, type);
            } else {
                return await this.splitOnServer(file, type);
            }
        } else {
            // Large PDF: Always split on server (32GB container)
            return await this.splitOnServer(file, type);
        }
    }
    
    isBrowserCapable() {
        // Check available memory, browser type, etc.
        const memory = navigator.deviceMemory || 4; // GB
        return memory >= 8 && this.isModernBrowser();
    }
}
```

```plaintext

Save this as `/Users/ashishrajshekhar/Desktop/Job_interview_tasks/Job_trial/documentation/SELFJOURNEY.md`.

The document covers:
- Client-side PDF splitting concept
- Implementation code
- Server-side changes
- Benefits and limitations
- Hybrid approach recommendations
- Implementation checklist

Should I add anything else to this document?
```

